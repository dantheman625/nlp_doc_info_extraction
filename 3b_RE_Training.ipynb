{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRDh0DOTlfMd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbq3q2QYn3K3"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlN6yKHRdBB-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import wandb\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIjI3OxelekE"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqVAN7HVJSU"
      },
      "source": [
        "# Establish Google Drive Connection (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66MhS3djMnu0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZUk5Z6TlhA8"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R82b8HwhlkzF"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME   = \"RE_SpanBERT_Finetune\"\n",
        "#DATA_FILES     = {\"train\": \"train_re.csv\", \"validation\": \"val_re.csv\", \"test\": \"test_re.csv\"}\n",
        "MODEL_NAME     = \"SpanBERT/spanbert-large-cased\"\n",
        "NUM_EPOCHS     = 20\n",
        "LEARNING_RATES = [5e-5, 3e-5, 2e-5, 1e-5]\n",
        "FIXED_LR       = 2e-5\n",
        "BATCH_SIZES    = [8, 16, 24]\n",
        "LOG_STEPS      = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkC94aEmlmjZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdNEsDAPl8EI"
      },
      "source": [
        "# Data loading & preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QflUJWLHmA54"
      },
      "source": [
        "## Load datasets - Challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WRV4tOwKv4r"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/project_files/data/raw/train\"\n",
        "\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    raise FileNotFoundError(f\"Directory not found: {DATA_DIR}. Please verify the path.\")\n",
        "\n",
        "rows = []\n",
        "for fname in os.listdir(DATA_DIR):\n",
        "    if not fname.endswith(\".json\"):\n",
        "        continue\n",
        "    path = os.path.join(DATA_DIR, fname)\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    records = data if isinstance(data, list) else [data]\n",
        "\n",
        "    for rec in records:\n",
        "        doc       = rec.get(\"doc\", \"\")\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', doc)\n",
        "\n",
        "        for triple in rec.get(\"triples\", []):\n",
        "            head     = triple[\"head\"]\n",
        "            tail     = triple[\"tail\"]\n",
        "            relation = triple[\"relation\"]\n",
        "\n",
        "            sentence = next((s for s in sentences if head in s and tail in s), \"\")\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            rows.append({\n",
        "                \"entity1\": head,\n",
        "                \"entity2\": tail,\n",
        "                \"text\":    sentence,\n",
        "                \"relation\": relation\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"relation_dataset.csv\", index=False)\n",
        "print(f\"Built dataset with {len(df)} rows and wrote relation_dataset.csv\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x3hf-FKHLXU"
      },
      "source": [
        "### New challenge dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR0ZEZlnmAKT"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"relation_dataset.csv\")\n",
        "print(\"Counts per relation before split:\\n\")\n",
        "print(df[\"relation\"].value_counts())\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "train_labels = set(train_df[\"relation\"])\n",
        "\n",
        "val_rogue  = val_df[~val_df[\"relation\"].isin(train_labels)]\n",
        "test_rogue = test_df[~test_df[\"relation\"].isin(train_labels)]\n",
        "\n",
        "if not val_rogue.empty or not test_rogue.empty:\n",
        "    print(f\"Moving {len(val_rogue)} val + {len(test_rogue)} test rogue rows → train\")\n",
        "    train_df = pd.concat([train_df, val_rogue, test_rogue], ignore_index=True)\n",
        "    val_df   = val_df[val_df[\"relation\"].isin(train_labels)]\n",
        "    test_df  = test_df[test_df[\"relation\"].isin(train_labels)]\n",
        "\n",
        "print(f\"\\nFinal sizes → train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}\")\n",
        "print(\"Relations per split:\")\n",
        "print(f\"  train: {train_df['relation'].value_counts().to_dict()}\")\n",
        "print(f\"  val  : {val_df['relation'].value_counts().to_dict()}\")\n",
        "print(f\"  test : {test_df['relation'].value_counts().to_dict()}\")\n",
        "\n",
        "raw_datasets = DatasetDict({\n",
        "    \"train\":      Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
        "    \"test\":       Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "def make_example(example):\n",
        "    sent = example[\"text\"]\n",
        "    e1, e2 = example[\"entity1\"], example[\"entity2\"]\n",
        "    marked = sent.replace(e1, f\"[E1]{e1}[/E1]\") \\\n",
        "                 .replace(e2, f\"[E2]{e2}[/E2]\")\n",
        "    return {\n",
        "        \"text\":           marked,\n",
        "        \"entity1_label\":  e1,\n",
        "        \"entity2_label\":  e2,\n",
        "        \"relation_label\": example[\"relation\"],\n",
        "    }\n",
        "\n",
        "token_input_datasets = raw_datasets.map(\n",
        "    make_example,\n",
        "    remove_columns=[\"text\", \"entity1\", \"entity2\", \"relation\"],\n",
        ")\n",
        "\n",
        "unique_rels = sorted(token_input_datasets[\"train\"].unique(\"relation_label\"))\n",
        "label2id = {rel: i for i, rel in enumerate(unique_rels)}\n",
        "id2label = {i: rel for rel, i in label2id.items()}\n",
        "\n",
        "def add_label_ids(example):\n",
        "    return {\"labels\": label2id[example[\"relation_label\"]]}\n",
        "\n",
        "final_datasets = token_input_datasets.map(\n",
        "    add_label_ids,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e5tDDKPcoJd"
      },
      "source": [
        "## Own custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPy8ZFwQcnJw"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/project_files/data/processed/RE_datasets/\"\n",
        "train_df = pd.read_csv(base_path + \"train_re.csv\")\n",
        "val_df   = pd.read_csv(base_path + \"val_re.csv\")\n",
        "test_df  = pd.read_csv(base_path + \"test_re.csv\")\n",
        "\n",
        "print(f\"Loaded {len(train_df)} train / {len(val_df)} val / {len(test_df)} test rows\")\n",
        "\n",
        "raw_datasets = DatasetDict({\n",
        "    \"train\":      Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
        "    \"test\":       Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
        "})\n",
        "\n",
        "def make_example(example):\n",
        "    sent = example[\"RE_sentence\"]\n",
        "    e1 = example[\"entity1_label\"]\n",
        "    e2 = example[\"entity2_label\"]\n",
        "    marked = sent.replace(e1, f\"[E1]{e1}[/E1]\").replace(e2, f\"[E2]{e2}[/E2]\")\n",
        "    return {\n",
        "        \"text\": marked,\n",
        "        \"entity1_label\": e1,\n",
        "        \"entity2_label\": e2,\n",
        "        \"relation_label\": example[\"relation_label\"],\n",
        "    }\n",
        "\n",
        "formatted_datasets = raw_datasets.map(\n",
        "    make_example,\n",
        "    remove_columns=[\"RE_sentence\", \"relation\", \"entity1_id\", \"entity2_id\"],\n",
        ")\n",
        "\n",
        "unique_rels = sorted(formatted_datasets[\"train\"].unique(\"relation_label\"))\n",
        "label2id = {rel: i for i, rel in enumerate(unique_rels)}\n",
        "id2label = {i: rel for rel, i in label2id.items()}\n",
        "\n",
        "def add_label_ids(example):\n",
        "    return {\"labels\": label2id[example[\"relation_label\"]]}\n",
        "\n",
        "final_datasets = formatted_datasets.map(add_label_ids)\n",
        "\n",
        "print(f\"Final dataset features: {final_datasets['train'].features}\")\n",
        "print(f\"Label mapping:\\n{label2id}\")\n",
        "\n",
        "print(\"\\n🔍 Sample examples from training set:\\n\")\n",
        "final_datasets[\"train\"].shuffle(seed=42).select(range(3)).to_pandas()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0Ii9WNbmFWs"
      },
      "source": [
        "## Insert entity markers and map labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYqftmsQmL-n"
      },
      "source": [
        "## Tokenizer and data collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAD00qIOxVCW"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=256,\n",
        "    )\n",
        "\n",
        "tokenized = final_datasets.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFGZtFHMmQ7H"
      },
      "source": [
        "# Baseline Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsGPc__amT6f"
      },
      "source": [
        "## Start W&B run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tq7lRdEmWxP"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=PROJECT_NAME,\n",
        "    name=\"baseline_new_training\",\n",
        "    reinit=True,\n",
        "    config={\n",
        "        \"model\": MODEL_NAME,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7b13dycmg5B"
      },
      "outputs": [],
      "source": [
        "baseline_table = wandb.Table(columns=[\"split\", \"eval_loss\", \"precision\", \"recall\", \"f1\", \"accuracy\"])\n",
        "\n",
        "target_labels = list(label2id.keys())\n",
        "\n",
        "baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(target_labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "def compute_seq_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    labels = p.label_ids\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": acc}\n",
        "\n",
        "for split in [\"validation\", \"test\"]:\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"./baseline_{split}\",\n",
        "        do_train=False,\n",
        "        do_eval=True,\n",
        "        per_device_eval_batch_size=32,\n",
        "        logging_strategy=\"no\",\n",
        "        save_strategy=\"no\",\n",
        "        report_to=[],\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=baseline_model,\n",
        "        args=args,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "        eval_dataset=tokenized[split],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_seq_metrics,\n",
        "    )\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    baseline_table.add_data(\n",
        "        split,\n",
        "        metrics[\"eval_loss\"],\n",
        "        metrics[\"eval_precision\"],\n",
        "        metrics[\"eval_recall\"],\n",
        "        metrics[\"eval_f1\"],\n",
        "        metrics[\"eval_accuracy\"],\n",
        "    )\n",
        "\n",
        "wandb.log({\"baseline_metrics_table\": baseline_table})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt5yH9GAm61U"
      },
      "source": [
        "## Evaluate and log on both splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZIbK8s7m8zs"
      },
      "outputs": [],
      "source": [
        "for split in [\"validation\",\"test\"]:\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"baseline_{split}\",\n",
        "        per_device_eval_batch_size=BATCH_SIZES[-1],\n",
        "        do_train=False, do_eval=True,\n",
        "        logging_strategy=\"no\", save_strategy=\"no\",\n",
        "        report_to=[\"wandb\"],\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=baseline_model, args=args,\n",
        "        data_collator=data_collator,\n",
        "        eval_dataset=tokenized[split], tokenizer=tokenizer,\n",
        "        compute_metrics=compute_seq_metrics,\n",
        "    )\n",
        "    res = trainer.evaluate()\n",
        "    wandb.log({f\"baseline/{split}_{k}\":v for k,v in res.items()})\n",
        "    baseline_table.add_data(\n",
        "        split,\n",
        "        res['eval_loss'],\n",
        "        res['eval_precision'],\n",
        "        res['eval_recall'],\n",
        "        res['eval_f1'],\n",
        "        res['eval_accuracy'],\n",
        "    )\n",
        "    print(f\"Baseline on {split}:\",res)\n",
        "\n",
        "wandb.log({\"baseline_metrics_table\":baseline_table})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8Z3WUgqnE4m"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acSYiqdqnGyJ"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmKph4KtnSZM"
      },
      "outputs": [],
      "source": [
        "del baseline_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for lr in LEARNING_RATES:\n",
        "    wandb.init(\n",
        "        project=PROJECT_NAME,\n",
        "        config={\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"epochs\": NUM_EPOCHS,\n",
        "            \"learning_rate\": lr,\n",
        "            \"batch_size\": 4,\n",
        "            \"log_steps\": LOG_STEPS,\n",
        "        },\n",
        "        reinit=True,\n",
        "        name=f\"bs_4_lr_{lr}\",\n",
        "        resume=False,\n",
        "    )\n",
        "\n",
        "    metrics_table = wandb.Table(columns=[\"epoch\",\"train_loss\",\"eval_loss\",\"precision\",\"recall\",\"f1\",\"accuracy\"])\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=len(target_labels), id2label=id2label, label2id=label2id,\n",
        "    )\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"outputs/bs_4\", overwrite_output_dir=True,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        learning_rate=FIXED_LR,\n",
        "        logging_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        report_to=[\"wandb\"],\n",
        "        load_best_model_at_end=False,\n",
        "        fp16=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=args,\n",
        "        train_dataset=tokenized[\"train\"],\n",
        "        eval_dataset=tokenized[\"validation\"],\n",
        "        data_collator=data_collator, tokenizer=tokenizer,\n",
        "        compute_metrics=compute_seq_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    for log in trainer.state.log_history:\n",
        "        if all(k in log for k in [\"epoch\",\"eval_loss\"]):\n",
        "            metrics_table.add_data(\n",
        "                log[\"epoch\"], log.get(\"loss\"), log[\"eval_loss\"],\n",
        "                log.get(\"eval_precision\"), log.get(\"eval_recall\"), log.get(\"eval_f1\"), log.get(\"eval_accuracy\"),\n",
        "            )\n",
        "    wandb.log({\"metrics_table\":metrics_table})\n",
        "\n",
        "    final_metrics = trainer.evaluate()\n",
        "    wandb.log({f\"final/bs_4_{k}\":v for k,v in final_metrics.items()})\n",
        "\n",
        "    preds_out = trainer.predict(tokenized[\"validation\"])\n",
        "    preds = np.argmax(preds_out.predictions,axis=1)\n",
        "    cm = wandb.plot.confusion_matrix(probs=None, y_true=preds_out.label_ids, preds=preds, class_names=target_labels)\n",
        "    wandb.log({\"confusion_matrix\":cm})\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA_46-u0YpdO"
      },
      "source": [
        "# 200 epoch training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuBJ0-FpYs4z"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaHAP_UhYrQu"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=PROJECT_NAME,\n",
        "    name=\"train_new_200e_lr2e-5_16b_3\",\n",
        "    reinit=True,\n",
        "    config={\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"epochs\": 200,\n",
        "        \"learning_rate\": FIXED_LR,\n",
        "        \"batch_size\": 16,\n",
        "        \"log_steps\": LOG_STEPS,\n",
        "    }\n",
        ")\n",
        "\n",
        "train200_table = wandb.Table(columns=[\"epoch\",\"train_loss\",\"eval_loss\",\"precision\",\"recall\",\"f1\",\"accuracy\"])\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(target_labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"outputs/train_200e_lr2e-5_16b\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=200,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=FIXED_LR,\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    report_to=[\"wandb\"],\n",
        "    load_best_model_at_end=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_seq_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFGLF4-SY2DE"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ASZDQfLY1QW"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "\n",
        "for log in trainer.state.log_history:\n",
        "    if \"epoch\" in log and \"eval_loss\" in log:\n",
        "        train200_table.add_data(\n",
        "            log[\"epoch\"],\n",
        "            log.get(\"loss\"),\n",
        "            log.get(\"eval_loss\"),\n",
        "            log.get(\"eval_precision\"),\n",
        "            log.get(\"eval_recall\"),\n",
        "            log.get(\"eval_f1\"),\n",
        "            log.get(\"eval_accuracy\"),\n",
        "        )\n",
        "wandb.log({\"train_200e_metrics_table\": train200_table})\n",
        "\n",
        "final_metrics = trainer.evaluate()\n",
        "wandb.log({f\"train_200e_lr2e-5_16b_{k}\": v for k, v in final_metrics.items()})\n",
        "preds_out = trainer.predict(tokenized[\"validation\"])\n",
        "preds = np.argmax(preds_out.predictions, axis=1)\n",
        "cm = wandb.plot.confusion_matrix(\n",
        "    probs=None,\n",
        "    y_true=preds_out.label_ids,\n",
        "    preds=preds,\n",
        "    class_names=target_labels\n",
        ")\n",
        "wandb.log({\"train_200e_confusion_matrix\": cm})\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhvtBcYi81Go"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHwd_IaK84gN"
      },
      "outputs": [],
      "source": [
        "# If running immediately after training, `trainer.model` is already the final model.\n",
        "# Otherwise, load from the output directory of your best run:\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"outputs/train_200e_lr2e-5_16b\", num_labels=len(target_labels), id2label=id2label, label2id=label2id)\n",
        "model = trainer.model  # reuse the model from the last training cell\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsdhU-5W_6j7"
      },
      "source": [
        "## Inference only trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTF95Ke4_8MY"
      },
      "outputs": [],
      "source": [
        "test_args = TrainingArguments(\n",
        "    output_dir=\"inference\",\n",
        "    per_device_eval_batch_size= 16,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    logging_strategy=\"no\",\n",
        "    save_strategy=\"no\",\n",
        ")\n",
        "inference_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=test_args,\n",
        "    data_collator=data_collator,\n",
        "    eval_dataset=tokenized[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=lambda p: {\n",
        "        **{\"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))},\n",
        "        **{k: v for k, v in zip([\"precision\",\"recall\",\"f1\"], precision_recall_fscore_support(p.label_ids, np.argmax(p.predictions, axis=1), average='macro', zero_division=0)[:3])}\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k14mz5kvAUCb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTR2Cb1LAZu8"
      },
      "outputs": [],
      "source": [
        "test_results = inference_trainer.evaluate()\n",
        "print(\"Test metrics:\", test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCAtNPR7AeQd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbNhig56AfLM"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=PROJECT_NAME, name=\"test_evaluation_1\", reinit=True)\n",
        "wandb.log({f\"test/{k}\": v for k, v in test_results.items() if k.startswith(\"eval_\")})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQF9OVfAiMg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib6Ieo6GAijL"
      },
      "outputs": [],
      "source": [
        "preds_out = inference_trainer.predict(tokenized[\"test\"])\n",
        "pred_ids = np.argmax(preds_out.predictions, axis=1)\n",
        "\n",
        "triples = []\n",
        "for i, pred_id in enumerate(pred_ids):\n",
        "    ex = tokenized[\"test\"][i]\n",
        "    triples.append({\n",
        "        \"head\": ex[\"entity1_label\"],\n",
        "        \"relation\": id2label[pred_id],\n",
        "        \"tail\": ex[\"entity2_label\"],\n",
        "    })\n",
        "\n",
        "output = {\"triples\": triples, \"label_set\": target_labels}\n",
        "json_path = \"predictions.json\"\n",
        "with open(json_path, \"w\") as fp:\n",
        "    json.dump(output, fp, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAxQfkwcAmGh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-1z5nakAlc5"
      },
      "outputs": [],
      "source": [
        "wandb.save(json_path)\n",
        "wandb.finish()\n",
        "print(f\"Wrote {len(triples)} triples and logged test metrics to W&B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp2-qFdcCHCo"
      },
      "source": [
        "#Final training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKTEtvhBCjzy"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=PROJECT_NAME,\n",
        "    name=\"labels_new_train_20e_lr2e-5_bs16\",\n",
        "    reinit=True,\n",
        "    config={\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"epochs\": 20,\n",
        "        \"learning_rate\": FIXED_LR,\n",
        "        \"batch_size\": 16,\n",
        "        \"log_steps\": LOG_STEPS,\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydm359nlCm70"
      },
      "outputs": [],
      "source": [
        "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwsa2lVKCopN"
      },
      "outputs": [],
      "source": [
        "final_args = TrainingArguments(\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=1,\n",
        "    report_to=[\"wandb\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ede_3ITLWpeP"
      },
      "source": [
        "### Old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DeHvKM3CqNe"
      },
      "outputs": [],
      "source": [
        "def compute_seq_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    labels = p.label_ids\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": acc}\n",
        "\n",
        "final_trainer = Trainer(\n",
        "    model=final_model,\n",
        "    args=final_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_seq_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwW0oQpTWqnD"
      },
      "source": [
        "### New"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi2A3lEYWrzM"
      },
      "outputs": [],
      "source": [
        "def compute_seq_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "    per_label_precision, per_label_recall, per_label_f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=None, zero_division=0, labels=list(label2id.values())\n",
        "    )\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": macro_precision,\n",
        "        \"recall\": macro_recall,\n",
        "        \"f1\": macro_f1,\n",
        "    }\n",
        "\n",
        "    for i, label_id in enumerate(label2id.values()):\n",
        "        label_name = id2label[label_id]\n",
        "        metrics[f\"{label_name}_precision\"] = per_label_precision[i]\n",
        "        metrics[f\"{label_name}_recall\"] = per_label_recall[i]\n",
        "        metrics[f\"{label_name}_f1\"] = per_label_f1[i]\n",
        "\n",
        "    return metrics\n",
        "\n",
        "final_trainer = Trainer(\n",
        "      model=final_model,\n",
        "      args=final_args,\n",
        "      train_dataset=tokenized[\"train\"],\n",
        "      eval_dataset=tokenized[\"validation\"],\n",
        "      data_collator=data_collator,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_seq_metrics,\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XxGxD4ZCr3c"
      },
      "outputs": [],
      "source": [
        "final_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyyKOJMmCtRg"
      },
      "outputs": [],
      "source": [
        "best_ckpt_dir = final_args.output_dir\n",
        "final_trainer.save_model(best_ckpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfaeGOyGCusk"
      },
      "outputs": [],
      "source": [
        "artifact = wandb.Artifact(\"spanbert_new_best_checkpoint\", type=\"model\")\n",
        "artifact.add_dir(best_ckpt_dir)\n",
        "wandb.log_artifact(artifact, aliases=[\"best\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0K6hI3qj55d"
      },
      "outputs": [],
      "source": [
        "parent_dir = \"/content/drive/MyDrive/project_files/checkpoints/RE/\"\n",
        "\n",
        "best_ckpt = final_trainer.state.best_model_checkpoint\n",
        "\n",
        "best_name = os.path.basename(best_ckpt.rstrip(\"/\"))\n",
        "new_folder = os.path.join(parent_dir, f\"best_{best_name}\")\n",
        "\n",
        "os.makedirs(new_folder, exist_ok=True)\n",
        "\n",
        "final_trainer.save_model(new_folder)\n",
        "\n",
        "tokenizer.save_pretrained(new_folder)\n",
        "\n",
        "print(f\"Saved best model and tokenizer into {new_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ_2h7SyD8p9"
      },
      "outputs": [],
      "source": [
        "final_table = wandb.Table(columns=[\"epoch\",\"train_loss\",\"eval_loss\",\"precision\",\"recall\",\"f1\",\"accuracy\"])\n",
        "for log in final_trainer.state.log_history:\n",
        "    if all(k in log for k in [\"epoch\",\"eval_loss\"]):\n",
        "        final_table.add_data(\n",
        "            log[\"epoch\"],\n",
        "            log.get(\"loss\"),\n",
        "            log.get(\"eval_loss\"),\n",
        "            log.get(\"eval_precision\"),\n",
        "            log.get(\"eval_recall\"),\n",
        "            log.get(\"eval_f1\"),\n",
        "            log.get(\"eval_accuracy\"),\n",
        "        )\n",
        "wandb.log({\"final_training_table\": final_table})\n",
        "\n",
        "val_preds = final_trainer.predict(tokenized[\"validation\"])\n",
        "val_pred_ids = np.argmax(val_preds.predictions, axis=1)\n",
        "cm = wandb.plot.confusion_matrix(\n",
        "    probs=None,\n",
        "    y_true=val_preds.label_ids,\n",
        "    preds=val_pred_ids,\n",
        "    class_names=target_labels\n",
        ")\n",
        "wandb.log({\"final_confusion_matrix\": cm})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BRTpAdBW72W"
      },
      "source": [
        "## Save predictions in files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CSppUv0W_sn"
      },
      "outputs": [],
      "source": [
        "val_preds = final_trainer.predict(tokenized[\"validation\"])\n",
        "val_pred_ids = np.argmax(val_preds.predictions, axis=1)\n",
        "\n",
        "val_texts = tokenized[\"validation\"][\"text\"]\n",
        "true_ids = val_preds.label_ids\n",
        "pred_ids = val_pred_ids\n",
        "\n",
        "pred_df = pd.DataFrame({\n",
        "    \"sentence\": val_texts,\n",
        "    \"actual_relation\": [id2label[i] for i in true_ids],\n",
        "    \"predicted_relation\": [id2label[i] for i in pred_ids],\n",
        "})\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/project_files/data/processed/re_val_predictions.csv\"\n",
        "json_path = \"/content/drive/MyDrive/project_files/data/processed/re_val_predictions.json\"\n",
        "pred_df.to_csv(csv_path, index=False)\n",
        "pred_df.to_json(json_path, orient=\"records\", lines=True)\n",
        "\n",
        "wandb.save(csv_path)\n",
        "wandb.save(json_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w31clUEDCwDa"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv-kQwSiHiIq"
      },
      "outputs": [],
      "source": [
        "local_src = '/content/drive/MyDrive/project_files/checkpoints/RE/checkpoint-306'\n",
        "temp_dir = '/content/tmp_checkpoint-306'\n",
        "if os.path.exists(local_src):\n",
        "    shutil.copytree(local_src, temp_dir, dirs_exist_ok=True)\n",
        "    print(f\"Temporary copy of checkpoint created at {temp_dir}\")\n",
        "else:\n",
        "    print(f\"Error: local src {local_src} does not exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEN20VMfJleG"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PyBVEdlJl_Z"
      },
      "outputs": [],
      "source": [
        "dst = '/content/gdrive/MyDrive/project_files/checkpoints/RE/'\n",
        "os.makedirs(dst, exist_ok=True)\n",
        "shutil.copytree(temp_dir, os.path.join(dst, 'checkpoint-306'), dirs_exist_ok=True)\n",
        "\n",
        "print(f\"Checkpoint folder copied from temp to Drive at {dst}/checkpoint-306\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}