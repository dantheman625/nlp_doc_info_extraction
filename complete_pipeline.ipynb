{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantheman625/nlp_doc_info_extraction/blob/complete_pipe/complete_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB-78NmTAxDn"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJgrjPQwCqbh"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval scikit-learn datasets wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayfPLr3R_zLO"
      },
      "outputs": [],
      "source": [
        "print(\"hello world\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldcCU7mJPVll"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    LongformerTokenizerFast,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3DTsURh-79i"
      },
      "source": [
        "## Wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cu4-vLz_fT5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9cEecqA9pj"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "Import Challenge data set (Final_eval.json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCnzzC7f3NLs"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejY9V79h3JUb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive   # only in Colab; skip if youâ€™re on a different setup\n",
        "import os\n",
        "import json\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkCwny0seKjj"
      },
      "source": [
        "## Set Project folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqXgjBvS3T5h"
      },
      "outputs": [],
      "source": [
        "drive_folder = \"NLP_project_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aog3WRix3YZC"
      },
      "source": [
        "## Load file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nShjsVxM3X4E"
      },
      "outputs": [],
      "source": [
        "data_base_path= os.path.join('data/')\n",
        "model_base_path = 'models/checkpoints/'\n",
        "eval_path   = os.path.join(data_base_path, 'Final_eval.json')\n",
        "\n",
        "eval_data = []\n",
        "folder_path = f'{data_base_path}raw/dev'\n",
        "\n",
        "print(folder_path)\n",
        "\n",
        "# loop through all files in the given folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file_name in files:\n",
        "        with open(f\"{folder_path}/{file_name}\", \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for d in data:\n",
        "          eval_data.append(d)\n",
        "\n",
        "dataset = Dataset.from_list(eval_data)\n",
        "print(\"Sample example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PjxtL22yv4t"
      },
      "outputs": [],
      "source": [
        "entity_labels = dataset[0]['entity_label_set']  # list of entity types\n",
        "label_list = ['O'] + [f\"B-{l}\" for l in entity_labels] + [f\"I-{l}\" for l in entity_labels]\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm53dfjvAz-B"
      },
      "source": [
        "# Define models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti1frmdMPYUz"
      },
      "source": [
        "# Baseline models\n",
        "Define which model you used as a baseline model for the specific task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNJb31CMPda7"
      },
      "outputs": [],
      "source": [
        "baseline_ner_name = \"allenai/longformer-base-4096\"\n",
        "baseline_re_name = \"SpanBERT/spanbert-large-cased\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KhLAFzTA13R"
      },
      "source": [
        "# Trained models\n",
        "\n",
        "Define your trained model for the specific task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8IAyiZ1RhEO"
      },
      "outputs": [],
      "source": [
        "trained_ner_name = f\"{model_base_path}longformer_big_tuned\"\n",
        "trained_re_name = \"/path/to/your/re/checkpoint\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_5UIxiOA3dF"
      },
      "source": [
        "# Model selection\n",
        "\n",
        "Which model for NER, which for RE? -> Combination untrained/ untrained, trained/ trained, untrained/ trained, trained/ untrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysRNjlYDRBBl"
      },
      "source": [
        "## Both baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM-hBYn1Q1EV"
      },
      "outputs": [],
      "source": [
        "ner_model_name = baseline_ner_name\n",
        "re_model_name  = baseline_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjl7gqUZRD64"
      },
      "source": [
        "## Both trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wQL6dNbRWgd"
      },
      "outputs": [],
      "source": [
        "ner_model_name = trained_ner_name\n",
        "re_model_name  = trained_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jWBBmYNRHGC"
      },
      "source": [
        "## NER: trained, RE: baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOqWb9yHRS_c"
      },
      "outputs": [],
      "source": [
        "ner_model_name = trained_ner_name\n",
        "re_model_name  = baseline_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs8GFyxCRKS1"
      },
      "source": [
        "## NER: baseline, RE: trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqCg5zyaRZ3a"
      },
      "outputs": [],
      "source": [
        "ner_model_name = baseline_ner_name\n",
        "re_model_name  = trained_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV83Si0vUPvB"
      },
      "source": [
        "# Load Models and Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RHQ1PzcUTJJ"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZe36BXf1GIQ"
      },
      "outputs": [],
      "source": [
        "print(ner_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuPIe9-9UVUk"
      },
      "outputs": [],
      "source": [
        "## Update once Daniel has finished setup\n",
        "ner_tokenizer = LongformerTokenizerFast.from_pretrained(baseline_ner_name)\n",
        "ner_model     = AutoModelForTokenClassification.from_pretrained(\n",
        "    ner_model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "ner_pipe = pipeline(\n",
        "    'ner',\n",
        "    model=ner_model,\n",
        "    tokenizer=ner_tokenizer,\n",
        "    device=-1,\n",
        "    aggregation_strategy='simple'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnvQ9uhbUXnt"
      },
      "source": [
        "## RE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlLQJe-CUSKx"
      },
      "outputs": [],
      "source": [
        "re_tokenizer  = AutoTokenizer.from_pretrained(ner_model_name)\n",
        "re_model      = AutoModelForSequenceClassification.from_pretrained(re_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91WtT1p3-bm9"
      },
      "source": [
        "#Initialize Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsgYrS88-fkj"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"model-eval\",\n",
        "    name=f\"eval_{ner_model_name.split('/')[-1]}_{re_model_name.split('/')[-1]}\",\n",
        "    config={\n",
        "        \"ner_model\": ner_model_name,\n",
        "        \"re_model\": re_model_name,\n",
        "        \"dataset\": \"Final_eval.json\",\n",
        "        \"batch_size\": 32,\n",
        "        \"max_length\": 256,\n",
        "        \"seed\": 42,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jf36s9hA7SY"
      },
      "source": [
        "# NER Eval\n",
        "\n",
        "Output: Entity file -> content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM1Ifo4Pyv4z"
      },
      "outputs": [],
      "source": [
        "ner_val_results = []\n",
        "for idx, example in enumerate(eval_data[:2]):\n",
        "    preds = ner_pipe(example['doc'])\n",
        "    ner_val_results.append({\n",
        "        'index': idx,\n",
        "        'domain': example.get('domain'),\n",
        "        'doc_title': example.get('title', f'doc_{idx}'),\n",
        "        'entities': preds,\n",
        "        'doc': example.get('doc')\n",
        "    })\n",
        "\n",
        "print(ner_val_results[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaorJn5gDiot"
      },
      "outputs": [],
      "source": [
        "def convert_numpy_floats(obj):\n",
        "    if isinstance(obj, np.float32):\n",
        "        return float(obj)\n",
        "    raise TypeError\n",
        "\n",
        "def save_model_output(output, output_path):\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(output, f, ensure_ascii=False, indent=2, default=convert_numpy_floats)\n",
        "    print(f\"Saved validation NER predictions to {output_path}\")\n",
        "\n",
        "def compute_f1(predictions_file_path, output_path, validation_dataset):\n",
        "  # Load saved predictions\n",
        "  with open(predictions_file_path, 'r') as f:\n",
        "      saved_preds = json.load(f)\n",
        "\n",
        "  # Prepare gold and predicted lists\n",
        "  all_gold = []\n",
        "  all_pred = []\n",
        "  for pred in saved_preds:\n",
        "      idx = pred['index']\n",
        "      gold_entities = validation_dataset[idx]['entities']\n",
        "      # flatten gold mentions: (mention_text, type)\n",
        "      gold_set = set()\n",
        "      for ent in gold_entities:\n",
        "          for m in ent['mentions']:\n",
        "              gold_set.add((m, ent['type']))\n",
        "      # flatten predicted mentions: pipeline outputs 'word' and 'entity_group'\n",
        "      pred_list = pred['entities']\n",
        "      pred_set = set()\n",
        "      for p in pred_list:\n",
        "          w = p.get('word').lstrip()\n",
        "          et = p.get('entity_group')\n",
        "          pred_set.add((w, et))\n",
        "      # Append to global lists\n",
        "      all_gold.append(gold_set)\n",
        "      all_pred.append(pred_set)\n",
        "\n",
        "  # Compute micro-level counts\n",
        "  tp = 0\n",
        "  pred_count = 0\n",
        "  gold_count = 0\n",
        "  for gold_set, pred_set in zip(all_gold, all_pred):\n",
        "      tp += len(gold_set & pred_set)\n",
        "      pred_count += len(pred_set)\n",
        "      gold_count += len(gold_set)\n",
        "\n",
        "  precision = tp / pred_count if pred_count > 0 else 0.0\n",
        "  recall = tp / gold_count if gold_count > 0 else 0.0\n",
        "  f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "  # Print and save metrics\n",
        "  metrics = {\n",
        "      'precision': precision,\n",
        "      'recall': recall,\n",
        "      'f1': f1,\n",
        "      'true_positives': tp,\n",
        "      'predicted': pred_count,\n",
        "      'gold': gold_count\n",
        "  }\n",
        "  print(\"NER Validation Mention-level Metrics:\")\n",
        "  print(metrics)\n",
        "\n",
        "  # Save metrics to JSON\n",
        "  with open(output_path, 'w') as f:\n",
        "      json.dump(metrics, f, indent=2)\n",
        "  print(f\"Saved evaluation metrics to {output_path}\")\n",
        "\n",
        "new_folder_path = f'logs/ner/{ner_model_name}'\n",
        "\n",
        "if not os.path.exists(new_folder_path):\n",
        "    os.makedirs(new_folder_path)\n",
        "    print(f\"Folder created at: {new_folder_path}\")\n",
        "else:\n",
        "    print(f\"Folder already exists: {new_folder_path}\")\n",
        "\n",
        "output_path = f'{new_folder_path}/predictions.json'\n",
        "save_model_output(ner_val_results, output_path)\n",
        "\n",
        "scores_path = f'{new_folder_path}/scores.json'\n",
        "compute_f1(output_path, scores_path, eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_g6H9G--tRp"
      },
      "source": [
        "## Log Metrics in Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcVR516C-piY"
      },
      "outputs": [],
      "source": [
        "from seqeval.metrics import precision_score as ner_prec, recall_score as ner_rec, f1_score as ner_f1\n",
        "prec_ner = ner_prec(true_ner_labels, pred_ner_labels)\n",
        "rec_ner  = ner_rec(true_ner_labels, pred_ner_labels)\n",
        "f1_ner   = ner_f1(true_ner_labels, pred_ner_labels)\n",
        "\n",
        "wandb.log({\n",
        "    \"ner/precision\": prec_ner,\n",
        "    \"ner/recall\":    rec_ner,\n",
        "    \"ner/f1\":        f1_ner,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HxNSKq-A8by"
      },
      "source": [
        "# RE Eval\n",
        "\n",
        "Input: Entity file, original challenge test file -> matching of entities to sentences (siehe wa) -> Liste mit dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLcf67F-6cJ0"
      },
      "source": [
        "## Extract unique label values for matching to docred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3Uhvztr6iDL"
      },
      "outputs": [],
      "source": [
        "unique_labels = set()\n",
        "for item in eval_data:\n",
        "    unique_labels.update(item.get(\"label_set\", []))\n",
        "\n",
        "# If you want them sorted for readability:\n",
        "unique_labels = sorted(unique_labels)\n",
        "\n",
        "print(unique_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nc1v9Gu-w_9"
      },
      "source": [
        "## Log metrics in Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIXDmaMeFJiM"
      },
      "outputs": [],
      "source": [
        "# placeholder\n",
        "true_re_labels = [\"O\", \"O\"]\n",
        "pred_re_labels = [\"O\", \"O\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUaHgZmn-05m"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "prec_re, rec_re, f1_re, _ = precision_recall_fscore_support(\n",
        "    true_re_labels, pred_re_labels, average=\"micro\"\n",
        ")\n",
        "\n",
        "wandb.log({\n",
        "    \"re/precision\": prec_re,\n",
        "    \"re/recall\":    rec_re,\n",
        "    \"re/f1\":        f1_re,\n",
        "})\n",
        "\n",
        "\n",
        "summary_table = wandb.Table(\n",
        "    columns=[\n",
        "      \"ner_precision\",\n",
        "      \"ner_recall\",\n",
        "      \"ner_f1\",\n",
        "      \"re_precision\",\n",
        "      \"re_recall\",\n",
        "      \"re_f1\"\n",
        "    ],\n",
        "    data=[[prec_ner, rec_ner, f1_ner, prec_re, rec_re, f1_re]]\n",
        ")\n",
        "wandb.log({\"metrics_summary\": summary_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4yVOJzh_vwb"
      },
      "source": [
        "Wrap Up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J8TqQZE_xCM"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_project_p311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
