{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantheman625/nlp_doc_info_extraction/blob/complete_pipe/complete_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB-78NmTAxDn"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJgrjPQwCqbh"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval scikit-learn datasets wandb nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldcCU7mJPVll"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    LongformerTokenizerFast,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3DTsURh-79i"
      },
      "source": [
        "## Wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cu4-vLz_fT5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9cEecqA9pj"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "Import Challenge data set (Final_eval.json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCnzzC7f3NLs"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejY9V79h3JUb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive   # only in Colab; skip if you’re on a different setup\n",
        "import os\n",
        "import json\n",
        "\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkCwny0seKjj"
      },
      "source": [
        "## Set Project folder in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqXgjBvS3T5h"
      },
      "outputs": [],
      "source": [
        "drive_folder = \"NLP_project_data\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aog3WRix3YZC"
      },
      "source": [
        "## Load file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nShjsVxM3X4E"
      },
      "outputs": [],
      "source": [
        "base_path   = os.path.join('drive/MyDrive/', drive_folder)\n",
        "eval_path   = os.path.join(base_path, 'Final_eval.json')\n",
        "\n",
        "eval_data = []\n",
        "folder_path = f'{base_path}/raw/dev'\n",
        "\n",
        "print(folder_path)\n",
        "\n",
        "# loop through all files in the given folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file_name in files:\n",
        "        with open(f\"{folder_path}/{file_name}\", \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for d in data:\n",
        "          eval_data.append(d)\n",
        "\n",
        "dataset = Dataset.from_list(eval_data)\n",
        "print(\"Sample example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PjxtL22yv4t"
      },
      "outputs": [],
      "source": [
        "entity_labels = dataset[0]['entity_label_set']  # list of entity types\n",
        "label_list = ['O'] + [f\"B-{l}\" for l in entity_labels] + [f\"I-{l}\" for l in entity_labels]\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm53dfjvAz-B"
      },
      "source": [
        "# Define models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti1frmdMPYUz"
      },
      "source": [
        "# Baseline models\n",
        "Define which model you used as a baseline model for the specific task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNJb31CMPda7"
      },
      "outputs": [],
      "source": [
        "baseline_ner_name = \"allenai/longformer-base-4096\"\n",
        "baseline_re_name = \"SpanBERT/spanbert-large-cased\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KhLAFzTA13R"
      },
      "source": [
        "# Trained models\n",
        "\n",
        "Define your trained model for the specific task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8IAyiZ1RhEO"
      },
      "outputs": [],
      "source": [
        "trained_ner_name = f\"{base_path}/models/longformer/1/\"\n",
        "trained_ner_name = f\"{base_path}/Model_checkpoints/longformer_big_tuned_35\" #Nina only\n",
        "trained_re_name = f\"{base_path}/Model_checkpoints/checkpoint-306\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_5UIxiOA3dF"
      },
      "source": [
        "# Model selection\n",
        "\n",
        "Which model for NER, which for RE? -> Combination untrained/ untrained, trained/ trained, untrained/ trained, trained/ untrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysRNjlYDRBBl"
      },
      "source": [
        "## Both baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM-hBYn1Q1EV"
      },
      "outputs": [],
      "source": [
        "ner_model_name = baseline_ner_name\n",
        "re_model_name  = baseline_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjl7gqUZRD64"
      },
      "source": [
        "## Both trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wQL6dNbRWgd"
      },
      "outputs": [],
      "source": [
        "ner_model_name = trained_ner_name\n",
        "re_model_name  = trained_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jWBBmYNRHGC"
      },
      "source": [
        "## NER: trained, RE: baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOqWb9yHRS_c"
      },
      "outputs": [],
      "source": [
        "ner_model_name = trained_ner_name\n",
        "re_model_name  = baseline_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs8GFyxCRKS1"
      },
      "source": [
        "## NER: baseline, RE: trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqCg5zyaRZ3a"
      },
      "outputs": [],
      "source": [
        "ner_model_name = baseline_ner_name\n",
        "re_model_name  = trained_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV83Si0vUPvB"
      },
      "source": [
        "# Load Models and Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RHQ1PzcUTJJ"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZe36BXf1GIQ"
      },
      "outputs": [],
      "source": [
        "print(ner_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuPIe9-9UVUk"
      },
      "outputs": [],
      "source": [
        "## Update once Daniel has finished setup\n",
        "ner_tokenizer = LongformerTokenizerFast.from_pretrained(baseline_ner_name)\n",
        "ner_model     = AutoModelForTokenClassification.from_pretrained(\n",
        "    ner_model_name\n",
        ")\n",
        "\n",
        "ner_pipe = pipeline(\n",
        "    'ner',\n",
        "    model=ner_model,\n",
        "    tokenizer=ner_tokenizer,\n",
        "    device=-1,\n",
        "    aggregation_strategy='simple'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnvQ9uhbUXnt"
      },
      "source": [
        "## RE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlLQJe-CUSKx"
      },
      "outputs": [],
      "source": [
        "re_tokenizer  = AutoTokenizer.from_pretrained(re_model_name)\n",
        "if re_model_name == baseline_re_name:\n",
        "    # baseline: override classification head to fixed number of labels\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        re_model_name,\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    )\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        re_model_name,\n",
        "        config=cfg\n",
        "    )\n",
        "    print(f\"Loaded baseline RE model '{re_model_name}' with overridden head size num_labels={re_model.config.num_labels}\")\n",
        "else:\n",
        "    # trained: load checkpoint head as-is\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(re_model_name)\n",
        "    print(f\"Loaded trained RE model '{re_model_name}' with head size num_labels={re_model.config.num_labels}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91WtT1p3-bm9"
      },
      "source": [
        "#Initialize Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsgYrS88-fkj"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"model-eval\",\n",
        "    name=f\"eval_{ner_model_name.split('/')[-1]}_{re_model_name.split('/')[-1]}\",\n",
        "    config={\n",
        "        \"ner_model\": ner_model_name,\n",
        "        \"re_model\": re_model_name,\n",
        "        \"dataset\": \"Final_eval.json\",\n",
        "        \"batch_size\": 32,\n",
        "        \"max_length\": 256,\n",
        "        \"seed\": 42,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jf36s9hA7SY"
      },
      "source": [
        "# NER Eval\n",
        "\n",
        "Output: Entity file -> content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM1Ifo4Pyv4z"
      },
      "outputs": [],
      "source": [
        "ner_val_results = []\n",
        "for idx, example in enumerate(eval_data):\n",
        "    preds = ner_pipe(example['doc'])\n",
        "    ner_val_results.append({\n",
        "        'domain': example.get('domain'),\n",
        "        'doc_title': example.get('title', f'doc_{idx}'),\n",
        "        'entities': preds,\n",
        "        'doc': example.get('doc')\n",
        "    })\n",
        "\n",
        "print(ner_val_results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0KaOyyVq1nI"
      },
      "source": [
        "Print NER Output for Nina to check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGBCYBJCq3hW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "with open(f'/content/ner_val_results.json','w') as f:\n",
        "    json.dump(\n",
        "        ner_val_results,\n",
        "        f,\n",
        "        default=lambda o: o.item() if isinstance(o, np.generic) else o\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaorJn5gDiot"
      },
      "outputs": [],
      "source": [
        "# 2. Index them by (domain, title) for easy lookup\n",
        "gt_index = {(ex['domain'], ex['title']): ex for ex in eval_data}\n",
        "pred_index = {(p['domain'], p['doc_title']): p for p in ner_val_results}\n",
        "\n",
        "true_ner_labels = []\n",
        "pred_ner_labels = []\n",
        "\n",
        "for key, gt in gt_index.items():\n",
        "    pred = pred_index.get(key)\n",
        "    if pred is None:\n",
        "        continue\n",
        "\n",
        "    text = gt['doc']\n",
        "    # simple whitespace tokenization\n",
        "    tokens = text.split()\n",
        "    n = len(tokens)\n",
        "\n",
        "    # map each character position → token idx\n",
        "    char2tok = {}\n",
        "    offset = 0\n",
        "    for i, tok in enumerate(tokens):\n",
        "        start = text.find(tok, offset)\n",
        "        end = start + len(tok)\n",
        "        for c in range(start, end):\n",
        "            char2tok[c] = i\n",
        "        offset = end\n",
        "\n",
        "    # initialize all O’s\n",
        "    true_labels = ['O'] * n\n",
        "    pred_labels = ['O'] * n\n",
        "\n",
        "    # 3. Fill in ground-truth labels\n",
        "    for ent in gt['entities']:\n",
        "        ent_type = ent['type']\n",
        "        for mention in ent['mentions']:\n",
        "            start = text.find(mention)\n",
        "            while start != -1:\n",
        "                end = start + len(mention)\n",
        "                t0 = char2tok.get(start)\n",
        "                t1 = char2tok.get(end-1)\n",
        "                if t0 is not None and t1 is not None:\n",
        "                    true_labels[t0] = f'B-{ent_type}'\n",
        "                    for t in range(t0+1, t1+1):\n",
        "                        true_labels[t] = f'I-{ent_type}'\n",
        "                start = text.find(mention, end)\n",
        "\n",
        "    # 4. Fill in predicted labels\n",
        "    for ent in pred['entities']:\n",
        "        t0 = char2tok.get(ent['start'])\n",
        "        t1 = char2tok.get(ent['end'] - 1)\n",
        "        et = ent['entity_group']\n",
        "        if t0 is not None and t1 is not None:\n",
        "            pred_labels[t0] = f'B-{et}'\n",
        "            for t in range(t0+1, t1+1):\n",
        "                pred_labels[t] = f'I-{et}'\n",
        "\n",
        "    true_ner_labels.append(true_labels)\n",
        "    pred_ner_labels.append(pred_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_g6H9G--tRp"
      },
      "source": [
        "## Log Metrics in Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcVR516C-piY"
      },
      "outputs": [],
      "source": [
        "from seqeval.metrics import precision_score as ner_prec, recall_score as ner_rec, f1_score as ner_f1\n",
        "prec_ner = ner_prec(true_ner_labels, pred_ner_labels)\n",
        "rec_ner  = ner_rec(true_ner_labels, pred_ner_labels)\n",
        "f1_ner   = ner_f1(true_ner_labels, pred_ner_labels)\n",
        "\n",
        "print(prec_ner)\n",
        "print(rec_ner)\n",
        "print(f1_ner)\n",
        "\n",
        "wandb.log({\n",
        "    \"ner/precision\": prec_ner,\n",
        "    \"ner/recall\":    rec_ner,\n",
        "    \"ner/f1\":        f1_ner,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HxNSKq-A8by"
      },
      "source": [
        "# RE Eval\n",
        "\n",
        "Input: Entity file, original challenge test file -> matching of entities to sentences (siehe wa) -> Liste mit dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCXERBrXzR5x"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_Bym75fqMYe"
      },
      "outputs": [],
      "source": [
        "# Mapping: challenge labels (Set 2) → docred labels (Set 1)\n",
        "mapping_challenge_to_docred = {\n",
        "    # ── Direct one-to-ones ───────────────────────────────────────────────────────\n",
        "    \"Affiliation\":                         \"member of\",\n",
        "    \"ApprovedBy\":                          \"ApprovedBy\",\n",
        "    \"Author\":                              \"author\",\n",
        "    \"AwardReceived\":                       \"award received\",\n",
        "    \"BasedOn\":                             \"BasedOn\",\n",
        "    \"Capital\":                             \"capital\",\n",
        "    \"Causes\":                              \"Causes\",\n",
        "    \"Continent\":                           \"continent\",\n",
        "    \"ContributedToCreativeWork\":           \"ContributedToCreativeWork\",\n",
        "    \"Country\":                             \"country\",\n",
        "    \"CountryOfCitizenship\":                \"country of citizenship\",\n",
        "    \"Creator\":                             \"creator\",\n",
        "    \"Developer\":                           \"developer\",\n",
        "    \"DifferentFrom\":                       \"DifferentFrom\",\n",
        "    \"Director\":                            \"director\",\n",
        "    \"EducatedAt\":                          \"educated at\",\n",
        "    \"Employer\":                            \"employer\",\n",
        "    \"FieldOfWork\":                         \"FieldOfWork\",\n",
        "    \"FollowedBy\":                          \"followed by\",\n",
        "    \"Follows\":                             \"follows\",\n",
        "    \"Founded\":                             \"founded\",\n",
        "    \"FoundedBy\":                           \"founded by\",\n",
        "    \"HasCause\":                            \"HasCause\",\n",
        "    \"HasEffect\":                           \"HasEffect\",\n",
        "    \"HasPart\":                             \"HasPart\",\n",
        "    \"HasWorksInTheCollection\":             \"HasWorksInTheCollection\",\n",
        "    \"InfluencedBy\":                        \"influenced by\",\n",
        "    \"IssuedBy\":                            \"IssuedBy\",\n",
        "    \"LocatedIn\":                           \"located in the administrative territorial entity\",\n",
        "    \"Location\":                            \"location\",\n",
        "    \"MemberOf\":                            \"member of\",\n",
        "    \"NamedBy\":                             \"NamedBy\",\n",
        "    \"NominatedFor\":                        \"nominated for\",\n",
        "    \"OfficialLanguage\":                    \"official language\",\n",
        "    \"OwnedBy\":                             \"owned by\",\n",
        "    \"OwnerOf\":                             \"owner of\",\n",
        "    \"ParentOrganization\":                  \"parent organization\",\n",
        "    \"PartOf\":                              \"part of\",\n",
        "    \"Partner\":                             \"partner\",\n",
        "    \"PlaceOfBirth\":                        \"place of birth\",\n",
        "    \"PositionHeld\":                        \"position held\",\n",
        "    \"PublishedIn\":                         \"PublishedIn\",\n",
        "    \"Replaces\":                            \"replaces\",\n",
        "    \"SaidToBeTheSameAs\":                   \"SaidToBeTheSameAs\",\n",
        "    \"Studies\":                             \"Studies\",\n",
        "    \"UsedBy\":                              \"UsedBy\",\n",
        "    \"Uses\":                                \"Uses\",\n",
        "    \"WorkLocation\":                        \"work location\",\n",
        "\n",
        "    # ── Very close synonyms ────────────────────────────────────────────────────\n",
        "    \"LanguageOfWorkOrName\":                \"original language of work\",\n",
        "    \"LanguageUsed\":                        \"languages spoken, written or signed\",\n",
        "    \"OriginalLanguageOfFilmOrTvShow\":      \"original language of work\",\n",
        "    \"PartyChiefRepresentative\":            \"head of government\",\n",
        "    \"PrimeFactor\":                         \"part of\",\n",
        "    \"TwinnedAdministrativeBody\":           \"sister city\",\n",
        "\n",
        "    # ── Functional / looser mappings ──────────────────────────────────────────\n",
        "    \"AcademicDegree\":                      \"educated at\",\n",
        "    \"AdjacentStation\":                     \"shares border with\",\n",
        "    \"AppliesToPeople\":                     \"applies to jurisdiction\",\n",
        "    \"CitesWork\":                           \"present in work\",\n",
        "    \"ContainsAdministrativeTerritorialEntity\":     \"contains administrative territorial entity\",\n",
        "    \"ContainsTheAdministrativeTerritorialEntity\":  \"contains administrative territorial entity\",\n",
        "    \"DiplomaticRelation\":                  \"conflict\",\n",
        "    \"HasQuality\":                          \"genre\",\n",
        "    \"InOppositionTo\":                      \"separated from\",\n",
        "    \"InspiredBy\":                          \"BasedOn\",\n",
        "    \"InterestedIn\":                        \"Studies\",\n",
        "    \"NamedAfter\":                          \"NamedBy\",\n",
        "    \"NativeLanguage\":                      \"languages spoken, written or signed\",\n",
        "    \"OperatingSystem\":                     \"platform\",\n",
        "    \"PhysicallyInteractsWith\":             \"shares border with\",\n",
        "    \"PracticedBy\":                         \"UsedBy\",\n",
        "    \"PresentedIn\":                         \"present in work\",\n",
        "    \"Promoted\":                            \"HasEffect\",\n",
        "    \"RegulatedBy\":                         \"IssuedBy\",\n",
        "    \"SharesBorderWith\":                    \"shares border with\",\n",
        "    \"SignificantEvent\":                    \"location\",\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "# make sure you have punkt\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "candidates = []\n",
        "for doc in ner_val_results:\n",
        "    text  = doc[\"doc\"]\n",
        "    title = doc[\"doc_title\"].strip().lower()\n",
        "\n",
        "    # 1) split into sentences AND remember their character offsets\n",
        "    sentences = sent_tokenize(text)\n",
        "    offsets = []\n",
        "    cursor = 0\n",
        "    for s in sentences:\n",
        "        # find s at or after cursor\n",
        "        start = text.find(s, cursor)\n",
        "        end   = start + len(s)\n",
        "        offsets.append((s, start, end))\n",
        "        cursor = end\n",
        "\n",
        "    # 2) build a flat list of all NER spans in this doc\n",
        "    #    (word, start, end, entity_group)\n",
        "    mentions = [\n",
        "      (ent[\"word\"].strip(), ent[\"start\"], ent[\"end\"], ent[\"entity_group\"])\n",
        "      for ent in doc[\"entities\"]\n",
        "    ]\n",
        "\n",
        "    # 3) for each sentence, collect the mentions whose spans fall into it\n",
        "    for sent, s_start, s_end in offsets:\n",
        "        sent_mentions = [\n",
        "          (w, a, b, label)\n",
        "          for (w,a,b,label) in mentions\n",
        "          if s_start <= a < s_end\n",
        "        ]\n",
        "\n",
        "        # if fewer than two mentions, nothing to pair\n",
        "        if len(sent_mentions) < 2:\n",
        "            continue\n",
        "\n",
        "        # 4) enumerate all unordered pairs in this sentence\n",
        "        for i in range(len(sent_mentions)):\n",
        "            w1, a1, b1, label1 = sent_mentions[i]\n",
        "            for j in range(i+1, len(sent_mentions)):\n",
        "                w2, a2, b2, label2 = sent_mentions[j]\n",
        "\n",
        "                # wrap them in the text snippet\n",
        "                # note: replace only the first occurrence inside `sent`\n",
        "                snippet = sent.replace(w1, f\"[E1]{w1}[/E1]\", 1) \\\n",
        "                              .replace(w2, f\"[E2]{w2}[/E2]\", 1)\n",
        "\n",
        "                candidates.append({\n",
        "                  \"doc_title\":       title,\n",
        "                  \"text\":            snippet,\n",
        "                  \"entity1_label\":   w1,\n",
        "                  \"entity2_label\":   w2,\n",
        "                  # placeholder—no gold lookup here:\n",
        "                  \"relation_label\":  \"no_relation\"\n",
        "                })\n",
        "\n",
        "# now `candidates` is all of your sentence-level pairs.\n",
        "# you can turn it into an HF Dataset and, if you still want,\n",
        "# overlay the gold-triples on top of it:\n",
        "\n",
        "from datasets import Dataset\n",
        "ds = Dataset.from_list(candidates)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- NEW: print sizes ---\n",
        "print(f\"Built {len(candidates)} candidate pairs\")\n",
        "print(f\"HF Dataset contains {len(ds)} rows\")\n",
        "\n",
        "\n",
        "# 1) overwrite re_val_ds\n",
        "re_val_ds = ds\n"
      ],
      "metadata": {
        "id": "R7LZyGD8mdsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer option"
      ],
      "metadata": {
        "id": "E5zamdf_LC0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 2) load RE model & tokenizer (baseline vs. trained)\n",
        "re_tokenizer = AutoTokenizer.from_pretrained(re_model_name)\n",
        "if re_model_name == baseline_re_name:\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        re_model_name,\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    )\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        re_model_name, config=cfg\n",
        "    )\n",
        "else:\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(re_model_name)\n",
        "\n",
        "# 3) tokenize\n",
        "def tokenize_fn(batch):\n",
        "    return re_tokenizer(batch['text'],\n",
        "                        padding='max_length',\n",
        "                        truncation=True,\n",
        "                        max_length=256)\n",
        "tokenized_val = re_val_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "# 4) setup Trainer without metrics\n",
        "from transformers import Trainer, TrainingArguments\n",
        "eval_args = TrainingArguments(\n",
        "    output_dir='/content/re_predict_output',\n",
        "    per_device_eval_batch_size=32,\n",
        "    do_train=False,\n",
        "    do_eval=False,       # ← no evaluation\n",
        "    logging_dir='/content/logs',\n",
        "    report_to='wandb'\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=re_model,\n",
        "    args=eval_args,\n",
        "    tokenizer=re_tokenizer\n",
        ")\n",
        "\n",
        "# 5) predict\n",
        "preds_output = trainer.predict(tokenized_val)\n",
        "logits = preds_output.predictions    # shape [N, num_labels]\n",
        "pred_ids = logits.argmax(axis=-1)\n",
        "\n",
        "# pick id2label mapping\n",
        "if hasattr(re_model.config, 'id2label') and re_model.config.id2label:\n",
        "    pred_id2label = {int(k):v for k,v in re_model.config.id2label.items()}\n",
        "else:\n",
        "    pred_id2label = id2label\n",
        "\n",
        "# 6) save predictions\n",
        "import json, os\n",
        "safe = os.path.basename(re_model_name.rstrip('/'))\n",
        "out_path = f'/content/re_{safe}_candidates_with_preds.json'\n",
        "outputs = []\n",
        "for ex, pid in zip(re_val_ds, pred_ids):\n",
        "    outputs.append({\n",
        "      'text':               ex['text'],\n",
        "      'entity1_label':      ex['entity1_label'],\n",
        "      'entity2_label':      ex['entity2_label'],\n",
        "      'predicted_relation': pred_id2label.get(int(pid), 'UNKNOWN')\n",
        "    })\n",
        "with open(out_path, 'w') as f:\n",
        "    json.dump(outputs, f, indent=2)\n",
        "print(f\"Wrote {len(outputs)} predictions to {out_path}\")\n"
      ],
      "metadata": {
        "id": "jkcuuyERo1Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline option"
      ],
      "metadata": {
        "id": "wZaCjxABLFdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# 1) Load RE model & tokenizer (baseline vs. trained)\n",
        "re_tokenizer = AutoTokenizer.from_pretrained(re_model_name)\n",
        "if re_model_name == baseline_re_name:\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        re_model_name,\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    )\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        re_model_name, config=cfg\n",
        "    )\n",
        "    print(f\"> Loaded baseline RE model '{re_model_name}' with head size {re_model.config.num_labels}\")\n",
        "else:\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(re_model_name)\n",
        "    print(f\"> Loaded trained RE model '{re_model_name}' with head size {re_model.config.num_labels}\")\n",
        "\n",
        "# 2) Build a pipeline\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "re_pipe = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=re_model,\n",
        "    tokenizer=re_tokenizer,\n",
        "    device=device,\n",
        "    return_all_scores=False,  # just top label+score\n",
        ")\n",
        "\n",
        "# 3) Run predictions\n",
        "outputs = []\n",
        "for ex in re_val_ds:\n",
        "    # pipeline will automatically truncate/pad\n",
        "    pred = re_pipe(ex[\"text\"])[0]\n",
        "    outputs.append({\n",
        "        \"text\":               ex[\"text\"],\n",
        "        \"entity1_label\":      ex[\"entity1_label\"],\n",
        "        \"entity2_label\":      ex[\"entity2_label\"],\n",
        "        \"predicted_relation\": pred[\"label\"],\n",
        "        \"score\":              pred[\"score\"],\n",
        "    })\n",
        "\n",
        "# 4) Save to disk\n",
        "safe = os.path.basename(re_model_name.rstrip(\"/\"))\n",
        "out_path = f\"/content/re_{safe}_candidates_with_preds.json\"\n",
        "with open(out_path, \"w\") as f:\n",
        "    json.dump(outputs, f, indent=2)\n",
        "\n",
        "print(f\"Wrote {len(outputs)} predictions to {out_path}\")\n"
      ],
      "metadata": {
        "id": "pP9EnaXmJdcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "# — assume mapping_challenge_to_docred is already defined above —\n",
        "# — and `dataset` is your HF Dataset built from Final_eval.json —\n",
        "\n",
        "# 1) Build a lookup from (head, tail) → mapped_docred_label\n",
        "gold_map = {}\n",
        "for meta in dataset:\n",
        "    for t in meta.get(\"triples\", []):\n",
        "        h = t[\"head\"].strip().lower()\n",
        "        te = t[\"tail\"].strip().lower()\n",
        "        mapped = mapping_challenge_to_docred.get(t[\"relation\"])\n",
        "        if not mapped:\n",
        "            continue\n",
        "        # record both orders so swapped predictions still match\n",
        "        gold_map[(h, te)] = mapped\n",
        "        gold_map[(te, h)] = mapped\n",
        "\n",
        "# 2) Load your model’s predictions JSON\n",
        "predictions_path = out_path  # ← adjust path\n",
        "with open(predictions_path, \"r\") as f:\n",
        "    preds_list = json.load(f)\n",
        "\n",
        "gold_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for ex in preds_list:\n",
        "    e1 = ex[\"entity1_label\"].strip().lower()\n",
        "    e2 = ex[\"entity2_label\"].strip().lower()\n",
        "    # if that pair appears in gold_map, grab it; otherwise \"no_relation\"\n",
        "    gold = gold_map.get((e1, e2), \"no_relation\")\n",
        "    gold_labels.append(gold)\n",
        "    pred_labels.append(ex[\"predicted_relation\"])\n",
        "\n",
        "# 3) Print per‐class P/R/F₁\n",
        "all_labels = sorted(set(gold_labels) | set(pred_labels))\n",
        "print(classification_report(\n",
        "    gold_labels,\n",
        "    pred_labels,\n",
        "    labels=all_labels,\n",
        "    target_names=all_labels,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# 4) And the micro-averaged P/R/F₁\n",
        "p, r, f1, _ = precision_recall_fscore_support(\n",
        "    gold_labels, pred_labels, average=\"micro\", zero_division=0\n",
        ")\n",
        "print(f\"→ micro precision={p:.4f}   recall={r:.4f}   f1={f1:.4f}\")\n",
        "\n",
        "import json\n",
        "\n",
        "# 1) assume preds_list, gold_labels, pred_labels are already defined\n",
        "detailed = []\n",
        "for ex, gold, pred in zip(preds_list, gold_labels, pred_labels):\n",
        "    detailed.append({\n",
        "        \"text\":             ex[\"text\"],\n",
        "        \"entity1_label\":    ex[\"entity1_label\"],\n",
        "        \"entity2_label\":    ex[\"entity2_label\"],\n",
        "        \"gold_relation\":    gold,\n",
        "        \"predicted_relation\": pred\n",
        "    })\n",
        "\n",
        "# 2) write it out\n",
        "out_detail_path = \"/content/detailed_re_predictions.json\"\n",
        "with open(out_detail_path, \"w\") as f:\n",
        "    json.dump(detailed, f, indent=2)\n",
        "\n",
        "print(f\"Wrote {len(detailed)} detailed examples to {out_detail_path}\")\n"
      ],
      "metadata": {
        "id": "25l7-Zjsu7Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "import wandb\n",
        "\n",
        "# — assume you already have:\n",
        "#    gold_labels  : List[str]  # e.g. [\"HasPart\",\"no_relation\",…]\n",
        "#    pred_labels  : List[str]  # e.g. [\"HasPart\",\"part of\",…]\n",
        "#    prec_ner,rec_ner,f1_ner    # from your NER eval earlier\n",
        "\n",
        "# 1) pick only the labels that actually occur\n",
        "unique_labels = sorted(set(gold_labels) | set(pred_labels))\n",
        "\n",
        "# 2) build and log the detailed classification report\n",
        "report = classification_report(\n",
        "    gold_labels,\n",
        "    pred_labels,\n",
        "    labels=unique_labels,\n",
        "    target_names=unique_labels,\n",
        "    output_dict=True,\n",
        "    zero_division=0\n",
        ")\n",
        "wandb.log({\"classification_report\": report})\n",
        "\n",
        "# 3) compute & log the micro-averaged RE precision/recall/f1\n",
        "prec_re, rec_re, f1_re, _ = precision_recall_fscore_support(\n",
        "    gold_labels,\n",
        "    pred_labels,\n",
        "    labels=unique_labels,\n",
        "    average=\"micro\",\n",
        "    zero_division=0\n",
        ")\n",
        "wandb.log({\n",
        "    \"re/precision\": prec_re,\n",
        "    \"re/recall\":    rec_re,\n",
        "    \"re/f1\":        f1_re,\n",
        "})\n",
        "\n",
        "# 4) finally log your summary table combining NER+RE\n",
        "summary = wandb.Table(\n",
        "    columns=[\n",
        "      \"ner_precision\",\"ner_recall\",\"ner_f1\",\n",
        "      \"re_precision\",  \"re_recall\",  \"re_f1\"\n",
        "    ],\n",
        "    data=[[prec_ner, rec_ner, f1_ner, prec_re, rec_re, f1_re]]\n",
        ")\n",
        "wandb.log({\"metrics_summary\": summary})\n",
        "\n",
        "# 5) print out the summary metrics\n",
        "print(f\"NER   → precision: {prec_ner:.4f}, recall: {rec_ner:.4f}, f1: {f1_ner:.4f}\")\n",
        "print(f\"RE    → precision: {prec_re:.4f}, recall: {rec_re:.4f}, f1: {f1_re:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UC8Ep5z7znTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exclude no relation\n",
        "Since the RE model was only trained on docred labels and not no relation"
      ],
      "metadata": {
        "id": "QE5D-3uUQQ8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "# 1) load your detailed preds (with gold mapped labels)\n",
        "with open(\"/content/detailed_re_predictions.json\", \"r\") as f:\n",
        "    detailed = json.load(f)\n",
        "\n",
        "# 2) filter out all no_relation\n",
        "filtered = [ex for ex in detailed if ex[\"gold_relation\"] != \"no_relation\"]\n",
        "\n",
        "gold_filt = [ex[\"gold_relation\"]      for ex in filtered]\n",
        "pred_filt = [ex[\"predicted_relation\"] for ex in filtered]\n",
        "\n",
        "print(f\"→ {len(filtered)} positive examples (out of {len(detailed)})\\n\")\n",
        "\n",
        "# 3) per-class P/R/F₁ (excluding no_relation automatically, since we filtered it out)\n",
        "labels = sorted(set(gold_filt) | set(pred_filt))\n",
        "print(classification_report(\n",
        "    gold_filt,\n",
        "    pred_filt,\n",
        "    labels=labels,\n",
        "    target_names=labels,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# 4) micro-averaged over all non-no_relation examples\n",
        "p, r, f1, _ = precision_recall_fscore_support(\n",
        "    gold_filt,\n",
        "    pred_filt,\n",
        "    average=\"micro\",\n",
        "    zero_division=0\n",
        ")\n",
        "print(f\"→ POSITIVE-only micro precision={p:.4f}   recall={r:.4f}   f1={f1:.4f}\")\n",
        "# 2) write it out\n",
        "out_detail_positive_only_path = \"/content/detailed_re_predictions_positive_only.json\"\n",
        "with open(out_detail_positive_only_path, \"w\") as f:\n",
        "    json.dump(detailed, f, indent=2)\n",
        "\n",
        "print(f\"Wrote {len(detailed)} detailed examples to {out_detail_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "R0XbOT5bQWwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Old do not use"
      ],
      "metadata": {
        "id": "d8-ve0Bipe1F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJLZBSNIWK3n"
      },
      "outputs": [],
      "source": [
        "#Old do not use\n",
        "\n",
        "\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer\n",
        ")\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# === 0) Define fixed relation label mapping ===\n",
        "target_labels = sorted(set(mapping_challenge_to_docred.values()))\n",
        "label2id = {lbl: idx for idx, lbl in enumerate(target_labels)}\n",
        "id2label = {idx: lbl for lbl, idx in label2id.items()}\n",
        "print(f\"Using fixed RE labels (len={len(label2id)}): {label2id}\")\n",
        "\n",
        "# === 1) Load RE model & tokenizer with conditional head sizing ===\n",
        "def load_re_model(name_or_path, baseline_name):\n",
        "    # shared tokenizer across models\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name_or_path)\n",
        "    if name_or_path == baseline_name:\n",
        "        cfg = AutoConfig.from_pretrained(\n",
        "            name_or_path,\n",
        "            num_labels=len(label2id),\n",
        "            label2id=label2id,\n",
        "            id2label=id2label\n",
        "        )\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            name_or_path,\n",
        "            config=cfg\n",
        "        )\n",
        "        print(f\"Loaded baseline RE model '{name_or_path}' with overridden head size num_labels={model.config.num_labels}\")\n",
        "    else:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(name_or_path)\n",
        "        print(f\"Loaded trained RE model '{name_or_path}' with head size num_labels={model.config.num_labels}\")\n",
        "    return model, tokenizer\n",
        "\n",
        "re_model, re_tokenizer = load_re_model(re_model_name, baseline_re_name)\n",
        "\n",
        "# ensure NLTK sentence tokenizer is available\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# === 2) Index gold triples by normalized document title ===\n",
        "gold_by_doc = defaultdict(list)\n",
        "for meta in dataset:\n",
        "    key = meta[\"title\"].strip().lower()\n",
        "    for t in meta.get(\"triples\", []):\n",
        "        gold_by_doc[key].append(t)\n",
        "\n",
        "# === Helpers to split text into sentences and extract containing sentence ===\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def split_sentences(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "def get_sentence(text, a, b):\n",
        "    for sent in split_sentences(text):\n",
        "        idx = text.find(sent)\n",
        "        if idx <= a < idx + len(sent) and idx <= b < idx + len(sent):\n",
        "            return sent\n",
        "    # no single sentence contains both spans → skip example\n",
        "    return None\n",
        "\n",
        "# === 3) Build positive-only RE examples from NER outputs + gold triples ===\n",
        "examples = []\n",
        "for doc in ner_val_results:\n",
        "    title = doc['doc_title'].strip().lower()\n",
        "    text  = doc.get('doc', '')\n",
        "    spans = defaultdict(list)\n",
        "    for ent in doc.get('entities', []):\n",
        "        key = ent.get('word', '').strip().lower()\n",
        "        spans[key].append((ent['start'], ent['end']))\n",
        "    for head, rel, tail in [(t['head'], t['relation'], t['tail']) for t in gold_by_doc.get(title, [])]:\n",
        "        mapped = mapping_challenge_to_docred.get(rel)\n",
        "        if mapped is None:\n",
        "            continue\n",
        "        hsp = spans.get(head.lower().strip(), [])\n",
        "        tsp = spans.get(tail.lower().strip(), [])\n",
        "        if not (hsp and tsp):\n",
        "            continue\n",
        "        sent = get_sentence(text, hsp[0][0], tsp[0][0])\n",
        "        if sent is None:\n",
        "            continue\n",
        "        examples.append({\n",
        "            'RE_sentence':    sent,\n",
        "            'entity1_label':  head,\n",
        "            'entity2_label':  tail,\n",
        "            'relation_label': mapped\n",
        "        })\n",
        "print(f\"Built {len(examples)} RE examples across {len(gold_by_doc)} docs\")\n",
        "if not examples:\n",
        "    raise RuntimeError(\"No RE examples built; check your mapping and NER outputs.\")\n",
        "\n",
        "# === 4) Define transforms for HF Dataset ===\n",
        "def make_example(ex):\n",
        "    sent, e1, e2 = ex['RE_sentence'], ex['entity1_label'], ex['entity2_label']\n",
        "    # only wrap the single sentence containing both entities\n",
        "    text = sent.replace(e1, f\"[E1]{e1}[/E1]\").replace(e2, f\"[E2]{e2}[/E2]\")\n",
        "    return {\n",
        "        'text': text,\n",
        "        'entity1_label': e1,\n",
        "        'entity2_label': e2,\n",
        "        'relation_label': ex['relation_label'],\n",
        "    }\n",
        "\n",
        "def add_label_ids(ex):\n",
        "    return {'labels': label2id[ex['relation_label']]}\n",
        "\n",
        "# === 5) Build HF Dataset and apply transforms ===\n",
        "re_val_ds = Dataset.from_list(examples)\n",
        "print(\"Columns before mapping:\", re_val_ds.column_names)\n",
        "re_val_ds = re_val_ds.map(make_example, remove_columns=['RE_sentence'])\n",
        "re_val_ds = re_val_ds.map(add_label_ids)\n",
        "print(\"→ Prepared RE validation set:\", re_val_ds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6PL9C2O2kNY"
      },
      "source": [
        "### Build RE validation examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHCm2DcupBrX"
      },
      "source": [
        "### Create HF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvqazyZtbAJH"
      },
      "outputs": [],
      "source": [
        "# First, load the correct RE model & tokenizer (baseline vs. trained)\n",
        "re_tokenizer  = AutoTokenizer.from_pretrained(re_model_name)\n",
        "if re_model_name == baseline_re_name:\n",
        "    # baseline: override classification head to fixed number of labels\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        re_model_name,\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    )\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        re_model_name,\n",
        "        config=cfg\n",
        "    )\n",
        "    print(f\"Loaded baseline RE model '{re_model_name}' with overridden head size num_labels={re_model.config.num_labels}\")\n",
        "else:\n",
        "    # trained: load checkpoint head as-is\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(re_model_name)\n",
        "    print(f\"Loaded trained RE model '{re_model_name}' with head size num_labels={re_model.config.num_labels}\")\n",
        "\n",
        "# 7) Tokenization function using the newly loaded tokenizer\n",
        "def tokenize_fn(batch):\n",
        "    return re_tokenizer(\n",
        "        batch['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "# Apply tokenization to your validation dataset\n",
        "tokenized_val = re_val_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "# 8) Define evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1}\n",
        "\n",
        "# 9) Set up Trainer for evaluation\n",
        "eval_args = TrainingArguments(\n",
        "    output_dir='/content/re_eval_output',\n",
        "    per_device_eval_batch_size=32,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    logging_dir='/content/logs',\n",
        "    report_to='wandb'\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=re_model,\n",
        "    args=eval_args,\n",
        "    tokenizer=re_tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 10) Run evaluation\n",
        "eval_result = trainer.evaluate(eval_dataset=tokenized_val)\n",
        "print(\"🔍 RE Validation Results:\", eval_result)\n",
        "\n",
        "# 11) Generate and save predictions as before\n",
        "preds_output = trainer.predict(tokenized_val)\n",
        "pred_ids = np.argmax(preds_output.predictions, axis=-1)\n",
        "\n",
        "# Determine id2label mapping to use for predictions\n",
        "# If model config defines id2label, prefer that (e.g., for trained model with 87 classes)\n",
        "if hasattr(re_model.config, 'id2label') and re_model.config.id2label:\n",
        "    pred_id2label = re_model.config.id2label\n",
        "else:\n",
        "    pred_id2label = id2label\n",
        "\n",
        "# sanitize filename & write out JSON\n",
        "import os\n",
        "safe_model_name = os.path.basename(re_model_name.rstrip('/'))\n",
        "output_path = f'/content/re_{safe_model_name}_predictions.json'\n",
        "\n",
        "outputs = []\n",
        "for ex, pred in zip(re_val_ds, pred_ids):\n",
        "    # guard against out-of-range predictions\n",
        "    pred_label = pred_id2label.get(pred, 'UNKNOWN')\n",
        "    outputs.append({\n",
        "        'text': ex['text'],\n",
        "        'entity1_label': ex['entity1_label'],\n",
        "        'entity2_label': ex['entity2_label'],\n",
        "        'gold_relation': ex['relation_label'],\n",
        "        'predicted_relation': pred_label\n",
        "    })\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(outputs, f, indent=2)\n",
        "print(f\"Wrote predictions to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LLxXxc_0iXU"
      },
      "source": [
        "### Tokenize Validation Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIM4suAG07MK"
      },
      "source": [
        "### Classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mdkf-sM07jD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# true / pred IDs\n",
        "true_ids = re_val_ds['labels']\n",
        "pred_ids = pred_ids   # from trainer.predict()\n",
        "\n",
        "# pick the right id→label map\n",
        "if hasattr(re_model.config, 'id2label') and re_model.config.id2label:\n",
        "    # HF stores them as strings, so cast keys back to ints:\n",
        "    model_id2label = { int(k):v for k,v in re_model.config.id2label.items() }\n",
        "else:\n",
        "    model_id2label = id2label  # your 0–58 fixed mapping\n",
        "\n",
        "# only include the classes we actually see\n",
        "unique_labels = sorted(set(true_ids) | set(pred_ids))\n",
        "target_names   = [ model_id2label[l] for l in unique_labels ]\n",
        "\n",
        "# build and log the report\n",
        "report = classification_report(\n",
        "    true_ids,\n",
        "    pred_ids,\n",
        "    labels=unique_labels,\n",
        "    target_names=target_names,\n",
        "    output_dict=True,\n",
        "    zero_division=0\n",
        ")\n",
        "wandb.log({\"classification_report\": report})\n",
        "\n",
        "# micro-average metrics\n",
        "prec_re, rec_re, f1_re, _ = precision_recall_fscore_support(\n",
        "    true_ids,\n",
        "    pred_ids,\n",
        "    labels=unique_labels,\n",
        "    average='micro'\n",
        ")\n",
        "wandb.log({\n",
        "    \"re/precision\": prec_re,\n",
        "    \"re/recall\":    rec_re,\n",
        "    \"re/f1\":        f1_re,\n",
        "})\n",
        "\n",
        "# finally, your summary table (assuming prec_ner etc.)\n",
        "summary_table = wandb.Table(\n",
        "    columns=[\n",
        "      \"ner_precision\",\"ner_recall\",\"ner_f1\",\n",
        "      \"re_precision\", \"re_recall\", \"re_f1\"\n",
        "    ],\n",
        "    data=[[prec_ner, rec_ner, f1_ner, prec_re, rec_re, f1_re]]\n",
        ")\n",
        "wandb.log({\"metrics_summary\": summary_table})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdg2E9LmHEoN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "# 1) Build an equivalence map to collapse synonyms\n",
        "equiv = {\n",
        "    \"HasPart\": \"HasPart\",\n",
        "    \"part of\": \"HasPart\",\n",
        "    # ── add any other pairs you want to collapse ──\n",
        "    \"BasedOn\":      \"BasedOn\",\n",
        "    \"HasEffect\":    \"HasEffect\",\n",
        "    \"Causes\":       \"Causes\",\n",
        "    \"influenced by\":\"InfluencedBy\",\n",
        "    \"InfluencedBy\": \"InfluencedBy\",\n",
        "    # …\n",
        "}\n",
        "\n",
        "# 2) Pull out the *string* gold & predicted labels\n",
        "gold_str = re_val_ds[\"relation_label\"]         # e.g. [\"HasPart\",\"HasPart\",…]\n",
        "pred_str = [ pred_id2label.get(p, \"UNKNOWN\")   # your pred_id2label from earlier\n",
        "             for p in pred_ids ]\n",
        "\n",
        "# 3) Normalize both through the equivalence map\n",
        "gold_norm = [ equiv[g] if g in equiv else g for g in gold_str ]\n",
        "pred_norm = [ equiv[p] if p in equiv else p for p in pred_str ]\n",
        "\n",
        "# 4) Compute unique *string* labels actually present\n",
        "unique_labels_str = sorted(set(gold_norm) | set(pred_norm))\n",
        "\n",
        "# 5) Run classification_report on strings directly\n",
        "report = classification_report(\n",
        "    gold_norm,\n",
        "    pred_norm,\n",
        "    labels=unique_labels_str,\n",
        "    target_names=unique_labels_str,\n",
        "    output_dict=True,\n",
        "    zero_division=0\n",
        ")\n",
        "wandb.log({\"classification_report\": report})\n",
        "\n",
        "# 6) And micro‐averaged PRF on the strings as well:\n",
        "prec_re, rec_re, f1_re, _ = precision_recall_fscore_support(\n",
        "    gold_norm,\n",
        "    pred_norm,\n",
        "    labels=unique_labels_str,\n",
        "    average='micro'\n",
        ")\n",
        "wandb.log({\n",
        "    \"re/precision\": prec_re,\n",
        "    \"re/recall\":    rec_re,\n",
        "    \"re/f1\":        f1_re,\n",
        "})\n",
        "\n",
        "print(prec_re)\n",
        "print(rec_re)\n",
        "print(f1_re)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4yVOJzh_vwb"
      },
      "source": [
        "Wrap Up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J8TqQZE_xCM"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}