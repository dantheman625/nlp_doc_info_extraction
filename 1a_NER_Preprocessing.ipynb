{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3f45c4",
   "metadata": {},
   "source": [
    "# Prepare Combined NER Dataset\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "1. Load all your raw JSON files (with `doc` & `entities`) from `/data/raw/train/`.  \n",
    "2. Create a Hugging Face `Dataset` from them.  \n",
    "3. Load the OntoNotes 5.0 **train** split.  \n",
    "4. Concatenate the two into one large `Dataset`.  \n",
    "5. Save it to `/data/processed/` so your training notebook can load it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e33df1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f4d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 51 examples from raw JSONs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'domain': Value(dtype='string', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'doc': Value(dtype='string', id=None),\n",
       " 'entities': [{'id': Value(dtype='int64', id=None),\n",
       "   'mentions': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "   'type': Value(dtype='string', id=None)}],\n",
       " 'triples': [{'head': Value(dtype='string', id=None),\n",
       "   'relation': Value(dtype='string', id=None),\n",
       "   'tail': Value(dtype='string', id=None)}],\n",
       " 'label_set': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'entity_label_set': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_folder = \"data/raw/train/\"\n",
    "all_examples = []\n",
    "\n",
    "for fname in os.listdir(raw_folder):\n",
    "    if not fname.endswith(\".json\"):\n",
    "        continue\n",
    "    path = os.path.join(raw_folder, fname)\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            all_examples.extend(data)\n",
    "        else:\n",
    "            all_examples.append(data)\n",
    "\n",
    "existing_ds = Dataset.from_list(all_examples)\n",
    "print(f\"Loaded {len(existing_ds)} examples from raw JSONs\")\n",
    "existing_ds.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b64dcbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'tags'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Columns: ['tokens', 'tags']\n"
     ]
    }
   ],
   "source": [
    "# 3. Load OntoNotes 5.0 train split\n",
    "# (this contains 'tokens' and 'tags' columns)\n",
    "onto_train = load_dataset(\"tner/ontonotes5\", split=\"train[:10]\")\n",
    "print(onto_train)\n",
    "print(\"Columns:\", onto_train.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e08fdc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "9\n",
      "['People', 'start', 'their', 'own', 'businesses', 'for', 'many', 'reasons', '.']\n",
      "16\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "16\n",
      "['But', 'a', 'chance', 'to', 'fill', 'out', 'sales', '-', 'tax', 'records', 'is', 'rarely', 'one', 'of', 'them', '.']\n",
      "9\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "9\n",
      "['Red', 'tape', 'is', 'the', 'bugaboo', 'of', 'small', 'business', '.']\n",
      "42\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "42\n",
      "['Ironically', ',', 'the', 'person', 'who', 'wants', 'to', 'run', 'his', 'or', 'her', 'own', 'business', 'is', 'probably', 'the', 'active', ',', 'results', '-', 'oriented', 'sort', 'most', 'likely', 'to', 'hate', 'meeting', 'the', 'rules', 'and', 'record', '-', 'keeping', 'demands', 'of', 'federal', ',', 'state', 'and', 'local', 'regulators', '.']\n",
      "25\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "25\n",
      "['Yet', 'every', 'business', 'owner', 'has', 'to', 'face', 'the', 'mound', 'of', 'forms', 'and', 'regulations', '--', 'and', 'often', 'is', 'the', 'only', 'one', 'available', 'to', 'tackle', 'it', '.']\n",
      "6\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "6\n",
      "['There', 'is', 'hope', 'of', 'change', '.']\n",
      "31\n",
      "[2, 3, 0, 0, 4, 5, 0, 6, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "31\n",
      "['Last', 'week', ',', 'Sen.', 'Malcolm', 'Wallop', '-LRB-', 'R.', ',', 'Wyo', '.', '-RRB-', 'held', 'hearings', 'on', 'a', 'bill', 'to', 'strengthen', 'an', 'existing', 'law', 'designed', 'to', 'reduce', 'regulatory', 'hassles', 'for', 'small', 'businesses', '.']\n",
      "32\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 4, 0]\n",
      "32\n",
      "['``', 'A', 'great', 'many', 'federal', 'regulations', 'are', 'meant', 'for', 'larger', 'entities', 'and', 'do', \"n't\", 'really', 'apply', 'to', 'small', 'businesses', ',', \"''\", 'says', 'Marian', 'Jacob', ',', 'a', 'legislative', 'aide', 'to', 'Sen.', 'Wallop', '.']\n",
      "21\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "21\n",
      "['Other', 'lawmakers', 'are', 'busy', 'trying', 'to', 'revive', 'the', 'recently', 'lapsed', 'Paperwork', 'Reduction', 'Act', ',', 'which', 'many', 'feel', 'benefited', 'small', 'enterprises', '.']\n",
      "25\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0]\n",
      "25\n",
      "['Thus', ',', 'optimistic', 'entrepreneurs', 'await', 'a', 'promised', 'land', 'of', 'less', 'red', 'tape', '--', 'just', 'as', 'soon', 'as', 'Uncle', 'Sam', 'gets', 'around', 'to', 'arranging', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for row in onto_train:\n",
    "    print(len(row['tags']))\n",
    "    print(row['tags'])\n",
    "    print(len(row['tokens']))\n",
    "    print(row['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0576f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b187e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10 examples to data/processed/onto_sample_10.json\n"
     ]
    }
   ],
   "source": [
    "# 2) Define the original label2id and build id2label + label_list\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-CARDINAL\": 1,\n",
    "    \"B-DATE\": 2,\n",
    "    \"I-DATE\": 3,\n",
    "    \"B-PERSON\": 4,\n",
    "    \"I-PERSON\": 5,\n",
    "    \"B-NORP\": 6,\n",
    "    \"B-GPE\": 7,\n",
    "    \"I-GPE\": 8,\n",
    "    \"B-LAW\": 9,\n",
    "    \"I-LAW\": 10,\n",
    "    \"B-ORG\": 11,\n",
    "    \"I-ORG\": 12,\n",
    "    \"B-PERCENT\": 13,\n",
    "    \"I-PERCENT\": 14,\n",
    "    \"B-ORDINAL\": 15,\n",
    "    \"B-MONEY\": 16,\n",
    "    \"I-MONEY\": 17,\n",
    "    \"B-WORK_OF_ART\": 18,\n",
    "    \"I-WORK_OF_ART\": 19,\n",
    "    \"B-FAC\": 20,\n",
    "    \"B-TIME\": 21,\n",
    "    \"I-CARDINAL\": 22,\n",
    "    \"B-LOC\": 23,\n",
    "    \"B-QUANTITY\": 24,\n",
    "    \"I-QUANTITY\": 25,\n",
    "    \"I-NORP\": 26,\n",
    "    \"I-LOC\": 27,\n",
    "    \"B-PRODUCT\": 28,\n",
    "    \"I-TIME\": 29,\n",
    "    \"B-EVENT\": 30,\n",
    "    \"I-EVENT\": 31,\n",
    "    \"I-FAC\": 32,\n",
    "    \"B-LANGUAGE\": 33,\n",
    "    \"I-PRODUCT\": 34,\n",
    "    \"I-ORDINAL\": 35,\n",
    "    \"I-LANGUAGE\": 36\n",
    "}\n",
    "# invert\n",
    "id2label   = {v:k for k,v in label2id.items()}\n",
    "label_list = [id2label[i] for i in range(len(id2label))]\n",
    "\n",
    "# 3) Convert each OntoNotes example into your JSON schema\n",
    "output = []\n",
    "for ex in onto_train:\n",
    "    tokens = ex[\"tokens\"]\n",
    "    tags   = [ label_list[i] for i in ex[\"tags\"] ]\n",
    "\n",
    "    # 1) rebuild the string and get offsets\n",
    "    #    (convert tokens→string)\n",
    "    doc = tokenizer.convert_tokens_to_string(tokens)\n",
    "    #    then ask the tokenizer for char‐offsets\n",
    "    enc = tokenizer(\n",
    "        doc,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    offsets = enc[\"offset_mapping\"]\n",
    "\n",
    "    # 2) merge spans & pull out mention text\n",
    "    entities = []\n",
    "    i = 0\n",
    "    id = 0\n",
    "    while i < len(tags):\n",
    "        if tags[i].startswith(\"B-\"):\n",
    "            etype = tags[i][2:]\n",
    "            start_char, end_char = offsets[i]\n",
    "            j = i + 1\n",
    "            while j < len(tags) and tags[j] == f\"I-{etype}\":\n",
    "                end_char = offsets[j][1]\n",
    "                j += 1\n",
    "\n",
    "            mention_tokens = tokens[i:j]\n",
    "            mention = tokenizer.convert_tokens_to_string(mention_tokens)\n",
    "\n",
    "            entities.append({\n",
    "                \"id\": id,\n",
    "                \"type\":   etype,\n",
    "                \"mentions\": [mention]\n",
    "            })\n",
    "            i = j\n",
    "            id += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    output.append({\"doc\": doc, \"entities\": entities})\n",
    "\n",
    "# 4) Write out the JSON\n",
    "out_dir = \"data/processed\"\n",
    "out_file = os.path.join(out_dir, \"onto_sample_10.json\")\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(output)} examples to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3afb6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: 59975 examples\n",
      "Combined columns: ['domain', 'title', 'doc', 'entities', 'triples', 'label_set', 'entity_label_set', 'tokens', 'tags']\n"
     ]
    }
   ],
   "source": [
    "# 4. Concatenate your data + OntoNotes\n",
    "combined = concatenate_datasets([existing_ds, onto_train])\n",
    "print(f\"Combined dataset size: {len(combined)} examples\")\n",
    "print(\"Combined columns:\", combined.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44dd1c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 59975/59975 [00:00<00:00, 309313.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined dataset to data/processed/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Save to disk\n",
    "output_path = \"data/processed/\"\n",
    "combined.save_to_disk(output_path)\n",
    "print(f\"Saved combined dataset to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
