{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3f45c4",
   "metadata": {},
   "source": [
    "# Prepare Combined NER Dataset\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "1. Load all your raw JSON files (with `doc` & `entities`) from `/data/raw/train/`.  \n",
    "2. Create a Hugging Face `Dataset` from them.  \n",
    "3. Load the OntoNotes 5.0 **train** split.  \n",
    "4. Concatenate the two into one large `Dataset`.  \n",
    "5. Save it to `/data/processed/` so your training notebook can load it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33df1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_folder = \"data/raw/train/\"\n",
    "all_examples = []\n",
    "\n",
    "for fname in os.listdir(raw_folder):\n",
    "    if not fname.endswith(\".json\"):\n",
    "        continue\n",
    "    path = os.path.join(raw_folder, fname)\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            all_examples.extend(data)\n",
    "        else:\n",
    "            all_examples.append(data)\n",
    "\n",
    "existing_ds = Dataset.from_list(all_examples)\n",
    "print(f\"Loaded {len(existing_ds)} examples from raw JSONs\")\n",
    "existing_ds.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64dcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load OntoNotes 5.0 train split\n",
    "# (this contains 'tokens' and 'tags' columns)\n",
    "onto_train = load_dataset(\"tner/ontonotes5\", split=\"train\")\n",
    "print(onto_train)\n",
    "print(\"Columns:\", onto_train.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0576f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b187e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Define the original label2id and build id2label + label_list\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-CARDINAL\": 1,\n",
    "    \"B-DATE\": 2,\n",
    "    \"I-DATE\": 3,\n",
    "    \"B-PERSON\": 4,\n",
    "    \"I-PERSON\": 5,\n",
    "    \"B-NORP\": 6,\n",
    "    \"B-GPE\": 7,\n",
    "    \"I-GPE\": 8,\n",
    "    \"B-LAW\": 9,\n",
    "    \"I-LAW\": 10,\n",
    "    \"B-ORG\": 11,\n",
    "    \"I-ORG\": 12,\n",
    "    \"B-PERCENT\": 13,\n",
    "    \"I-PERCENT\": 14,\n",
    "    \"B-ORDINAL\": 15,\n",
    "    \"B-MONEY\": 16,\n",
    "    \"I-MONEY\": 17,\n",
    "    \"B-WORK_OF_ART\": 18,\n",
    "    \"I-WORK_OF_ART\": 19,\n",
    "    \"B-FAC\": 20,\n",
    "    \"B-TIME\": 21,\n",
    "    \"I-CARDINAL\": 22,\n",
    "    \"B-LOC\": 23,\n",
    "    \"B-QUANTITY\": 24,\n",
    "    \"I-QUANTITY\": 25,\n",
    "    \"I-NORP\": 26,\n",
    "    \"I-LOC\": 27,\n",
    "    \"B-PRODUCT\": 28,\n",
    "    \"I-TIME\": 29,\n",
    "    \"B-EVENT\": 30,\n",
    "    \"I-EVENT\": 31,\n",
    "    \"I-FAC\": 32,\n",
    "    \"B-LANGUAGE\": 33,\n",
    "    \"I-PRODUCT\": 34,\n",
    "    \"I-ORDINAL\": 35,\n",
    "    \"I-LANGUAGE\": 36\n",
    "}\n",
    "# invert\n",
    "id2label   = {v:k for k,v in label2id.items()}\n",
    "label_list = [id2label[i] for i in range(len(id2label))]\n",
    "\n",
    "# 3) Convert each OntoNotes example into your JSON schema\n",
    "output = []\n",
    "for ex in onto_train:\n",
    "    tokens = ex[\"tokens\"]\n",
    "    tags   = [ label_list[i] for i in ex[\"tags\"] ]\n",
    "\n",
    "    # 1) rebuild the string and get offsets\n",
    "    #    (convert tokens→string)\n",
    "    doc = tokenizer.convert_tokens_to_string(tokens)\n",
    "    #    then ask the tokenizer for char‐offsets\n",
    "    enc = tokenizer(\n",
    "        doc,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    offsets = enc[\"offset_mapping\"]\n",
    "\n",
    "    # 2) merge spans & pull out mention text\n",
    "    entities = []\n",
    "    i = 0\n",
    "    id = 0\n",
    "    while i < len(tags):\n",
    "        if tags[i].startswith(\"B-\"):\n",
    "            etype = tags[i][2:]\n",
    "            start_char, end_char = offsets[i]\n",
    "            j = i + 1\n",
    "            while j < len(tags) and tags[j] == f\"I-{etype}\":\n",
    "                end_char = offsets[j][1]\n",
    "                j += 1\n",
    "\n",
    "            mention_tokens = tokens[i:j]\n",
    "            mention = tokenizer.convert_tokens_to_string(mention_tokens)\n",
    "\n",
    "            entities.append({\n",
    "                \"id\": id,\n",
    "                \"type\":   etype,\n",
    "                \"mentions\": [mention]\n",
    "            })\n",
    "            i = j\n",
    "            id += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    output.append(\n",
    "        {\n",
    "            \"title\": \"Onto dataset\",\n",
    "            \"doc\": doc, \n",
    "            \"entities\": entities\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 4) Write out the JSON\n",
    "out_dir = \"data/processed\"\n",
    "out_file = os.path.join(out_dir, \"onto_sample_10.json\")\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(output)} examples to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afb6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Concatenate your data + OntoNotes\n",
    "combined_ds = concatenate_datasets([existing_ds, onto_train])\n",
    "print(f\"Combined dataset size: {len(combined_ds)} examples\")\n",
    "print(\"Combined columns:\", combined_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save to disk\n",
    "output_path = \"data/processed/hf_datasets/\"\n",
    "combined_ds.save_to_disk(output_path)\n",
    "print(f\"Saved combined dataset to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3491568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter for entity types\n",
    "entity_type_counts = Counter()\n",
    "\n",
    "# Count occurrences of each entity type across all documents\n",
    "for record in combined_ds:\n",
    "    if 'labels' in record:  # OntoNotes format\n",
    "        # Convert numeric tags to label names using id2label\n",
    "        tags = [id2label[tag] for tag in record['labels']]\n",
    "        # Count only the B- tags (beginning of entities)\n",
    "        for tag in tags:\n",
    "            if tag.startswith('B-'):\n",
    "                entity_type = tag[2:]  # Remove 'B-' prefix\n",
    "                entity_type_counts[entity_type] += 1\n",
    "    elif 'entities' in record:  # Your JSON format\n",
    "        entities = record['entities']\n",
    "        if entities == None:\n",
    "            continue\n",
    "        for entity in entities:\n",
    "            entity_type = entity.get('type', '')\n",
    "            if entity_type:\n",
    "                entity_type_counts[entity_type] += 1\n",
    "\n",
    "# Create lists for plotting\n",
    "types = list(entity_type_counts.keys())\n",
    "counts = list(entity_type_counts.values())\n",
    "\n",
    "# Sort by counts in descending order\n",
    "types, counts = zip(*sorted(zip(types, counts), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "bars = plt.bar(types, counts)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Frequency of Entity Types in Combined Dataset')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
