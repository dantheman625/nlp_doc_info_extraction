{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    texts = [\n",
    "        {\"id\": record.get(\"title\", f\"doc_{i}\"), \"text\": record.get(\"doc\", \"\")}\n",
    "        for i, record in enumerate(data)\n",
    "    ]\n",
    "\n",
    "    unique_labels = {\n",
    "        label\n",
    "        for record in data\n",
    "        for label in record.get(\"entity_label_set\", [])\n",
    "    }\n",
    "\n",
    "    print(f\"Loaded {len(texts)} texts.\")\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    return texts, unique_labels, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_entities(text: str, labels: list) -> dict:\n",
    "    \"\"\"\n",
    "    Prompt the LLaMA model to extract entities of interest and return a dict mapping labels to lists of entities.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "      {\"role\": \"system\", \"content\": f\"You are an expert in Named Entity Recognition. Please extract entities that match the schema definition from the input. Return an empty list if the entity type does not exist. Please respond in the format of a JSON string.\\\", \\\"schema\\\": {labels}\"},\n",
    "      {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def convert_model_output_to_json(\n",
    "    title: str,\n",
    "    model_output: str,\n",
    "    output_path: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts a JSON object from a model output string and returns it as a Python dict.\n",
    "    If `output_path` is provided, also writes the JSON to that file.\n",
    "\n",
    "    Args:\n",
    "        model_output: The raw string returned by the model, containing a JSON snippet.\n",
    "        output_path: Optional path (including '.json') to save the extracted JSON.\n",
    "\n",
    "    Returns:\n",
    "        A Python dict representing the JSON data.\n",
    "    \"\"\"\n",
    "    # 1. Try to grab anything between ```...``` fences first\n",
    "    fence_match = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", model_output, re.DOTALL)\n",
    "    if fence_match:\n",
    "        json_str = fence_match.group(1)\n",
    "    else:\n",
    "        # 2. Fallback: grab from first '{' to last '}'\n",
    "        start = model_output.find('{')\n",
    "        end = model_output.rfind('}') + 1\n",
    "        if start == -1 or end == -1:\n",
    "            raise ValueError(\"No JSON object found in the model output.\")\n",
    "        json_str = model_output[start:end]\n",
    "\n",
    "    # 3. Parse into dict\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Failed to parse JSON: {e}\")\n",
    "\n",
    "    new_entry = {title: data}\n",
    "\n",
    "    # 4. Optionally write or append to file.\n",
    "    if output_path:\n",
    "        if os.path.exists(output_path):\n",
    "            with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    existing_data = json.load(f)\n",
    "                    if not isinstance(existing_data, list):\n",
    "                        existing_data = [existing_data]\n",
    "                except json.JSONDecodeError:\n",
    "                    existing_data = []\n",
    "            existing_data.append(new_entry)\n",
    "            data_to_write = existing_data\n",
    "        else:\n",
    "            data_to_write = [new_entry]\n",
    "\n",
    "        print(f\"Writing JSON to {output_path}\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data_to_write, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"/content/drive/MyDrive/dataset/dev\"  # replace with your folder path\n",
    "\n",
    "# loop through all files in the given folder\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for filename in files:\n",
    "        path = os.path.join(root, filename)\n",
    "\n",
    "        texts, label_set, file_name = load_data(path)\n",
    "        print(file_name)\n",
    "\n",
    "        # loop through all texts in the given file\n",
    "        for item in texts:\n",
    "\n",
    "            id = item[\"id\"]\n",
    "            text = item[\"text\"]\n",
    "\n",
    "            # extract entities from the text\n",
    "            result = extract_entities(text, label_set)\n",
    "\n",
    "            output_file = f\"/content/drive/MyDrive/dataset/baseline_output/{file_name}.json\"\n",
    "\n",
    "            try:\n",
    "              convert_model_output_to_json(id, result[\"content\"], output_file)\n",
    "            except:\n",
    "              print(f\"Could not convert {id}. Saving raw conetent\")\n",
    "              with open(f\"/content/drive/MyDrive/dataset/baseline_output/{id}.json\", \"w\") as f:\n",
    "                f.write(result[\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.NEREvaluator import NEREvaluator\n",
    "\n",
    "base_folder = \"data/processed/NER/llama_baseline\"  # replace with your folder path\n",
    "\n",
    "# loop through all files in the given folder\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file_name in files:\n",
    "        with open(f\"data/raw/dev/{file_name}\", \"r\") as f:\n",
    "            gt = json.load(f)\n",
    "\n",
    "        with open(f\"data/processed/NER/llama_baseline/{file_name}\") as f:\n",
    "            preds = json.load(f)\n",
    "\n",
    "        results = []            \n",
    "\n",
    "        for p in preds:\n",
    "            doc_id = list(p.keys())[0]\n",
    "            pred = p[doc_id][\"entities\"]\n",
    "\n",
    "            gt_entry = next((entry for entry in gt if entry[\"title\"] == doc_id), None)\n",
    "            \n",
    "            if gt_entry is None:\n",
    "                print(f\"No matching entry found for {doc_id}\")\n",
    "                continue\n",
    "\n",
    "            doc = gt_entry[\"doc\"]\n",
    "            label_set = gt_entry[\"entities\"]\n",
    "\n",
    "            evaluator = NEREvaluator(\n",
    "                doc,\n",
    "                label_set,\n",
    "                pred\n",
    "            )\n",
    "\n",
    "            metrics = evaluator.evaluate()\n",
    "            results.append(\n",
    "                {\n",
    "                    doc_id: metrics\n",
    "                }\n",
    "            )\n",
    "\n",
    "        with open(f\"data/processed/NER/baseline_result/baseline_{file_name}\", \"w\") as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
