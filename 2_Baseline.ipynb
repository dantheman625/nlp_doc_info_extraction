{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUrADaqGCphn"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esSfdM-CCpho"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import torch\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx1M_ivCCpho"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    texts = [\n",
        "        {\"id\": record.get(\"title\", f\"doc_{i}\"), \"text\": record.get(\"doc\", \"\")}\n",
        "        for i, record in enumerate(data)\n",
        "    ]\n",
        "\n",
        "    unique_labels = {\n",
        "        label\n",
        "        for record in data\n",
        "        for label in record.get(\"entity_label_set\", [])\n",
        "    }\n",
        "\n",
        "    print(f\"Loaded {len(texts)} texts.\")\n",
        "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    return texts, unique_labels, file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFk311ClCphp"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXMKdbuTCphp"
      },
      "outputs": [],
      "source": [
        "def extract_entities(text: str, labels: list) -> dict:\n",
        "    \"\"\"\n",
        "    Prompt the LLaMA model to extract entities of interest and return a dict mapping labels to lists of entities.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": f\"You are an expert in Named Entity Recognition. Please extract entities that match the schema definition from the input. Return an empty list if the entity type does not exist. Please respond in the format of a JSON string.\\\", \\\"schema\\\": {labels}\"},\n",
        "      {\"role\": \"user\", \"content\": text},\n",
        "    ]\n",
        "\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "        messages,\n",
        "        max_new_tokens=2048,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    return(outputs[0][\"generated_text\"][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJfK9DS9Cphq"
      },
      "outputs": [],
      "source": [
        "def convert_model_output_to_json(\n",
        "    title: str,\n",
        "    model_output: str,\n",
        "    output_path: Optional[str] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extracts a JSON object from a model output string and returns it as a Python dict.\n",
        "    If `output_path` is provided, also writes the JSON to that file.\n",
        "\n",
        "    Args:\n",
        "        model_output: The raw string returned by the model, containing a JSON snippet.\n",
        "        output_path: Optional path (including '.json') to save the extracted JSON.\n",
        "\n",
        "    Returns:\n",
        "        A Python dict representing the JSON data.\n",
        "    \"\"\"\n",
        "    fence_match = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", model_output, re.DOTALL)\n",
        "    if fence_match:\n",
        "        json_str = fence_match.group(1)\n",
        "    else:\n",
        "        start = model_output.find('{')\n",
        "        end = model_output.rfind('}') + 1\n",
        "        if start == -1 or end == -1:\n",
        "            raise ValueError(\"No JSON object found in the model output.\")\n",
        "        json_str = model_output[start:end]\n",
        "\n",
        "    try:\n",
        "        data = json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"Failed to parse JSON: {e}\")\n",
        "\n",
        "    new_entry = {title: data}\n",
        "\n",
        "    if output_path:\n",
        "        if os.path.exists(output_path):\n",
        "            with open(output_path, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    existing_data = json.load(f)\n",
        "                    if not isinstance(existing_data, list):\n",
        "                        existing_data = [existing_data]\n",
        "                except json.JSONDecodeError:\n",
        "                    existing_data = []\n",
        "            existing_data.append(new_entry)\n",
        "            data_to_write = existing_data\n",
        "        else:\n",
        "            data_to_write = [new_entry]\n",
        "\n",
        "        print(f\"Writing JSON to {output_path}\")\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data_to_write, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSTDDBWECphq"
      },
      "outputs": [],
      "source": [
        "base_folder = \"/content/drive/MyDrive/project_files/data/raw/dev\"\n",
        "\n",
        "for root, dirs, files in os.walk(base_folder):\n",
        "    for filename in files:\n",
        "        path = os.path.join(root, filename)\n",
        "\n",
        "        texts, label_set, file_name = load_data(path)\n",
        "        print(file_name)\n",
        "\n",
        "        for item in texts:\n",
        "\n",
        "            id = item[\"id\"]\n",
        "            text = item[\"text\"]\n",
        "\n",
        "            result = extract_entities(text, label_set)\n",
        "\n",
        "            output_file = f\"/content/drive/MyDrive/project_files/data/processed/baseline_output/{file_name}.json\"\n",
        "\n",
        "            try:\n",
        "              convert_model_output_to_json(id, result[\"content\"], output_file)\n",
        "            except:\n",
        "              print(f\"Could not convert {id}. Saving raw conetent\")\n",
        "              with open(f\"/content/drive/MyDrive/project_files/data/processed/baseline_output/{id}.json\", \"w\") as f:\n",
        "                f.write(result[\"content\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NEREvaluator:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        text: str,\n",
        "        gt_entities: List[Dict[str, any]],\n",
        "        predicted_entities: List[Dict[str, str]],\n",
        "    ):\n",
        "        self.text = text\n",
        "\n",
        "        self.gt_mentions: List[Tuple[str, str]] = []\n",
        "        for ent in gt_entities:\n",
        "            etype = ent[\"type\"]\n",
        "            for m in ent.get(\"mentions\", []):\n",
        "                self.gt_mentions.append((m, etype))\n",
        "\n",
        "        self.pred_mentions: List[Tuple[str, str]] = [\n",
        "            (ent[\"text\"], ent[\"type\"]) for ent in predicted_entities\n",
        "        ]\n",
        "\n",
        "    def evaluate(self) -> Dict[str, float]:\n",
        "\n",
        "        print(\"check mention sets\")\n",
        "        print(self.gt_mentions)\n",
        "        print(self.pred_mentions)\n",
        "        remaining = self.gt_mentions.copy()\n",
        "        tp = 0\n",
        "        for m in self.pred_mentions:\n",
        "            if m in remaining:\n",
        "                tp += 1\n",
        "                remaining.remove(m)\n",
        "\n",
        "        fp = len(self.pred_mentions) - tp\n",
        "        fn = len(self.gt_mentions) - tp\n",
        "\n",
        "        precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
        "        recall    = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
        "        f1        = (2 * precision * recall / (precision + recall)\n",
        "                     if precision + recall > 0 else 0.0)\n",
        "\n",
        "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_folder = \"project_files/data/processed/baseline_output\"\n",
        "\n",
        "for root, dirs, files in os.walk(base_folder):\n",
        "    for file_name in files:\n",
        "        with open(f\"project_files/data/raw/dev/{file_name}\", \"r\") as f:\n",
        "            gt = json.load(f)\n",
        "\n",
        "        with open(f\"project_files/data/processed/baseline_output/{file_name}\") as f:\n",
        "            preds = json.load(f)\n",
        "\n",
        "        results = []            \n",
        "\n",
        "        for p in preds:\n",
        "            try:\n",
        "                doc_id = list(p.keys())[0]\n",
        "                pred = p[doc_id][\"entities\"]\n",
        "\n",
        "                gt_entry = next((entry for entry in gt if entry[\"title\"] == doc_id), None)\n",
        "                \n",
        "                if gt_entry is None:\n",
        "                    print(f\"No matching entry found for {doc_id}\")\n",
        "                    continue\n",
        "\n",
        "                doc = gt_entry[\"doc\"]\n",
        "                label_set = gt_entry[\"entities\"]\n",
        "\n",
        "                evaluator = NEREvaluator(\n",
        "                    doc,\n",
        "                    label_set,\n",
        "                    pred\n",
        "                )\n",
        "\n",
        "                metrics = evaluator.evaluate()\n",
        "                results.append(\n",
        "                    {\n",
        "                        doc_id: metrics\n",
        "                    }\n",
        "                )\n",
        "            except:\n",
        "                 continue\n",
        "            \n",
        "        total_precision = 0\n",
        "        total_recall = 0\n",
        "        total_f1 = 0\n",
        "        count = 0\n",
        "\n",
        "        for result in results:\n",
        "            for metrics in result.values():\n",
        "                total_precision += metrics[\"precision\"]\n",
        "                total_recall += metrics[\"recall\"]\n",
        "                total_f1 += metrics[\"f1\"]\n",
        "                count += 1\n",
        "\n",
        "        if count > 0:\n",
        "            avg_metrics = {\n",
        "                \"average_precision\": total_precision / count,\n",
        "                \"average_recall\": total_recall / count,\n",
        "                \"average_f1\": total_f1 / count\n",
        "            }\n",
        "            results.append({\"average_metrics\": avg_metrics})\n",
        "\n",
        "        with open(f\"project_files/data/processed/baseline_{file_name}\", \"w\") as f:\n",
        "                json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(count)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
