{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantheman625/nlp_doc_info_extraction/blob/final_touch/5_Complete_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB-78NmTAxDn"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJgrjPQwCqbh"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval scikit-learn datasets wandb nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldcCU7mJPVll"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    LongformerTokenizerFast,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from seqeval.metrics import precision_score as ner_prec, recall_score as ner_rec, f1_score as ner_f1\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjBgU7umahgi"
      },
      "outputs": [],
      "source": [
        "base_path = 'project_files'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3DTsURh-79i"
      },
      "source": [
        "## Wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cu4-vLz_fT5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9cEecqA9pj"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "Import Challenge data set (Final_eval.json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCnzzC7f3NLs"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejY9V79h3JUb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkCwny0seKjj"
      },
      "source": [
        "## Set Project folder in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqXgjBvS3T5h"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/project_files\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aog3WRix3YZC"
      },
      "source": [
        "## Load file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nShjsVxM3X4E"
      },
      "outputs": [],
      "source": [
        "eval_path   = os.path.join(base_path, '/data/processed/Final_eval.json')\n",
        "\n",
        "eval_data = []\n",
        "folder_path = f'{base_path}/data/raw/dev'\n",
        "\n",
        "print(folder_path)\n",
        "\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file_name in files:\n",
        "        with open(f\"{folder_path}/{file_name}\", \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for d in data:\n",
        "          eval_data.append(d)\n",
        "\n",
        "dataset = Dataset.from_list(eval_data)\n",
        "print(\"Sample example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PjxtL22yv4t"
      },
      "outputs": [],
      "source": [
        "entity_labels = dataset[0]['entity_label_set']\n",
        "label_list = ['O'] + [f\"B-{l}\" for l in entity_labels] + [f\"I-{l}\" for l in entity_labels]\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm53dfjvAz-B"
      },
      "source": [
        "# Define models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti1frmdMPYUz"
      },
      "source": [
        "# Baseline models\n",
        "Define which model you used as a baseline model for the specific task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNJb31CMPda7"
      },
      "outputs": [],
      "source": [
        "baseline_ner_name = \"allenai/longformer-base-4096\"\n",
        "baseline_re_name = \"SpanBERT/spanbert-large-cased\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KhLAFzTA13R"
      },
      "source": [
        "# Trained models\n",
        "\n",
        "Define your trained model for the specific task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8IAyiZ1RhEO"
      },
      "outputs": [],
      "source": [
        "trained_ner_name = f\"{base_path}/checkpoints/NER/longformer_tuned\"\n",
        "trained_re_name = f\"{base_path}/checkpoints/RE/checkpoint-306\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_5UIxiOA3dF"
      },
      "source": [
        "# Model selection\n",
        "\n",
        "Which model for NER, which for RE? -> Combination untrained/ untrained, trained/ trained, untrained/ trained, trained/ untrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysRNjlYDRBBl"
      },
      "source": [
        "## Both baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM-hBYn1Q1EV"
      },
      "outputs": [],
      "source": [
        "ner_model_name = baseline_ner_name\n",
        "re_model_name  = baseline_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjl7gqUZRD64"
      },
      "source": [
        "## Both trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wQL6dNbRWgd"
      },
      "outputs": [],
      "source": [
        "ner_model_name = trained_ner_name\n",
        "re_model_name  = trained_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jWBBmYNRHGC"
      },
      "source": [
        "## NER: trained, RE: baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOqWb9yHRS_c"
      },
      "outputs": [],
      "source": [
        "ner_model_name = trained_ner_name\n",
        "re_model_name  = baseline_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs8GFyxCRKS1"
      },
      "source": [
        "## NER: baseline, RE: trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqCg5zyaRZ3a"
      },
      "outputs": [],
      "source": [
        "ner_model_name = baseline_ner_name\n",
        "re_model_name  = trained_re_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV83Si0vUPvB"
      },
      "source": [
        "# Load Models and Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RHQ1PzcUTJJ"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuPIe9-9UVUk"
      },
      "outputs": [],
      "source": [
        "ner_tokenizer = LongformerTokenizerFast.from_pretrained(baseline_ner_name)\n",
        "ner_model     = AutoModelForTokenClassification.from_pretrained(\n",
        "    ner_model_name\n",
        ")\n",
        "\n",
        "ner_pipe = pipeline(\n",
        "    'ner',\n",
        "    model=ner_model,\n",
        "    tokenizer=ner_tokenizer,\n",
        "    device=-1,\n",
        "    aggregation_strategy='simple'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnvQ9uhbUXnt"
      },
      "source": [
        "## RE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlLQJe-CUSKx"
      },
      "outputs": [],
      "source": [
        "re_tokenizer  = AutoTokenizer.from_pretrained(re_model_name)\n",
        "if re_model_name == baseline_re_name:\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        re_model_name,\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    )\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        re_model_name,\n",
        "        config=cfg\n",
        "    )\n",
        "    print(f\"Loaded baseline RE model '{re_model_name}' with overridden head size num_labels={re_model.config.num_labels}\")\n",
        "else:\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(re_model_name)\n",
        "    print(f\"Loaded trained RE model '{re_model_name}' with head size num_labels={re_model.config.num_labels}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91WtT1p3-bm9"
      },
      "source": [
        "#Initialize Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsgYrS88-fkj"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"model-eval\",\n",
        "    name=f\"eval_{ner_model_name.split('/')[-1]}_{re_model_name.split('/')[-1]}\",\n",
        "    config={\n",
        "        \"ner_model\": ner_model_name,\n",
        "        \"re_model\": re_model_name,\n",
        "        \"dataset\": \"Final_eval.json\",\n",
        "        \"batch_size\": 32,\n",
        "        \"max_length\": 256,\n",
        "        \"seed\": 42,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jf36s9hA7SY"
      },
      "source": [
        "# NER Eval\n",
        "\n",
        "Output: Entity file -> content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM1Ifo4Pyv4z"
      },
      "outputs": [],
      "source": [
        "ner_val_results = []\n",
        "for idx, example in enumerate(eval_data):\n",
        "    preds = ner_pipe(example['doc'])\n",
        "    ner_val_results.append({\n",
        "        'domain': example.get('domain'),\n",
        "        'doc_title': example.get('title', f'doc_{idx}'),\n",
        "        'entities': preds,\n",
        "        'doc': example.get('doc')\n",
        "    })\n",
        "\n",
        "print(ner_val_results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0KaOyyVq1nI"
      },
      "source": [
        "Print NER Output for Nina to check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGBCYBJCq3hW"
      },
      "outputs": [],
      "source": [
        "with open(f'{base_path}/data/processed/ner_val_results.json','w') as f:\n",
        "    json.dump(\n",
        "        ner_val_results,\n",
        "        f,\n",
        "        default=lambda o: o.item() if isinstance(o, np.generic) else o\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaorJn5gDiot"
      },
      "outputs": [],
      "source": [
        "gt_index = {(ex['domain'], ex['title']): ex for ex in eval_data}\n",
        "pred_index = {(p['domain'], p['doc_title']): p for p in ner_val_results}\n",
        "\n",
        "true_ner_labels = []\n",
        "pred_ner_labels = []\n",
        "\n",
        "for key, gt in gt_index.items():\n",
        "    pred = pred_index.get(key)\n",
        "    if pred is None:\n",
        "        continue\n",
        "\n",
        "    text = gt['doc']\n",
        "    tokens = text.split()\n",
        "    n = len(tokens)\n",
        "\n",
        "    char2tok = {}\n",
        "    offset = 0\n",
        "    for i, tok in enumerate(tokens):\n",
        "        start = text.find(tok, offset)\n",
        "        end = start + len(tok)\n",
        "        for c in range(start, end):\n",
        "            char2tok[c] = i\n",
        "        offset = end\n",
        "\n",
        "    true_labels = ['O'] * n\n",
        "    pred_labels = ['O'] * n\n",
        "\n",
        "    for ent in gt['entities']:\n",
        "        ent_type = ent['type']\n",
        "        for mention in ent['mentions']:\n",
        "            start = text.find(mention)\n",
        "            while start != -1:\n",
        "                end = start + len(mention)\n",
        "                t0 = char2tok.get(start)\n",
        "                t1 = char2tok.get(end-1)\n",
        "                if t0 is not None and t1 is not None:\n",
        "                    true_labels[t0] = f'B-{ent_type}'\n",
        "                    for t in range(t0+1, t1+1):\n",
        "                        true_labels[t] = f'I-{ent_type}'\n",
        "                start = text.find(mention, end)\n",
        "\n",
        "    for ent in pred['entities']:\n",
        "        t0 = char2tok.get(ent['start'])\n",
        "        t1 = char2tok.get(ent['end'] - 1)\n",
        "        et = ent['entity_group']\n",
        "        if t0 is not None and t1 is not None:\n",
        "            pred_labels[t0] = f'B-{et}'\n",
        "            for t in range(t0+1, t1+1):\n",
        "                pred_labels[t] = f'I-{et}'\n",
        "\n",
        "    true_ner_labels.append(true_labels)\n",
        "    pred_ner_labels.append(pred_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_g6H9G--tRp"
      },
      "source": [
        "## Log Metrics in Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcVR516C-piY"
      },
      "outputs": [],
      "source": [
        "prec_ner = ner_prec(true_ner_labels, pred_ner_labels)\n",
        "rec_ner  = ner_rec(true_ner_labels, pred_ner_labels)\n",
        "f1_ner   = ner_f1(true_ner_labels, pred_ner_labels)\n",
        "\n",
        "print(prec_ner)\n",
        "print(rec_ner)\n",
        "print(f1_ner)\n",
        "\n",
        "wandb.log({\n",
        "    \"ner/precision\": prec_ner,\n",
        "    \"ner/recall\":    rec_ner,\n",
        "    \"ner/f1\":        f1_ner,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HxNSKq-A8by"
      },
      "source": [
        "# RE Eval\n",
        "\n",
        "Input: Entity file, original challenge test file -> matching of entities to sentences (siehe wa) -> Liste mit dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCXERBrXzR5x"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_Bym75fqMYe"
      },
      "outputs": [],
      "source": [
        "mapping_challenge_to_docred = {\n",
        "    \"Affiliation\":                         \"member of\",\n",
        "    \"ApprovedBy\":                          \"ApprovedBy\",\n",
        "    \"Author\":                              \"author\",\n",
        "    \"AwardReceived\":                       \"award received\",\n",
        "    \"BasedOn\":                             \"BasedOn\",\n",
        "    \"Capital\":                             \"capital\",\n",
        "    \"Causes\":                              \"Causes\",\n",
        "    \"Continent\":                           \"continent\",\n",
        "    \"ContributedToCreativeWork\":           \"ContributedToCreativeWork\",\n",
        "    \"Country\":                             \"country\",\n",
        "    \"CountryOfCitizenship\":                \"country of citizenship\",\n",
        "    \"Creator\":                             \"creator\",\n",
        "    \"Developer\":                           \"developer\",\n",
        "    \"DifferentFrom\":                       \"DifferentFrom\",\n",
        "    \"Director\":                            \"director\",\n",
        "    \"EducatedAt\":                          \"educated at\",\n",
        "    \"Employer\":                            \"employer\",\n",
        "    \"FieldOfWork\":                         \"FieldOfWork\",\n",
        "    \"FollowedBy\":                          \"followed by\",\n",
        "    \"Follows\":                             \"follows\",\n",
        "    \"Founded\":                             \"founded\",\n",
        "    \"FoundedBy\":                           \"founded by\",\n",
        "    \"HasCause\":                            \"HasCause\",\n",
        "    \"HasEffect\":                           \"HasEffect\",\n",
        "    \"HasPart\":                             \"HasPart\",\n",
        "    \"HasWorksInTheCollection\":             \"HasWorksInTheCollection\",\n",
        "    \"InfluencedBy\":                        \"influenced by\",\n",
        "    \"IssuedBy\":                            \"IssuedBy\",\n",
        "    \"LocatedIn\":                           \"located in the administrative territorial entity\",\n",
        "    \"Location\":                            \"location\",\n",
        "    \"MemberOf\":                            \"member of\",\n",
        "    \"NamedBy\":                             \"NamedBy\",\n",
        "    \"NominatedFor\":                        \"nominated for\",\n",
        "    \"OfficialLanguage\":                    \"official language\",\n",
        "    \"OwnedBy\":                             \"owned by\",\n",
        "    \"OwnerOf\":                             \"owner of\",\n",
        "    \"ParentOrganization\":                  \"parent organization\",\n",
        "    \"PartOf\":                              \"part of\",\n",
        "    \"Partner\":                             \"partner\",\n",
        "    \"PlaceOfBirth\":                        \"place of birth\",\n",
        "    \"PositionHeld\":                        \"position held\",\n",
        "    \"PublishedIn\":                         \"PublishedIn\",\n",
        "    \"Replaces\":                            \"replaces\",\n",
        "    \"SaidToBeTheSameAs\":                   \"SaidToBeTheSameAs\",\n",
        "    \"Studies\":                             \"Studies\",\n",
        "    \"UsedBy\":                              \"UsedBy\",\n",
        "    \"Uses\":                                \"Uses\",\n",
        "    \"WorkLocation\":                        \"work location\",\n",
        "\n",
        "    \"LanguageOfWorkOrName\":                \"original language of work\",\n",
        "    \"LanguageUsed\":                        \"languages spoken, written or signed\",\n",
        "    \"OriginalLanguageOfFilmOrTvShow\":      \"original language of work\",\n",
        "    \"PartyChiefRepresentative\":            \"head of government\",\n",
        "    \"PrimeFactor\":                         \"part of\",\n",
        "    \"TwinnedAdministrativeBody\":           \"sister city\",\n",
        "\n",
        "    \"AcademicDegree\":                      \"educated at\",\n",
        "    \"AdjacentStation\":                     \"shares border with\",\n",
        "    \"AppliesToPeople\":                     \"applies to jurisdiction\",\n",
        "    \"CitesWork\":                           \"present in work\",\n",
        "    \"ContainsAdministrativeTerritorialEntity\":     \"contains administrative territorial entity\",\n",
        "    \"ContainsTheAdministrativeTerritorialEntity\":  \"contains administrative territorial entity\",\n",
        "    \"DiplomaticRelation\":                  \"conflict\",\n",
        "    \"HasQuality\":                          \"genre\",\n",
        "    \"InOppositionTo\":                      \"separated from\",\n",
        "    \"InspiredBy\":                          \"BasedOn\",\n",
        "    \"InterestedIn\":                        \"Studies\",\n",
        "    \"NamedAfter\":                          \"NamedBy\",\n",
        "    \"NativeLanguage\":                      \"languages spoken, written or signed\",\n",
        "    \"OperatingSystem\":                     \"platform\",\n",
        "    \"PhysicallyInteractsWith\":             \"shares border with\",\n",
        "    \"PracticedBy\":                         \"UsedBy\",\n",
        "    \"PresentedIn\":                         \"present in work\",\n",
        "    \"Promoted\":                            \"HasEffect\",\n",
        "    \"RegulatedBy\":                         \"IssuedBy\",\n",
        "    \"SharesBorderWith\":                    \"shares border with\",\n",
        "    \"SignificantEvent\":                    \"location\",\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7LZyGD8mdsM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "candidates = []\n",
        "for doc in ner_val_results:\n",
        "    text  = doc[\"doc\"]\n",
        "    title = doc[\"doc_title\"].strip().lower()\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    offsets = []\n",
        "    cursor = 0\n",
        "    for s in sentences:\n",
        "        start = text.find(s, cursor)\n",
        "        end   = start + len(s)\n",
        "        offsets.append((s, start, end))\n",
        "        cursor = end\n",
        "\n",
        "    mentions = [\n",
        "      (ent[\"word\"].strip(), ent[\"start\"], ent[\"end\"], ent[\"entity_group\"])\n",
        "      for ent in doc[\"entities\"]\n",
        "    ]\n",
        "\n",
        "    for sent, s_start, s_end in offsets:\n",
        "        sent_mentions = [\n",
        "          (w, a, b, label)\n",
        "          for (w,a,b,label) in mentions\n",
        "          if s_start <= a < s_end\n",
        "        ]\n",
        "\n",
        "        if len(sent_mentions) < 2:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(sent_mentions)):\n",
        "            w1, a1, b1, label1 = sent_mentions[i]\n",
        "            for j in range(i+1, len(sent_mentions)):\n",
        "                w2, a2, b2, label2 = sent_mentions[j]\n",
        "\n",
        "                snippet = sent.replace(w1, f\"[E1]{w1}[/E1]\", 1) \\\n",
        "                              .replace(w2, f\"[E2]{w2}[/E2]\", 1)\n",
        "\n",
        "                candidates.append({\n",
        "                  \"doc_title\":       title,\n",
        "                  \"text\":            snippet,\n",
        "                  \"entity1_label\":   w1,\n",
        "                  \"entity2_label\":   w2,\n",
        "                  \"relation_label\":  \"no_relation\"\n",
        "                })\n",
        "\n",
        "from datasets import Dataset\n",
        "ds = Dataset.from_list(candidates)\n",
        "\n",
        "print(f\"Built {len(candidates)} candidate pairs\")\n",
        "print(f\"HF Dataset contains {len(ds)} rows\")\n",
        "\n",
        "\n",
        "re_val_ds = ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5zamdf_LC0a"
      },
      "source": [
        "## Trainer option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkcuuyERo1Ae"
      },
      "outputs": [],
      "source": [
        "re_tokenizer = AutoTokenizer.from_pretrained(re_model_name)\n",
        "if re_model_name == baseline_re_name:\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        re_model_name,\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    )\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        re_model_name, config=cfg\n",
        "    )\n",
        "else:\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(re_model_name)\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return re_tokenizer(batch['text'],\n",
        "                        padding='max_length',\n",
        "                        truncation=True,\n",
        "                        max_length=256)\n",
        "tokenized_val = re_val_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "eval_args = TrainingArguments(\n",
        "    output_dir=f'{base_path}/data/processed/re_predict_output',\n",
        "    per_device_eval_batch_size=32,\n",
        "    do_train=False,\n",
        "    do_eval=False,\n",
        "    logging_dir=f'{base_path}/logs',\n",
        "    report_to='wandb'\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=re_model,\n",
        "    args=eval_args,\n",
        "    tokenizer=re_tokenizer\n",
        ")\n",
        "\n",
        "preds_output = trainer.predict(tokenized_val)\n",
        "logits = preds_output.predictions\n",
        "pred_ids = logits.argmax(axis=-1)\n",
        "\n",
        "if hasattr(re_model.config, 'id2label') and re_model.config.id2label:\n",
        "    pred_id2label = {int(k):v for k,v in re_model.config.id2label.items()}\n",
        "else:\n",
        "    pred_id2label = id2label\n",
        "\n",
        "import json, os\n",
        "safe = os.path.basename(re_model_name.rstrip('/'))\n",
        "out_path = f'{base_path}/data/processed/re_{safe}_candidates_with_preds.json'\n",
        "outputs = []\n",
        "for ex, pid in zip(re_val_ds, pred_ids):\n",
        "    outputs.append({\n",
        "      'text':               ex['text'],\n",
        "      'entity1_label':      ex['entity1_label'],\n",
        "      'entity2_label':      ex['entity2_label'],\n",
        "      'predicted_relation': pred_id2label.get(int(pid), 'UNKNOWN')\n",
        "    })\n",
        "with open(out_path, 'w') as f:\n",
        "    json.dump(outputs, f, indent=2)\n",
        "print(f\"Wrote {len(outputs)} predictions to {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZaCjxABLFdh"
      },
      "source": [
        "## Pipeline option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP9EnaXmJdcE"
      },
      "outputs": [],
      "source": [
        "re_tokenizer = AutoTokenizer.from_pretrained(re_model_name)\n",
        "if re_model_name == baseline_re_name:\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        re_model_name,\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    )\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        re_model_name, config=cfg\n",
        "    )\n",
        "    print(f\"> Loaded baseline RE model '{re_model_name}' with head size {re_model.config.num_labels}\")\n",
        "else:\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(re_model_name)\n",
        "    print(f\"> Loaded trained RE model '{re_model_name}' with head size {re_model.config.num_labels}\")\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "re_pipe = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=re_model,\n",
        "    tokenizer=re_tokenizer,\n",
        "    device=device,\n",
        "    return_all_scores=False,\n",
        ")\n",
        "\n",
        "outputs = []\n",
        "for ex in re_val_ds:\n",
        "    pred = re_pipe(ex[\"text\"])[0]\n",
        "    outputs.append({\n",
        "        \"text\":               ex[\"text\"],\n",
        "        \"entity1_label\":      ex[\"entity1_label\"],\n",
        "        \"entity2_label\":      ex[\"entity2_label\"],\n",
        "        \"predicted_relation\": pred[\"label\"],\n",
        "        \"score\":              pred[\"score\"],\n",
        "    })\n",
        "\n",
        "safe = os.path.basename(re_model_name.rstrip(\"/\"))\n",
        "out_path = f\"{base_path}/data/processed/re_{safe}_candidates_with_preds.json\"\n",
        "with open(out_path, \"w\") as f:\n",
        "    json.dump(outputs, f, indent=2)\n",
        "\n",
        "print(f\"Wrote {len(outputs)} predictions to {out_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25l7-Zjsu7Db"
      },
      "outputs": [],
      "source": [
        "gold_map = {}\n",
        "for meta in dataset:\n",
        "    for t in meta.get(\"triples\", []):\n",
        "        h = t[\"head\"].strip().lower()\n",
        "        te = t[\"tail\"].strip().lower()\n",
        "        mapped = mapping_challenge_to_docred.get(t[\"relation\"])\n",
        "        if not mapped:\n",
        "            continue\n",
        "        gold_map[(h, te)] = mapped\n",
        "        gold_map[(te, h)] = mapped\n",
        "\n",
        "predictions_path = out_path\n",
        "with open(predictions_path, \"r\") as f:\n",
        "    preds_list = json.load(f)\n",
        "\n",
        "gold_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for ex in preds_list:\n",
        "    e1 = ex[\"entity1_label\"].strip().lower()\n",
        "    e2 = ex[\"entity2_label\"].strip().lower()\n",
        "    gold = gold_map.get((e1, e2), \"no_relation\")\n",
        "    gold_labels.append(gold)\n",
        "    pred_labels.append(ex[\"predicted_relation\"])\n",
        "\n",
        "all_labels = sorted(set(gold_labels) | set(pred_labels))\n",
        "print(classification_report(\n",
        "    gold_labels,\n",
        "    pred_labels,\n",
        "    labels=all_labels,\n",
        "    target_names=all_labels,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "p, r, f1, _ = precision_recall_fscore_support(\n",
        "    gold_labels, pred_labels, average=\"micro\", zero_division=0\n",
        ")\n",
        "print(f\"‚Üí micro precision={p:.4f}   recall={r:.4f}   f1={f1:.4f}\")\n",
        "\n",
        "import json\n",
        "\n",
        "detailed = []\n",
        "for ex, gold, pred in zip(preds_list, gold_labels, pred_labels):\n",
        "    detailed.append({\n",
        "        \"text\":             ex[\"text\"],\n",
        "        \"entity1_label\":    ex[\"entity1_label\"],\n",
        "        \"entity2_label\":    ex[\"entity2_label\"],\n",
        "        \"gold_relation\":    gold,\n",
        "        \"predicted_relation\": pred\n",
        "    })\n",
        "\n",
        "out_detail_path = f\"{base_path}/data/processed/detailed_re_predictions.json\"\n",
        "with open(out_detail_path, \"w\") as f:\n",
        "    json.dump(detailed, f, indent=2)\n",
        "\n",
        "print(f\"Wrote {len(detailed)} detailed examples to {out_detail_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC8Ep5z7znTv"
      },
      "outputs": [],
      "source": [
        "unique_labels = sorted(set(gold_labels) | set(pred_labels))\n",
        "\n",
        "report = classification_report(\n",
        "    gold_labels,\n",
        "    pred_labels,\n",
        "    labels=unique_labels,\n",
        "    target_names=unique_labels,\n",
        "    output_dict=True,\n",
        "    zero_division=0\n",
        ")\n",
        "wandb.log({\"classification_report\": report})\n",
        "\n",
        "prec_re, rec_re, f1_re, _ = precision_recall_fscore_support(\n",
        "    gold_labels,\n",
        "    pred_labels,\n",
        "    labels=unique_labels,\n",
        "    average=\"micro\",\n",
        "    zero_division=0\n",
        ")\n",
        "wandb.log({\n",
        "    \"re/precision\": prec_re,\n",
        "    \"re/recall\":    rec_re,\n",
        "    \"re/f1\":        f1_re,\n",
        "})\n",
        "\n",
        "summary = wandb.Table(\n",
        "    columns=[\n",
        "      \"ner_precision\",\"ner_recall\",\"ner_f1\",\n",
        "      \"re_precision\",  \"re_recall\",  \"re_f1\"\n",
        "    ],\n",
        "    data=[[prec_ner, rec_ner, f1_ner, prec_re, rec_re, f1_re]]\n",
        ")\n",
        "wandb.log({\"metrics_summary\": summary})\n",
        "\n",
        "print(f\"NER   ‚Üí precision: {prec_ner:.4f}, recall: {rec_ner:.4f}, f1: {f1_ner:.4f}\")\n",
        "print(f\"RE    ‚Üí precision: {prec_re:.4f}, recall: {rec_re:.4f}, f1: {f1_re:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE5D-3uUQQ8B"
      },
      "source": [
        "# Exclude no relation\n",
        "Since the RE model was only trained on docred labels and not no relation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0XbOT5bQWwN"
      },
      "outputs": [],
      "source": [
        "with open(f\"{base_path}/data/processed/detailed_re_predictions.json\", \"r\") as f:\n",
        "    detailed = json.load(f)\n",
        "\n",
        "filtered = [ex for ex in detailed if ex[\"gold_relation\"] != \"no_relation\"]\n",
        "\n",
        "gold_filt = [ex[\"gold_relation\"]      for ex in filtered]\n",
        "pred_filt = [ex[\"predicted_relation\"] for ex in filtered]\n",
        "\n",
        "print(f\"‚Üí {len(filtered)} positive examples (out of {len(detailed)})\\n\")\n",
        "\n",
        "labels = sorted(set(gold_filt) | set(pred_filt))\n",
        "print(classification_report(\n",
        "    gold_filt,\n",
        "    pred_filt,\n",
        "    labels=labels,\n",
        "    target_names=labels,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "p, r, f1, _ = precision_recall_fscore_support(\n",
        "    gold_filt,\n",
        "    pred_filt,\n",
        "    average=\"micro\",\n",
        "    zero_division=0\n",
        ")\n",
        "print(f\"‚Üí POSITIVE-only micro precision={p:.4f}   recall={r:.4f}   f1={f1:.4f}\")\n",
        "out_detail_positive_only_path = f\"{base_path}/data/processed/detailed_re_predictions_positive_only.json\"\n",
        "with open(out_detail_positive_only_path, \"w\") as f:\n",
        "    json.dump(detailed, f, indent=2)\n",
        "\n",
        "print(f\"Wrote {len(detailed)} detailed examples to {out_detail_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6PL9C2O2kNY"
      },
      "source": [
        "### Build RE validation examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHCm2DcupBrX"
      },
      "source": [
        "### Create HF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvqazyZtbAJH"
      },
      "outputs": [],
      "source": [
        "re_tokenizer  = AutoTokenizer.from_pretrained(re_model_name)\n",
        "if re_model_name == baseline_re_name:\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        re_model_name,\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    )\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        re_model_name,\n",
        "        config=cfg\n",
        "    )\n",
        "    print(f\"Loaded baseline RE model '{re_model_name}' with overridden head size num_labels={re_model.config.num_labels}\")\n",
        "else:\n",
        "    re_model = AutoModelForSequenceClassification.from_pretrained(re_model_name)\n",
        "    print(f\"Loaded trained RE model '{re_model_name}' with head size num_labels={re_model.config.num_labels}\")\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return re_tokenizer(\n",
        "        batch['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "tokenized_val = re_val_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1}\n",
        "\n",
        "eval_args = TrainingArguments(\n",
        "    output_dir=f'{base_path}/data/processed/re_eval_output',\n",
        "    per_device_eval_batch_size=32,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    logging_dir=f'{base_path}/logs',\n",
        "    report_to='wandb'\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=re_model,\n",
        "    args=eval_args,\n",
        "    tokenizer=re_tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "eval_result = trainer.evaluate(eval_dataset=tokenized_val)\n",
        "print(\"üîç RE Validation Results:\", eval_result)\n",
        "\n",
        "preds_output = trainer.predict(tokenized_val)\n",
        "pred_ids = np.argmax(preds_output.predictions, axis=-1)\n",
        "\n",
        "if hasattr(re_model.config, 'id2label') and re_model.config.id2label:\n",
        "    pred_id2label = re_model.config.id2label\n",
        "else:\n",
        "    pred_id2label = id2label\n",
        "\n",
        "import os\n",
        "safe_model_name = os.path.basename(re_model_name.rstrip('/'))\n",
        "output_path = f'{base_path}/data/processed/re_{safe_model_name}_predictions.json'\n",
        "\n",
        "outputs = []\n",
        "for ex, pred in zip(re_val_ds, pred_ids):\n",
        "    pred_label = pred_id2label.get(pred, 'UNKNOWN')\n",
        "    outputs.append({\n",
        "        'text': ex['text'],\n",
        "        'entity1_label': ex['entity1_label'],\n",
        "        'entity2_label': ex['entity2_label'],\n",
        "        'gold_relation': ex['relation_label'],\n",
        "        'predicted_relation': pred_label\n",
        "    })\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(outputs, f, indent=2)\n",
        "print(f\"Wrote predictions to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LLxXxc_0iXU"
      },
      "source": [
        "### Tokenize Validation Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIM4suAG07MK"
      },
      "source": [
        "### Classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mdkf-sM07jD"
      },
      "outputs": [],
      "source": [
        "true_ids = re_val_ds['labels']\n",
        "pred_ids = pred_ids\n",
        "\n",
        "if hasattr(re_model.config, 'id2label') and re_model.config.id2label:\n",
        "    model_id2label = { int(k):v for k,v in re_model.config.id2label.items() }\n",
        "else:\n",
        "    model_id2label = id2label\n",
        "\n",
        "unique_labels = sorted(set(true_ids) | set(pred_ids))\n",
        "target_names   = [ model_id2label[l] for l in unique_labels ]\n",
        "\n",
        "report = classification_report(\n",
        "    true_ids,\n",
        "    pred_ids,\n",
        "    labels=unique_labels,\n",
        "    target_names=target_names,\n",
        "    output_dict=True,\n",
        "    zero_division=0\n",
        ")\n",
        "wandb.log({\"classification_report\": report})\n",
        "\n",
        "prec_re, rec_re, f1_re, _ = precision_recall_fscore_support(\n",
        "    true_ids,\n",
        "    pred_ids,\n",
        "    labels=unique_labels,\n",
        "    average='micro'\n",
        ")\n",
        "wandb.log({\n",
        "    \"re/precision\": prec_re,\n",
        "    \"re/recall\":    rec_re,\n",
        "    \"re/f1\":        f1_re,\n",
        "})\n",
        "\n",
        "summary_table = wandb.Table(\n",
        "    columns=[\n",
        "      \"ner_precision\",\"ner_recall\",\"ner_f1\",\n",
        "      \"re_precision\", \"re_recall\", \"re_f1\"\n",
        "    ],\n",
        "    data=[[prec_ner, rec_ner, f1_ner, prec_re, rec_re, f1_re]]\n",
        ")\n",
        "wandb.log({\"metrics_summary\": summary_table})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdg2E9LmHEoN"
      },
      "outputs": [],
      "source": [
        "equiv = {\n",
        "    \"HasPart\": \"HasPart\",\n",
        "    \"part of\": \"HasPart\",\n",
        "    \"BasedOn\":      \"BasedOn\",\n",
        "    \"HasEffect\":    \"HasEffect\",\n",
        "    \"Causes\":       \"Causes\",\n",
        "    \"influenced by\":\"InfluencedBy\",\n",
        "    \"InfluencedBy\": \"InfluencedBy\",\n",
        "}\n",
        "\n",
        "gold_str = re_val_ds[\"relation_label\"]\n",
        "pred_str = [ pred_id2label.get(p, \"UNKNOWN\")\n",
        "             for p in pred_ids ]\n",
        "\n",
        "gold_norm = [ equiv[g] if g in equiv else g for g in gold_str ]\n",
        "pred_norm = [ equiv[p] if p in equiv else p for p in pred_str ]\n",
        "\n",
        "unique_labels_str = sorted(set(gold_norm) | set(pred_norm))\n",
        "\n",
        "report = classification_report(\n",
        "    gold_norm,\n",
        "    pred_norm,\n",
        "    labels=unique_labels_str,\n",
        "    target_names=unique_labels_str,\n",
        "    output_dict=True,\n",
        "    zero_division=0\n",
        ")\n",
        "wandb.log({\"classification_report\": report})\n",
        "\n",
        "prec_re, rec_re, f1_re, _ = precision_recall_fscore_support(\n",
        "    gold_norm,\n",
        "    pred_norm,\n",
        "    labels=unique_labels_str,\n",
        "    average='micro'\n",
        ")\n",
        "wandb.log({\n",
        "    \"re/precision\": prec_re,\n",
        "    \"re/recall\":    rec_re,\n",
        "    \"re/f1\":        f1_re,\n",
        "})\n",
        "\n",
        "print(prec_re)\n",
        "print(rec_re)\n",
        "print(f1_re)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4yVOJzh_vwb"
      },
      "source": [
        "Wrap Up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J8TqQZE_xCM"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}