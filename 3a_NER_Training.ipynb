{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "300eb73b",
      "metadata": {
        "id": "300eb73b"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9273274",
      "metadata": {
        "id": "a9273274"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch seqeval evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5f51e5",
      "metadata": {
        "id": "ff5f51e5"
      },
      "source": [
        "# Env Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5558fb9a",
      "metadata": {
        "id": "5558fb9a"
      },
      "outputs": [],
      "source": [
        "base_path = 'data/'\n",
        "max_token_length = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53def38a",
      "metadata": {
        "id": "53def38a"
      },
      "source": [
        "# Establish Google Drive Connection (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785ce452",
      "metadata": {
        "id": "785ce452"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = 'drive/MyDrive/NLP_project_data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4192d65",
      "metadata": {
        "id": "b4192d65"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "205b781e",
      "metadata": {
        "id": "205b781e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    LongformerTokenizerFast,\n",
        "    LongformerForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        "    TrainerCallback,\n",
        "    TrainerState,\n",
        "    TrainerControl,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "import evaluate\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "from collections import Counter\n",
        "import itertools\n",
        "from typing import Dict, Any\n",
        "import re, string"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32ad725",
      "metadata": {
        "id": "a32ad725"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d8299a",
      "metadata": {
        "id": "31d8299a"
      },
      "outputs": [],
      "source": [
        "def load_json_data(folder_path):\n",
        "    aggregated_data = []\n",
        "\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file_name in files:\n",
        "            with open(f'{folder_path}/{file_name}', 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            aggregated_data.append(data)\n",
        "\n",
        "    return aggregated_data\n",
        "\n",
        "# Convert NumPy float32 to native Python floats before JSON serialization\n",
        "def convert_numpy_floats(obj):\n",
        "    if isinstance(obj, np.float32):\n",
        "        return float(obj)\n",
        "    raise TypeError\n",
        "\n",
        "def save_model_output(output, output_path):\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(output, f, ensure_ascii=False, indent=2, default=convert_numpy_floats)\n",
        "    print(f'Saved validation NER predictions to {output_path}')\n",
        "\n",
        "def compute_f1(predictions_file_path, output_path, validation_dataset):\n",
        "  with open(predictions_file_path, 'r') as f:\n",
        "      saved_preds = json.load(f)\n",
        "\n",
        "  all_gold = []\n",
        "  all_pred = []\n",
        "  for pred in saved_preds:\n",
        "      idx = pred['index']\n",
        "      gold_entities = validation_dataset[idx]['entities']\n",
        "      gold_set = set()\n",
        "      for ent in gold_entities:\n",
        "          for m in ent['mentions']:\n",
        "              gold_set.add((m, ent['type']))\n",
        "      pred_list = pred['predictions']\n",
        "      pred_set = set()\n",
        "      for p in pred_list:\n",
        "          w = p.get('word').lstrip()\n",
        "          et = p.get('entity_group')\n",
        "          pred_set.add((w, et))\n",
        "      all_gold.append(gold_set)\n",
        "      all_pred.append(pred_set)\n",
        "\n",
        "  tp = 0\n",
        "  pred_count = 0\n",
        "  gold_count = 0\n",
        "  for gold_set, pred_set in zip(all_gold, all_pred):\n",
        "      tp += len(gold_set & pred_set)\n",
        "      pred_count += len(pred_set)\n",
        "      gold_count += len(gold_set)\n",
        "\n",
        "  precision = tp / pred_count if pred_count > 0 else 0.0\n",
        "  recall = tp / gold_count if gold_count > 0 else 0.0\n",
        "  f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "  metrics = {\n",
        "      'precision': precision,\n",
        "      'recall': recall,\n",
        "      'f1': f1,\n",
        "      'true_positives': tp,\n",
        "      'predicted': pred_count,\n",
        "      'gold': gold_count\n",
        "  }\n",
        "  print('NER Validation Mention-level Metrics:')\n",
        "  print(metrics)\n",
        "\n",
        "  with open(output_path, 'w') as f:\n",
        "      json.dump(metrics, f, indent=2)\n",
        "  print(f'Saved evaluation metrics to {output_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3281bd74",
      "metadata": {
        "id": "3281bd74"
      },
      "source": [
        "# Load data into Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a156e72",
      "metadata": {
        "id": "9a156e72"
      },
      "outputs": [],
      "source": [
        "aggregated_data = []\n",
        "folder_path = f'{base_path}raw/train'\n",
        "\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file_name in files:\n",
        "        with open(f'{folder_path}/{file_name}', 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for d in data:\n",
        "          if len(d['entities'])>0:\n",
        "            aggregated_data.append(d)\n",
        "\n",
        "\n",
        "\n",
        "dataset = Dataset.from_list(aggregated_data)\n",
        "print('Sample example:')\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f46f4dd",
      "metadata": {
        "id": "9f46f4dd"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5bfbcf",
      "metadata": {
        "id": "9e5bfbcf"
      },
      "outputs": [],
      "source": [
        "model_name = 'allenai/longformer-base-4096'\n",
        "\n",
        "entity_labels = dataset[0]['entity_label_set']\n",
        "label_list = ['O'] + [f'B-{l}' for l in entity_labels] + [f'I-{l}' for l in entity_labels]\n",
        "print(label_list)\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "tokenizer = LongformerTokenizerFast.from_pretrained(\n",
        "    model_name,\n",
        "    max_length = max_token_length\n",
        ")\n",
        "\n",
        "model = LongformerForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4bbd478",
      "metadata": {
        "id": "a4bbd478"
      },
      "source": [
        "# Split Data into Validation and Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb49594",
      "metadata": {
        "id": "4fb49594"
      },
      "outputs": [],
      "source": [
        "trunc_count = 0\n",
        "\n",
        "word_boundary = re.compile(r'\\w')\n",
        "punct = set(string.punctuation) - {'-'}\n",
        "\n",
        "def tokenize_and_align_labels(example):\n",
        "    enc = tokenizer(\n",
        "        example['doc'],\n",
        "        return_offsets_mapping=True,\n",
        "        truncation=True,\n",
        "        max_length=max_token_length,\n",
        "    )\n",
        "    offsets   = enc.pop('offset_mapping')\n",
        "    word_ids  = enc.word_ids()\n",
        "\n",
        "    n_words   = max(w for w in word_ids if w is not None) + 1\n",
        "    word_tags = ['O'] * n_words\n",
        "    used_wids = set()\n",
        "\n",
        "    text = example['doc']\n",
        "\n",
        "    for ent in example['entities']:\n",
        "        ent_type = ent['type']\n",
        "        for mention in ent['mentions']:\n",
        "\n",
        "            if len(mention) < 3 and not mention.isupper():\n",
        "                continue\n",
        "\n",
        "            pattern = r'(?<![\\w-])' + re.escape(mention) + r'(?![\\w-])'\n",
        "\n",
        "            for m in re.finditer(pattern, text):\n",
        "                s, e = m.span()\n",
        "\n",
        "                covered = {\n",
        "                    wid for tidx, (cs, ce) in enumerate(offsets)\n",
        "                    if (cs < e and ce > s) and (wid := word_ids[tidx]) is not None\n",
        "                }\n",
        "\n",
        "                if not covered or used_wids.intersection(covered):\n",
        "                    continue\n",
        "\n",
        "                if text[s] in punct or text[e - 1] in punct:\n",
        "                    continue\n",
        "\n",
        "                first, *rest = sorted(covered)\n",
        "                word_tags[first] = f'B-{ent_type}'\n",
        "                for wid in rest:\n",
        "                    word_tags[wid] = f'I-{ent_type}'\n",
        "                used_wids.update(covered)\n",
        "\n",
        "    labels, prev_wid = [], None\n",
        "    for wid in word_ids:\n",
        "        if wid is None:\n",
        "            labels.append(-100)\n",
        "        elif wid != prev_wid:\n",
        "            labels.append(label2id[word_tags[wid]])\n",
        "        else:\n",
        "            labels.append(-100)\n",
        "        prev_wid = wid\n",
        "\n",
        "    enc['labels'] = labels\n",
        "    return enc\n",
        "\n",
        "\n",
        "all_indices = list(range(len(dataset)))\n",
        "train_idx, val_idx = train_test_split(all_indices, test_size=0.1, random_state=42)\n",
        "train_orig = dataset.select(train_idx)\n",
        "val_orig = dataset.select(val_idx)\n",
        "print(f'Original train size: {len(train_orig)}, validation size: {len(val_orig)}')\n",
        "\n",
        "train_tok = train_orig.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=False,\n",
        "    remove_columns=['domain','title','doc','triples','entities','label_set','entity_label_set']\n",
        ")\n",
        "val_tok = val_orig.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=False,\n",
        "    remove_columns=['domain','title','doc','triples','entities','label_set','entity_label_set']\n",
        ")\n",
        "print(f'Documents truncated in training: {trunc_count} / {len(train_tok)}')\n",
        "\n",
        "train_ds = train_tok\n",
        "val_ds = val_tok\n",
        "print(f'Train set size: {len(train_ds)}, Validation set size: {len(val_ds)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7d0c48",
      "metadata": {
        "id": "1f7d0c48"
      },
      "source": [
        "# Baseline NER with Untrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025d714b",
      "metadata": {
        "id": "025d714b"
      },
      "outputs": [],
      "source": [
        "ner_pipe_untrained = pipeline(\n",
        "    'ner',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0,\n",
        "    aggregation_strategy='simple'\n",
        ")\n",
        "\n",
        "val_results = []\n",
        "for idx, example in enumerate(val_orig):\n",
        "    preds = ner_pipe_untrained(example['doc'])\n",
        "    val_results.append({\n",
        "        'index': idx,\n",
        "        'doc_title': example.get('title', f'doc_{idx}'),\n",
        "        'predictions': preds\n",
        "    })\n",
        "\n",
        "output_path = f'{base_path}processed/ner_untrained_predictions.json'\n",
        "save_model_output(val_results, output_path)\n",
        "\n",
        "scores_path = f'{base_path}processed/ner_untrained_scores.json'\n",
        "compute_f1(output_path, scores_path, val_orig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569023bb",
      "metadata": {
        "id": "569023bb"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y4WEIL8GRv9e",
      "metadata": {
        "id": "y4WEIL8GRv9e"
      },
      "outputs": [],
      "source": [
        "def make_safe_weights(\n",
        "    train_dataset: Dataset,\n",
        "    label_column: str = 'labels',\n",
        "    o_label_id: int = 0,\n",
        "    clip_range: tuple = (0.05, 3.0),\n",
        "    o_label_weight: float = 0.25,\n",
        ") -> torch.Tensor:\n",
        "    counts: Counter[int] = Counter()\n",
        "    for seq in train_dataset[label_column]:\n",
        "        for lbl in seq:\n",
        "            if lbl != -100:\n",
        "                counts[lbl] += 1\n",
        "\n",
        "    num_labels = max(counts) + 1\n",
        "\n",
        "    total = sum(counts.values())\n",
        "    inv_freq = {lbl: total / cnt for lbl, cnt in counts.items()}\n",
        "\n",
        "    mean_w = sum(inv_freq.values()) / len(inv_freq)\n",
        "    weights = {}\n",
        "    low, high = clip_range\n",
        "    for lbl in range(num_labels):\n",
        "        w = inv_freq.get(lbl, 1.0) / mean_w\n",
        "        w = max(low, min(w, high))\n",
        "        weights[lbl] = w\n",
        "\n",
        "    weights[o_label_id] = o_label_weight\n",
        "\n",
        "    weight_vector = torch.tensor(\n",
        "        [weights[i] for i in range(num_labels)],\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    return weight_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0w6n4GDR3ypC",
      "metadata": {
        "id": "0w6n4GDR3ypC"
      },
      "outputs": [],
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, *args, loss_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_weights = loss_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop('labels')\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = CrossEntropyLoss(\n",
        "            weight=self.loss_weights.to(model.device),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "        loss = loss_fct(\n",
        "            logits.view(-1, model.config.num_labels),\n",
        "            labels.view(-1)\n",
        "        )\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iDGq__8_HK0g",
      "metadata": {
        "id": "iDGq__8_HK0g"
      },
      "outputs": [],
      "source": [
        "from transformers.trainer_callback import TrainerState\n",
        "\n",
        "class StepPrinter(TrainerCallback):\n",
        "    def on_step_end(self, args, state: TrainerState, control, **kw):\n",
        "        if state.global_step % 10 == 0:\n",
        "            print('step', state.global_step, 'lr', trainer.optimizer.param_groups[0]['lr'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H2r5OCsIYwiP",
      "metadata": {
        "id": "H2r5OCsIYwiP"
      },
      "outputs": [],
      "source": [
        "def get_param_groups(model, base_lr=3e-5, decay=0.95):\n",
        "    groups = []\n",
        "    for n, p in model.named_parameters():\n",
        "        depth = n.count('encoder.layer')    # 0 for top, 11 for bottom\n",
        "        lr = base_lr * (decay ** depth)\n",
        "        groups.append({'params': [p], 'lr': lr})\n",
        "    return groups\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b30c2f",
      "metadata": {
        "id": "b9b30c2f"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "train_batch_size = 32\n",
        "eval_batch_size = 64\n",
        "gradient_accumulation_steps = 2\n",
        "num_epochs = 12\n",
        "learning_rate = 1e-5\n",
        "warmup_ratio = 0.1\n",
        "weight_decay = 0.01\n",
        "weight_vector = make_safe_weights(train_ds)\n",
        "print(f'Using learning_rate={learning_rate}, batch_size={train_batch_size}, epochs={num_epochs}')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='longformer-ner',\n",
        "    num_train_epochs        = num_epochs,\n",
        "    per_device_train_batch_size = train_batch_size,\n",
        "    per_device_eval_batch_size  = eval_batch_size,\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "    learning_rate = learning_rate,\n",
        "    warmup_ratio = warmup_ratio,\n",
        "    lr_scheduler_type = 'linear',\n",
        "    weight_decay = weight_decay,\n",
        "    fp16 = True,\n",
        "    gradient_checkpointing = True,\n",
        "    eval_strategy = 'steps',\n",
        "    eval_steps = 500,\n",
        "    logging_steps = 100,\n",
        "    save_strategy = 'steps',\n",
        "    save_total_limit = 2,\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'f1',\n",
        "    greater_is_better = True,\n",
        "    group_by_length = True,\n",
        "    seed = 42,\n",
        ")\n",
        "\n",
        "collator = DataCollatorForTokenClassification(\n",
        "    tokenizer,\n",
        "    pad_to_multiple_of=None,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "evaluator = evaluate.load('seqeval')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    preds = predictions.argmax(-1)\n",
        "    true_labels = [[id2label[l] for l in label_seq if l != -100] for label_seq in labels]\n",
        "    true_preds = [[id2label[p_] for (p_, l) in zip(pred_seq, label_seq) if l != -100]\n",
        "                  for pred_seq, label_seq in zip(preds, labels)]\n",
        "    results = evaluator.compute(predictions=true_preds, references=true_labels)\n",
        "    return {\n",
        "        'precision': results['overall_precision'],\n",
        "        'recall': results['overall_recall'],\n",
        "        'f1': results['overall_f1']\n",
        "    }\n",
        "\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collator,\n",
        "    loss_weights=weight_vector,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YOGKCutIGFU4",
      "metadata": {
        "id": "YOGKCutIGFU4"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca63db25",
      "metadata": {
        "id": "ca63db25"
      },
      "source": [
        "# NER on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e97528",
      "metadata": {
        "id": "d6e97528"
      },
      "outputs": [],
      "source": [
        "ner_pipe_finetuned = pipeline(\n",
        "    'ner',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0,\n",
        "    aggregation_strategy='simple'\n",
        ")\n",
        "\n",
        "val_results = []\n",
        "for idx, example in enumerate(val_orig):\n",
        "    preds = ner_pipe_finetuned(example['doc'])\n",
        "    val_results.append({\n",
        "        'index': idx,\n",
        "        'doc_title': example.get('title', f'doc_{idx}'),\n",
        "        'entities': preds,\n",
        "        'doc': example.get('doc')\n",
        "    })\n",
        "\n",
        "output_path = f'{base_path}processed/ner_trained_predictions.json'\n",
        "save_model_output(val_results, output_path)\n",
        "\n",
        "scores_path = f'{base_path}processed/ner_trained_scores.json'\n",
        "compute_f1(output_path, scores_path, val_orig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ta3nDN0j-uXm",
      "metadata": {
        "id": "Ta3nDN0j-uXm"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(f'{base_path}models/longformer');"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_project_p311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
