{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "300eb73b",
      "metadata": {
        "id": "300eb73b"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a9273274",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9273274",
        "outputId": "a853d1a5-9072-4bc6-d7a4-04d0b41e5ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets torch seqeval evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5f51e5",
      "metadata": {
        "id": "ff5f51e5"
      },
      "source": [
        "# Env Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5558fb9a",
      "metadata": {
        "id": "5558fb9a"
      },
      "outputs": [],
      "source": [
        "base_path = 'data/'\n",
        "max_token_length = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53def38a",
      "metadata": {
        "id": "53def38a"
      },
      "source": [
        "# Establish Google Drive Connection (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "785ce452",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "785ce452",
        "outputId": "0f2ffe04-ae61-41b4-f1e9-7a6f4d914e89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = 'drive/MyDrive/dataset/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4192d65",
      "metadata": {
        "id": "b4192d65"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "205b781e",
      "metadata": {
        "id": "205b781e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    LongformerTokenizerFast,\n",
        "    LongformerForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        "    TrainerCallback,\n",
        "    TrainerState,\n",
        "    TrainerControl,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import evaluate\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "from collections import Counter\n",
        "from typing import Dict, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32ad725",
      "metadata": {
        "id": "a32ad725"
      },
      "source": [
        "# Helper Functions\n",
        "\n",
        "## Load Data\n",
        "Loads all json files in a specified path and combines them in one aggregated list\n",
        "\n",
        "## Convert Numpy Floats\n",
        "\n",
        "## Save Model Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "31d8299a",
      "metadata": {
        "id": "31d8299a"
      },
      "outputs": [],
      "source": [
        "def load_json_data(folder_path):\n",
        "    aggregated_data = []\n",
        "\n",
        "    # loop through all files in the given folder\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file_name in files:\n",
        "            with open(f\"{folder_path}/{file_name}\", \"r\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            aggregated_data.append(data)\n",
        "\n",
        "    return aggregated_data\n",
        "\n",
        "# Convert NumPy float32 to native Python floats before JSON serialization\n",
        "def convert_numpy_floats(obj):\n",
        "    if isinstance(obj, np.float32):\n",
        "        return float(obj)\n",
        "    raise TypeError\n",
        "\n",
        "def save_model_output(output, output_path):\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(output, f, ensure_ascii=False, indent=2, default=convert_numpy_floats)\n",
        "    print(f\"Saved validation NER predictions to {output_path}\")\n",
        "\n",
        "def compute_f1(predictions_file_path, output_path, validation_dataset):\n",
        "  # Load saved predictions\n",
        "  with open(predictions_file_path, 'r') as f:\n",
        "      saved_preds = json.load(f)\n",
        "\n",
        "  # Prepare gold and predicted lists\n",
        "  all_gold = []\n",
        "  all_pred = []\n",
        "  for pred in saved_preds:\n",
        "      idx = pred['index']\n",
        "      gold_entities = validation_dataset[idx]['entities']\n",
        "      # flatten gold mentions: (mention_text, type)\n",
        "      gold_set = set()\n",
        "      for ent in gold_entities:\n",
        "          for m in ent['mentions']:\n",
        "              gold_set.add((m, ent['type']))\n",
        "      # flatten predicted mentions: pipeline outputs 'word' and 'entity_group'\n",
        "      pred_list = pred['predictions']\n",
        "      pred_set = set()\n",
        "      for p in pred_list:\n",
        "          w = p.get('word')\n",
        "          et = p.get('entity_group')\n",
        "          pred_set.add((w, et))\n",
        "      # Append to global lists\n",
        "      all_gold.append(gold_set)\n",
        "      all_pred.append(pred_set)\n",
        "\n",
        "  # Compute micro-level counts\n",
        "  tp = 0\n",
        "  pred_count = 0\n",
        "  gold_count = 0\n",
        "  for gold_set, pred_set in zip(all_gold, all_pred):\n",
        "      tp += len(gold_set & pred_set)\n",
        "      pred_count += len(pred_set)\n",
        "      gold_count += len(gold_set)\n",
        "\n",
        "  precision = tp / pred_count if pred_count > 0 else 0.0\n",
        "  recall = tp / gold_count if gold_count > 0 else 0.0\n",
        "  f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "  # Print and save metrics\n",
        "  metrics = {\n",
        "      'precision': precision,\n",
        "      'recall': recall,\n",
        "      'f1': f1,\n",
        "      'true_positives': tp,\n",
        "      'predicted': pred_count,\n",
        "      'gold': gold_count\n",
        "  }\n",
        "  print(\"NER Validation Mention-level Metrics:\")\n",
        "  print(metrics)\n",
        "\n",
        "  # Save metrics to JSON\n",
        "  with open(output_path, 'w') as f:\n",
        "      json.dump(metrics, f, indent=2)\n",
        "  print(f\"Saved evaluation metrics to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3281bd74",
      "metadata": {
        "id": "3281bd74"
      },
      "source": [
        "# Load data into Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9a156e72",
      "metadata": {
        "id": "9a156e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7fbe8a2-a857-4c20-f2f4-99a5551fdf32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample example:\n",
            "{'domain': 'Energy', 'title': 'Advanced_thermal_recycling_system', 'doc': 'An advanced thermal recycling system (or an ATR system) is the commercial brand name of the waste-to-energy incineration offering by Klean Power, which has been implemented in a single plant in Germany in 1999. WtE facilities such as the ATR transforms municipal solid waste (MSW) into electricity or steam for district heating or industrial customers. The combustion bottom ash, and the combustion fly ash, along with the air pollution control system fly ash, are treated to produce products that can be beneficially reused. Specifically, ATR systems consist of the following:\\nSolid waste combustion, boiler and combustion control system, energy recovery and air pollution control equipment;\\nCombustion bottom ash and fly ash treatment systems that produce commercially reusable products; and\\nAn optional pre-processing system to recover recyclable materials contained in the MSW delivered to the facility before the MSW enters the thermal processing area of the facility.\\nOne commercially operating ATR facility has been built so far. It is the Müllverwertung Rugenberger Damm WtE plant in Hamburg, Germany, commissioned in 1999. The German Green Party has endorsed the specific features of this facility in its \"Concept 2020\" initiative to cease all landfilling of waste by 2020 as an essential part of an integrated waste management system achieving the highest standards in the energy-from-waste industry. No landfilling of unprocessed waste has been allowed in Germany since 2005.Overhead refuse cranes are used to hold approximately five tons of garbage each. The waste is then mixed in the bunker to create a homogeneous mixture to ensure that the bottom ash byproduct has good combustion, and low carbon content. These cranes then deliver the mixed waste into the feeding hopper, which leads down onto stoker grates. These grates control the rate at which the waste travels through the boiler. The heat ignites the trash as it moves along the forward feeding grates until only the byproduct bottom ash remains at the end of the grate. Each combustion line feeds a boiler that operates above 1,560 °F (850 °C) for two seconds. The temperature in the combustion zone is measured through acoustic monitoring. A computer controls the temperature, the grate speed, the amount of air used, and all other aspects of the process that enable complete combustion and minimization of emissions.\\nMaintaining the furnace\\'s high temperature is essential to rid the waste and the resulting combustion gases of complex organic compounds such as dioxins and furans. To prevent the reformulation of pollutants, fly ash is separated from the flue gas downstream of the superheaters to reduce the fly ash content, which could act as a catalyst in the critical reformulation temperature range of 600 to 400 °F (316 to 204 °C). At the exit of the boiler, the flue gas is cooled down to a level of 340 °F (171 °C).\\nAs the waste is combusted, heat is released in the boiler. This heat produces high-pressure, high-temperature steam, which generates electrical energy when passed through a turbine generator. The electricity is fed into the public power grid or sold directly to a customer. The steam can also be exported directly for use in district heating or industrial processes.\\nEach unit has an independent air pollution control system. Flue gas cleaning begins in the boiler, where oxides of nitrogen are reduced by injecting ammonia water into the combustion chamber. Lightly loaded absorbents (activated carbon from the second bag house) are injected into the flue gas downstream of the first bag house to separate any contaminants that have reformed (such as organic compounds), any condensed heavy metals, salts and other gaseous contaminants, as well as residue fly ash.\\nThe first baghouse makes it possible to produce reusable by-products such as hydrochloric acid and gypsum from the consecutive air pollution control process steps. Acid gases are removed from the flue gases by passing through a two-stage scrubber to remove acid components, especially halogen compounds such as hydrochloric acid and hydrofluoric acid. A counter-flow neutral scrubber follows, using a lime slurry to remove sulphur oxides. The pollutant gases are either dissolved in water droplets (acids) or bound as calcium salts and thereby removed from the flue gas. A second baghouse acts as a polishing filter to capture any remaining aerosols, organic compounds and heavy metals, which thereby are reduced to levels usually below detection.\\nFollowing combustion, the material left consists of the non-combustible components of the waste and the inert materials produced during combustion. This is known as bottom ash. The bottom ash is washed to eliminate soluble salts. Iron scrap and non-ferrous metals such as aluminium, copper and brass are separated and sold in secondary metals markets. The bottom ash is then screened, crushed and sold for use as a construction material.\\nGypsum is created when the oxides of sulphur (SO2 and SO3) are separated by the single stage scrubber. It is purified, then sold to the construction industry.\\nThe acid scrubbing process in the flue gas treatment system also produces a raw hydrochloric acid at a concentration of 10% to 12%. The acid is distilled (rectified) to yield commercial-grade (30% concentration) hydrochloric acid.\\nFly ash, separated in the boiler and baghouses and constituting up to 5% by weight of the combusted MSW, is treated to recover metals and minerals for reuse, resulting in an overall ATR process landfill diversion rate of approximately 98.5%.\\n', 'entities': [{'id': 0, 'mentions': ['Müllverwertung Rugenberger Damm WtE plant', 'ATR systems', 'ATR system', 'ATR facility', 'advanced thermal recycling system'], 'type': 'PRODUCT'}, {'id': 1, 'mentions': ['Klean Power'], 'type': 'ORG'}, {'id': 2, 'mentions': ['Germany'], 'type': 'GPE'}, {'id': 3, 'mentions': ['1999'], 'type': 'DATE'}, {'id': 4, 'mentions': ['unprocessed waste', 'waste', 'MSW', 'garbage', 'municipal solid waste', 'mixed waste', 'landfilling of waste'], 'type': 'PRODUCT'}, {'id': 5, 'mentions': ['One'], 'type': 'CARDINAL'}, {'id': 6, 'mentions': ['Hamburg', 'Hamburg, Germany'], 'type': 'GPE'}, {'id': 7, 'mentions': ['German Green Party'], 'type': 'ORG'}, {'id': 8, 'mentions': ['2020'], 'type': 'DATE'}, {'id': 9, 'mentions': ['approximately five tons'], 'type': 'QUANTITY'}, {'id': 10, 'mentions': ['1,560 °'], 'type': 'QUANTITY'}, {'id': 11, 'mentions': ['850'], 'type': 'QUANTITY'}, {'id': 12, 'mentions': ['two seconds'], 'type': 'TIME'}, {'id': 13, 'mentions': ['600'], 'type': 'QUANTITY'}, {'id': 14, 'mentions': ['316'], 'type': 'QUANTITY'}, {'id': 15, 'mentions': ['204'], 'type': 'QUANTITY'}, {'id': 16, 'mentions': ['340'], 'type': 'QUANTITY'}, {'id': 17, 'mentions': ['second'], 'type': 'TIME'}, {'id': 18, 'mentions': ['first'], 'type': 'ORDINAL'}, {'id': 19, 'mentions': ['Gypsum', 'gypsum'], 'type': 'PRODUCT'}, {'id': 20, 'mentions': ['10% to 12%'], 'type': 'PERCENT'}, {'id': 21, 'mentions': ['30%'], 'type': 'PERCENT'}, {'id': 22, 'mentions': ['up to 5%'], 'type': 'PERCENT'}, {'id': 23, 'mentions': ['approximately 98.5%'], 'type': 'PERCENT'}, {'id': 24, 'mentions': ['combustion bottom ash', 'Combustion bottom ash', 'bottom ash'], 'type': 'MISC'}, {'id': 25, 'mentions': ['Fly ash', 'combustion fly ash', 'residue fly ash', 'fly ash'], 'type': 'MISC'}, {'id': 26, 'mentions': ['high-pressure, high-temperature steam'], 'type': 'MISC'}, {'id': 27, 'mentions': ['electrical energy'], 'type': 'MISC'}, {'id': 28, 'mentions': ['public power grid'], 'type': 'PRODUCT'}, {'id': 29, 'mentions': ['hydrochloric acid'], 'type': 'MISC'}, {'id': 30, 'mentions': ['two-stage scrubber'], 'type': 'PRODUCT'}, {'id': 31, 'mentions': ['acid scrubbing process', 'acid scrubbing process in the flue gas treatment system'], 'type': 'MISC'}], 'triples': [{'head': 'Klean Power', 'relation': 'OwnerOf', 'tail': 'advanced thermal recycling system'}, {'head': 'Müllverwertung Rugenberger Damm WtE plant', 'relation': 'LocatedIn', 'tail': 'Hamburg, Germany'}, {'head': 'advanced thermal recycling system', 'relation': 'SaidToBeTheSameAs', 'tail': 'waste-to-energy incineration offering'}, {'head': 'Müllverwertung Rugenberger Damm WtE plant', 'relation': 'LocatedIn', 'tail': 'Hamburg, Germany'}], 'label_set': ['Promoted', 'Replaces', 'Partner', 'NominatedFor', 'InterestedIn', 'NamedAfter', 'HasPart', 'SignificantEvent', 'MemberOf', 'InfluencedBy', 'ContainsAdministrativeTerritorialEntity', 'InOppositionTo', 'Country', 'SaidToBeTheSameAs', 'WorkLocation', 'DifferentFrom', 'LocatedIn', 'HasWorksInTheCollection', 'PartOf', 'Creator', 'AcademicDegree', 'UsedBy', 'EducatedAt', 'CountryOfCitizenship', 'Author', 'DiplomaticRelation', 'OwnerOf', 'Follows', 'FollowedBy', 'OwnedBy'], 'entity_label_set': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART', 'MISC']}\n"
          ]
        }
      ],
      "source": [
        "# Load JSON files and store them in memory\n",
        "aggregated_data = []\n",
        "folder_path = f'{base_path}raw/train'\n",
        "\n",
        "# loop through all files in the given folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file_name in files:\n",
        "        with open(f\"{folder_path}/{file_name}\", \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for d in data:\n",
        "          aggregated_data.append(d)\n",
        "\n",
        "dataset = Dataset.from_list(aggregated_data)\n",
        "print(\"Sample example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f46f4dd",
      "metadata": {
        "id": "9f46f4dd"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9e5bfbcf",
      "metadata": {
        "id": "9e5bfbcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c9c58e-8dd7-4d54-d43f-2f6161817b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'B-CARDINAL', 'B-DATE', 'B-EVENT', 'B-FAC', 'B-GPE', 'B-LANGUAGE', 'B-LAW', 'B-LOC', 'B-MONEY', 'B-NORP', 'B-ORDINAL', 'B-ORG', 'B-PERCENT', 'B-PERSON', 'B-PRODUCT', 'B-QUANTITY', 'B-TIME', 'B-WORK_OF_ART', 'B-MISC', 'I-CARDINAL', 'I-DATE', 'I-EVENT', 'I-FAC', 'I-GPE', 'I-LANGUAGE', 'I-LAW', 'I-LOC', 'I-MONEY', 'I-NORP', 'I-ORDINAL', 'I-ORG', 'I-PERCENT', 'I-PERSON', 'I-PRODUCT', 'I-QUANTITY', 'I-TIME', 'I-WORK_OF_ART', 'I-MISC']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'allenai/longformer-base-4096'\n",
        "\n",
        "# Prepare label mappings\n",
        "entity_labels = dataset[0]['entity_label_set']  # list of entity types\n",
        "label_list = ['O'] + [f\"B-{l}\" for l in entity_labels] + [f\"I-{l}\" for l in entity_labels]\n",
        "print(label_list)\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "# Tokenizer and model init\n",
        "tokenizer = LongformerTokenizerFast.from_pretrained(\n",
        "    model_name,\n",
        "    max_length = max_token_length\n",
        ")\n",
        "\n",
        "model = LongformerForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4bbd478",
      "metadata": {
        "id": "a4bbd478"
      },
      "source": [
        "# Split Data into Validation and Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4fb49594",
      "metadata": {
        "id": "4fb49594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132,
          "referenced_widgets": [
            "6938d26e7e5b4d5fbf29309bc7b9d811",
            "91d4140d55f74fd59a5a6f699916d8aa",
            "cb3ff0f72c294154ab387f2309f829f8",
            "028dd4e760df41aab9391b6f0275ca1c",
            "e2c437f2aef34189b84a3a3c9f3897da",
            "9878cf94fb6b43148961246f45f425e0",
            "1c13941283af4a3e85838cf0d8f3633f",
            "624db6b255604c2b803f40a0c28b3bdc",
            "737b4f07cfdd491c99fd66ba442b2fe9",
            "da35c7f6df5b47929d03130b35dd3147",
            "9dd2b5e405ab4b56b27459cf6dd2b77c",
            "34f212dbdacc4d0395dde6f18fe35e3e",
            "ff5fdb895bd641f7b499a56fbce53a33",
            "3041af9c35514157a7c0f30120fdeeb5",
            "8f63fce4a00d4a4a91bd5affdad7a963",
            "5fb6f672a01343a89fa34747b5b0c1e6",
            "49c51890971141ac9d99475ce47a5c51",
            "67836243983145a88ccd7c93571b83a9",
            "adacd25cedf54dac8cf58591fbfa9535",
            "23c5a6790c05434790ed55d1837524a2",
            "a390836b03ca4068a5150e2d70a1837d",
            "a91d820aea4b4bea9ec718096aadd5e7"
          ]
        },
        "outputId": "3a0254f8-e2eb-4b09-e6cf-41faeb0c8c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train size: 945, validation size: 106\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/945 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6938d26e7e5b4d5fbf29309bc7b9d811"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/106 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34f212dbdacc4d0395dde6f18fe35e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents truncated in training: 0 / 945\n",
            "Train set size: 945, Validation set size: 106\n"
          ]
        }
      ],
      "source": [
        "# Function to tokenize and align labels\n",
        "trunc_count = 0\n",
        "\n",
        "whitespace = re.compile(r\"\\s\")\n",
        "\n",
        "def is_word_start(text: str, char_idx: int) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic: a character is the start of a word if it is at position 0\n",
        "    or the previous character is any whitespace.\n",
        "    Works well for normal prose tokenised with a BPE WordPiece tokenizer.\n",
        "    \"\"\"\n",
        "    return char_idx == 0 or bool(whitespace.match(text[char_idx - 1]))\n",
        "\n",
        "def tokenize_and_align_labels(example: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    • Pads / truncates to `max_token_length`.\n",
        "    • Sets label = -100 on:\n",
        "        – special tokens ([CLS], [SEP], etc.)\n",
        "        – all sub-tokens *except* the first piece of each word\n",
        "        – all padding tokens\n",
        "    • Emits exactly one label per word, using your B-/I- scheme.\n",
        "    \"\"\"\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        example[\"doc\"],\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_token_length,\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 1. Init every position with ignore_index (-100)\n",
        "    # ------------------------------------------------------------------\n",
        "    labels = [-100] * len(encoding[\"input_ids\"])\n",
        "    doc_text = example[\"doc\"]\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 2. Mark the first sub-token of every *word* as O\n",
        "    # ------------------------------------------------------------------\n",
        "    for idx, (off_start, off_end) in enumerate(encoding[\"offset_mapping\"]):\n",
        "        if off_start == off_end:          # special tokens → keep -100\n",
        "            continue\n",
        "        if is_word_start(doc_text, off_start):\n",
        "            labels[idx] = label2id[\"O\"]   # will be overwritten if it is an entity\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 3. Overwrite labels for entity mentions\n",
        "    # ------------------------------------------------------------------\n",
        "    for ent in example[\"entities\"]:\n",
        "        ent_type = ent[\"type\"]            # e.g. \"PERSON\"\n",
        "        for m_text in ent[\"mentions\"]:\n",
        "            for match in re.finditer(re.escape(m_text), doc_text):\n",
        "                start_char, end_char = match.start(), match.end()\n",
        "\n",
        "                for idx, (off_start, off_end) in enumerate(encoding[\"offset_mapping\"]):\n",
        "                    if off_start >= start_char and off_end <= end_char:\n",
        "                        if off_start == start_char:\n",
        "                            labels[idx] = label2id[f\"B-{ent_type}\"]\n",
        "                        elif is_word_start(doc_text, off_start):\n",
        "                            labels[idx] = label2id[f\"I-{ent_type}\"]\n",
        "                        # every other sub-token stays -100\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 4. Remove the offsets (Trainer doesn’t need them) and attach labels\n",
        "    # ------------------------------------------------------------------\n",
        "    encoding.pop(\"offset_mapping\")\n",
        "    encoding[\"labels\"] = labels\n",
        "    return encoding\n",
        "\n",
        "# Split original dataset into train and validation (preserve raw columns)\n",
        "all_indices = list(range(len(dataset)))\n",
        "train_idx, val_idx = train_test_split(all_indices, test_size=0.1, random_state=42)\n",
        "train_orig = dataset.select(train_idx)\n",
        "val_orig = dataset.select(val_idx)\n",
        "print(f\"Original train size: {len(train_orig)}, validation size: {len(val_orig)}\")\n",
        "\n",
        "# Tokenize & align labels separately, removing raw columns only from tokenized sets\n",
        "train_tok = train_orig.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=False,\n",
        "    remove_columns=['domain','title','doc','triples','entities','label_set','entity_label_set']\n",
        ")\n",
        "val_tok = val_orig.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=False,\n",
        "    remove_columns=['domain','title','doc','triples','entities','label_set','entity_label_set']\n",
        ")\n",
        "print(f\"Documents truncated in training: {trunc_count} / {len(train_tok)}\")\n",
        "\n",
        "# Use tokenized datasets for training and evaluation\n",
        "train_ds = train_tok\n",
        "val_ds = val_tok\n",
        "print(f\"Train set size: {len(train_ds)}, Validation set size: {len(val_ds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7d0c48",
      "metadata": {
        "id": "1f7d0c48"
      },
      "source": [
        "# Baseline NER with Untrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025d714b",
      "metadata": {
        "id": "025d714b"
      },
      "outputs": [],
      "source": [
        "# Create NER pipeline using the fine-tuned model\n",
        "ner_pipe_untrained = pipeline(\n",
        "    'ner',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0,\n",
        "    aggregation_strategy='simple'\n",
        ")\n",
        "\n",
        "# Run NER on validation documents and collect aggregated results\n",
        "val_results = []\n",
        "for idx, example in enumerate(val_orig):\n",
        "    preds = ner_pipe_untrained(example['doc'])\n",
        "    val_results.append({\n",
        "        'index': idx,\n",
        "        'doc_title': example.get('title', f'doc_{idx}'),\n",
        "        'predictions': preds\n",
        "    })\n",
        "\n",
        "# Save to JSON in Google Drive folder\n",
        "output_path = f'{base_path}processed/ner_untrained_predictions.json'\n",
        "save_model_output(val_results, output_path)\n",
        "\n",
        "scores_path = f'{base_path}processed/ner_untrained_scores.json'\n",
        "compute_f1(output_path, scores_path, val_orig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569023bb",
      "metadata": {
        "id": "569023bb"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "y4WEIL8GRv9e",
      "metadata": {
        "id": "y4WEIL8GRv9e"
      },
      "outputs": [],
      "source": [
        "def make_safe_weights(\n",
        "    train_dataset: Dataset,\n",
        "    label_column: str = \"labels\",\n",
        "    o_label_id: int = 0,\n",
        "    clip_range: tuple = (0.05, 5.0),\n",
        "    o_label_weight: float = 0.10,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Build a class-weight tensor for token-level NER that\n",
        "\n",
        "    1. is inverse-frequency based (rare labels ↑ weight);\n",
        "    2. has mean weight = 1 (keeps the overall loss scale stable);\n",
        "    3. is clipped to `clip_range` to avoid huge gradients;\n",
        "    4. overwrites the 'O' label weight with `o_label_weight`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_dataset : datasets.Dataset\n",
        "        Your training split after any `-100` masking.\n",
        "    label_column : str\n",
        "        Column that holds the integer tag sequences.\n",
        "    o_label_id : int\n",
        "        ID of the majority 'O' label (usually 0).\n",
        "    clip_range : (float, float)\n",
        "        Min / max weight allowed after normalisation.\n",
        "    o_label_weight : float\n",
        "        Final weight assigned to the 'O' label.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor  shape = (num_labels,)\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Count how many *labelled* tokens of each class you have\n",
        "    # ------------------------------------------------------------------\n",
        "    counts: Counter[int] = Counter()\n",
        "    for seq in train_dataset[label_column]:\n",
        "        for lbl in seq:\n",
        "            if lbl != -100:          # ignore the sub-token masks\n",
        "                counts[lbl] += 1\n",
        "\n",
        "    num_labels = max(counts) + 1     # assumes label ids are 0 … N-1\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Inverse-frequency weighting\n",
        "    # ------------------------------------------------------------------\n",
        "    total = sum(counts.values())\n",
        "    inv_freq = {lbl: total / cnt for lbl, cnt in counts.items()}\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Normalise so that mean(weight)=1, then clip\n",
        "    # ------------------------------------------------------------------\n",
        "    mean_w = sum(inv_freq.values()) / len(inv_freq)\n",
        "    weights = {}\n",
        "    low, high = clip_range\n",
        "    for lbl in range(num_labels):\n",
        "        w = inv_freq.get(lbl, 1.0) / mean_w     # unseen lbls → weight 1\n",
        "        w = max(low, min(w, high))              # clip to safe range\n",
        "        weights[lbl] = w\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Down-weight the 'O' label explicitly\n",
        "    # ------------------------------------------------------------------\n",
        "    weights[o_label_id] = o_label_weight\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Build tensor\n",
        "    # ------------------------------------------------------------------\n",
        "    weight_vector = torch.tensor(\n",
        "        [weights[i] for i in range(num_labels)],\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    return weight_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0w6n4GDR3ypC",
      "metadata": {
        "id": "0w6n4GDR3ypC"
      },
      "outputs": [],
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, *args, loss_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_weights = loss_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = CrossEntropyLoss(\n",
        "            weight=self.loss_weights.to(model.device),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "        # reshape to (batch_size*seq_len, num_labels)\n",
        "        loss = loss_fct(\n",
        "            logits.view(-1, model.config.num_labels),\n",
        "            labels.view(-1)\n",
        "        )\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_callback import TrainerState\n",
        "\n",
        "class StepPrinter(TrainerCallback):\n",
        "    def on_step_end(self, args, state: TrainerState, control, **kw):\n",
        "        if state.global_step % 10 == 0:\n",
        "            print(\"step\", state.global_step, \"lr\", trainer.optimizer.param_groups[0]['lr'])\n",
        "\n"
      ],
      "metadata": {
        "id": "iDGq__8_HK0g"
      },
      "id": "iDGq__8_HK0g",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_param_groups(model, base_lr=3e-5, decay=0.95):\n",
        "    groups = []\n",
        "    for n, p in model.named_parameters():\n",
        "        depth = n.count(\"encoder.layer\")    # 0 for top, 11 for bottom\n",
        "        lr = base_lr * (decay ** depth)\n",
        "        groups.append({\"params\": [p], \"lr\": lr})\n",
        "    return groups\n"
      ],
      "metadata": {
        "id": "H2r5OCsIYwiP"
      },
      "id": "H2r5OCsIYwiP",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b9b30c2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9b30c2f",
        "outputId": "72da16e9-be9e-4537-f29c-6355a030f925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using learning_rate=5e-05, batch_size=2, epochs=10, warmup_steps=15, weight_decay=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-437b5f515d10>:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "# Training arguments\n",
        "train_batch_size = 2\n",
        "gradient_accumulation_steps = 32\n",
        "num_epochs = 10\n",
        "learning_rate = 5e-5\n",
        "total_steps = math.ceil(len(train_ds) / train_batch_size / gradient_accumulation_steps) * num_epochs\n",
        "warmup_steps = int(total_steps * 0.1)\n",
        "weight_vector = make_safe_weights(train_ds)\n",
        "print(f\"Using learning_rate={learning_rate}, batch_size={train_batch_size}, epochs={num_epochs}, warmup_steps={warmup_steps}, weight_decay=0.01\")\n",
        "\n",
        "\"\"\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./models/longformer-ner',\n",
        "    num_train_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=train_batch_size,\n",
        "    warmup_steps=warmup_steps,\n",
        "    weight_decay=0.01,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./models/longformer/1',\n",
        "    num_train_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_ratio=0.1,\n",
        "    warmup_steps=warmup_steps,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=False,\n",
        "    fp16=True,\n",
        "\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(get_param_groups(model), eps=1e-8)\n",
        "\n",
        "# Metric computation\n",
        "evaluator = evaluate.load('seqeval')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    preds = predictions.argmax(-1)\n",
        "    true_labels = [[id2label[l] for l in label_seq if l != -100] for label_seq in labels]\n",
        "    true_preds = [[id2label[p_] for (p_, l) in zip(pred_seq, label_seq) if l != -100]\n",
        "                  for pred_seq, label_seq in zip(preds, labels)]\n",
        "    results = evaluator.compute(predictions=true_preds, references=true_labels)\n",
        "    return {\n",
        "        'precision': results['overall_precision'],\n",
        "        'recall': results['overall_recall'],\n",
        "        'f1': results['overall_f1']\n",
        "    }\n",
        "\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    #loss_weights=weight_vector,\n",
        "    optimizers=(optimizer, None),\n",
        "    callbacks=[StepPrinter()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "JWCYFUC9Qg3r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWCYFUC9Qg3r",
        "outputId": "4515a8a6-4b00-4324-d5c5-f61fe3ee56b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1000, 0.0500, 0.0500, 0.5205, 0.3535, 0.0500, 2.3421, 0.0500, 0.3346,\n",
            "        0.1193, 0.0500, 0.0500, 0.0500, 0.1142, 0.0500, 0.1874, 0.3287, 1.3383,\n",
            "        0.1523, 0.0500, 0.2465, 0.0500, 0.2639, 0.3176, 0.1301, 5.0000, 0.3470,\n",
            "        0.4931, 0.0684, 0.1629, 5.0000, 0.0500, 0.0914, 0.0500, 0.2974, 0.3674,\n",
            "        2.0819, 0.0910, 0.0500])\n"
          ]
        }
      ],
      "source": [
        "print(weight_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "YOGKCutIGFU4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "YOGKCutIGFU4",
        "outputId": "3e55a5f3-3adb-4fc9-d5e3-33225ac5d80c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-63a3bfbe6c37>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total truncated documents: {trunc_count} / {len(dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2563\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2565\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2566\u001b[0m                     ):\n\u001b[1;32m   2567\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "\n",
        "print(f\"Total truncated documents: {trunc_count} / {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. how many encoder params are actually trainable?\n",
        "n_trainable = sum(p.requires_grad for p in model.parameters())\n",
        "n_total     = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{n_trainable/n_total:.2%} parameters trainable\")\n",
        "\n",
        "# 2. evaluate on the TRAIN split\n",
        "train_metrics = trainer.evaluate(train_ds)\n",
        "print(train_metrics[\"eval_f1\"])\n"
      ],
      "metadata": {
        "id": "DH8qez1c-qdA",
        "outputId": "fe7e032a-df89-44f5-d2e0-c9240907e569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "id": "DH8qez1c-qdA",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00% parameters trainable\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='473' max='473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [473/473 01:32]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4253687579238971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in model.longformer.parameters():\n",
        "    p.requires_grad_(True)"
      ],
      "metadata": {
        "id": "kffWmDGlAAD5"
      },
      "id": "kffWmDGlAAD5",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total     = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{trainable/total:.2%} of parameters will update\")   # should say ~100 %\n"
      ],
      "metadata": {
        "id": "rCtEA81rAJvA",
        "outputId": "08dafd9f-8844-4cc7-85fb-5a5a096b6929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rCtEA81rAJvA",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.00% of parameters will update\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca63db25",
      "metadata": {
        "id": "ca63db25"
      },
      "source": [
        "# NER on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e97528",
      "metadata": {
        "id": "d6e97528"
      },
      "outputs": [],
      "source": [
        "# Create NER pipeline using the fine-tuned model\n",
        "ner_pipe_finetuned = pipeline(\n",
        "    'ner',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0,\n",
        "    aggregation_strategy='simple'\n",
        ")\n",
        "\n",
        "# Run NER on validation documents and collect aggregated results\n",
        "val_results = []\n",
        "for idx, example in enumerate(val_orig):\n",
        "    preds = ner_pipe_finetuned(example['doc'])\n",
        "    val_results.append({\n",
        "        'index': idx,\n",
        "        #'doc_title': example.get('title', f'doc_{idx}'),\n",
        "        'predictions': preds\n",
        "    })\n",
        "\n",
        "# Save to JSON in Google Drive folder\n",
        "output_path = f'{base_path}processed/ner_trained_predictions.json'\n",
        "save_model_output(val_results, output_path)\n",
        "\n",
        "scores_path = f'{base_path}processed/ner_trained_scores.json'\n",
        "compute_f1(output_path, scores_path, val_orig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n-qcvU47VvfK",
      "metadata": {
        "id": "n-qcvU47VvfK"
      },
      "outputs": [],
      "source": [
        "train_ds.save_to_disk(f'{base_path}processed/train_ds')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6938d26e7e5b4d5fbf29309bc7b9d811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91d4140d55f74fd59a5a6f699916d8aa",
              "IPY_MODEL_cb3ff0f72c294154ab387f2309f829f8",
              "IPY_MODEL_028dd4e760df41aab9391b6f0275ca1c"
            ],
            "layout": "IPY_MODEL_e2c437f2aef34189b84a3a3c9f3897da"
          }
        },
        "91d4140d55f74fd59a5a6f699916d8aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9878cf94fb6b43148961246f45f425e0",
            "placeholder": "​",
            "style": "IPY_MODEL_1c13941283af4a3e85838cf0d8f3633f",
            "value": "Map: 100%"
          }
        },
        "cb3ff0f72c294154ab387f2309f829f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_624db6b255604c2b803f40a0c28b3bdc",
            "max": 945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_737b4f07cfdd491c99fd66ba442b2fe9",
            "value": 945
          }
        },
        "028dd4e760df41aab9391b6f0275ca1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da35c7f6df5b47929d03130b35dd3147",
            "placeholder": "​",
            "style": "IPY_MODEL_9dd2b5e405ab4b56b27459cf6dd2b77c",
            "value": " 945/945 [00:02&lt;00:00, 455.59 examples/s]"
          }
        },
        "e2c437f2aef34189b84a3a3c9f3897da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9878cf94fb6b43148961246f45f425e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c13941283af4a3e85838cf0d8f3633f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "624db6b255604c2b803f40a0c28b3bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "737b4f07cfdd491c99fd66ba442b2fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da35c7f6df5b47929d03130b35dd3147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd2b5e405ab4b56b27459cf6dd2b77c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34f212dbdacc4d0395dde6f18fe35e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff5fdb895bd641f7b499a56fbce53a33",
              "IPY_MODEL_3041af9c35514157a7c0f30120fdeeb5",
              "IPY_MODEL_8f63fce4a00d4a4a91bd5affdad7a963"
            ],
            "layout": "IPY_MODEL_5fb6f672a01343a89fa34747b5b0c1e6"
          }
        },
        "ff5fdb895bd641f7b499a56fbce53a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49c51890971141ac9d99475ce47a5c51",
            "placeholder": "​",
            "style": "IPY_MODEL_67836243983145a88ccd7c93571b83a9",
            "value": "Map: 100%"
          }
        },
        "3041af9c35514157a7c0f30120fdeeb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adacd25cedf54dac8cf58591fbfa9535",
            "max": 106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23c5a6790c05434790ed55d1837524a2",
            "value": 106
          }
        },
        "8f63fce4a00d4a4a91bd5affdad7a963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a390836b03ca4068a5150e2d70a1837d",
            "placeholder": "​",
            "style": "IPY_MODEL_a91d820aea4b4bea9ec718096aadd5e7",
            "value": " 106/106 [00:00&lt;00:00, 415.41 examples/s]"
          }
        },
        "5fb6f672a01343a89fa34747b5b0c1e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49c51890971141ac9d99475ce47a5c51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67836243983145a88ccd7c93571b83a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adacd25cedf54dac8cf58591fbfa9535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c5a6790c05434790ed55d1837524a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a390836b03ca4068a5150e2d70a1837d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a91d820aea4b4bea9ec718096aadd5e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}