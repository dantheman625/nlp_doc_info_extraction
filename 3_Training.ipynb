{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "300eb73b",
      "metadata": {
        "id": "300eb73b"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9273274",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9273274",
        "outputId": "78e2d5a5-90fa-43fa-830e-e177518eb11d"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch seqeval evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5f51e5",
      "metadata": {
        "id": "ff5f51e5"
      },
      "source": [
        "# Env Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5558fb9a",
      "metadata": {
        "id": "5558fb9a"
      },
      "outputs": [],
      "source": [
        "base_path = 'data/'\n",
        "max_token_length = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53def38a",
      "metadata": {
        "id": "53def38a"
      },
      "source": [
        "# Establish Google Drive Connection (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785ce452",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "785ce452",
        "outputId": "fef05cc2-3824-4f72-94a5-60e997a4011d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = 'drive/MyDrive/dataset/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4192d65",
      "metadata": {
        "id": "b4192d65"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "205b781e",
      "metadata": {
        "id": "205b781e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    LongformerTokenizerFast,\n",
        "    LongformerForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        "    TrainerCallback,\n",
        "    TrainerState,\n",
        "    TrainerControl,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import evaluate\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "from collections import Counter\n",
        "from typing import Dict, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32ad725",
      "metadata": {
        "id": "a32ad725"
      },
      "source": [
        "# Helper Functions\n",
        "\n",
        "## Load Data\n",
        "Loads all json files in a specified path and combines them in one aggregated list\n",
        "\n",
        "## Convert Numpy Floats\n",
        "\n",
        "## Save Model Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d8299a",
      "metadata": {
        "id": "31d8299a"
      },
      "outputs": [],
      "source": [
        "def load_json_data(folder_path):\n",
        "    aggregated_data = []\n",
        "\n",
        "    # loop through all files in the given folder\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file_name in files:\n",
        "            with open(f\"{folder_path}/{file_name}\", \"r\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            aggregated_data.append(data)\n",
        "\n",
        "    return aggregated_data\n",
        "\n",
        "# Convert NumPy float32 to native Python floats before JSON serialization\n",
        "def convert_numpy_floats(obj):\n",
        "    if isinstance(obj, np.float32):\n",
        "        return float(obj)\n",
        "    raise TypeError\n",
        "\n",
        "def save_model_output(output, output_path):\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(output, f, ensure_ascii=False, indent=2, default=convert_numpy_floats)\n",
        "    print(f\"Saved validation NER predictions to {output_path}\")\n",
        "\n",
        "def compute_f1(predictions_file_path, output_path, validation_dataset):\n",
        "  # Load saved predictions\n",
        "  with open(predictions_file_path, 'r') as f:\n",
        "      saved_preds = json.load(f)\n",
        "\n",
        "  # Prepare gold and predicted lists\n",
        "  all_gold = []\n",
        "  all_pred = []\n",
        "  for pred in saved_preds:\n",
        "      idx = pred['index']\n",
        "      gold_entities = validation_dataset[idx]['entities']\n",
        "      # flatten gold mentions: (mention_text, type)\n",
        "      gold_set = set()\n",
        "      for ent in gold_entities:\n",
        "          for m in ent['mentions']:\n",
        "              gold_set.add((m, ent['type']))\n",
        "      # flatten predicted mentions: pipeline outputs 'word' and 'entity_group'\n",
        "      pred_list = pred['predictions']\n",
        "      pred_set = set()\n",
        "      for p in pred_list:\n",
        "          w = p.get('word')\n",
        "          et = p.get('entity_group')\n",
        "          pred_set.add((w, et))\n",
        "      # Append to global lists\n",
        "      all_gold.append(gold_set)\n",
        "      all_pred.append(pred_set)\n",
        "\n",
        "  # Compute micro-level counts\n",
        "  tp = 0\n",
        "  pred_count = 0\n",
        "  gold_count = 0\n",
        "  for gold_set, pred_set in zip(all_gold, all_pred):\n",
        "      tp += len(gold_set & pred_set)\n",
        "      pred_count += len(pred_set)\n",
        "      gold_count += len(gold_set)\n",
        "\n",
        "  precision = tp / pred_count if pred_count > 0 else 0.0\n",
        "  recall = tp / gold_count if gold_count > 0 else 0.0\n",
        "  f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "  # Print and save metrics\n",
        "  metrics = {\n",
        "      'precision': precision,\n",
        "      'recall': recall,\n",
        "      'f1': f1,\n",
        "      'true_positives': tp,\n",
        "      'predicted': pred_count,\n",
        "      'gold': gold_count\n",
        "  }\n",
        "  print(\"NER Validation Mention-level Metrics:\")\n",
        "  print(metrics)\n",
        "\n",
        "  # Save metrics to JSON\n",
        "  with open(output_path, 'w') as f:\n",
        "      json.dump(metrics, f, indent=2)\n",
        "  print(f\"Saved evaluation metrics to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3281bd74",
      "metadata": {
        "id": "3281bd74"
      },
      "source": [
        "# Load data into Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a156e72",
      "metadata": {
        "id": "9a156e72"
      },
      "outputs": [],
      "source": [
        "# Load JSON files and store them in memory\n",
        "aggregated_data = []\n",
        "folder_path = f'{base_path}raw/train'\n",
        "\n",
        "# loop through all files in the given folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file_name in files:\n",
        "        with open(f\"{folder_path}/{file_name}\", \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for d in data:\n",
        "          aggregated_data.append(d)\n",
        "\n",
        "dataset = Dataset.from_list(aggregated_data)\n",
        "print(\"Sample example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f46f4dd",
      "metadata": {
        "id": "9f46f4dd"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5bfbcf",
      "metadata": {
        "id": "9e5bfbcf"
      },
      "outputs": [],
      "source": [
        "model_name = 'allenai/longformer-base-4096'\n",
        "\n",
        "# Prepare label mappings\n",
        "entity_labels = dataset[0]['entity_label_set']  # list of entity types\n",
        "label_list = ['O'] + [f\"B-{l}\" for l in entity_labels] + [f\"I-{l}\" for l in entity_labels]\n",
        "print(label_list)\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "# Tokenizer and model init\n",
        "tokenizer = LongformerTokenizerFast.from_pretrained(\n",
        "    model_name,\n",
        "    max_length = max_token_length\n",
        ")\n",
        "\n",
        "model = LongformerForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4bbd478",
      "metadata": {
        "id": "a4bbd478"
      },
      "source": [
        "# Split Data into Validation and Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb49594",
      "metadata": {
        "id": "4fb49594"
      },
      "outputs": [],
      "source": [
        "# Function to tokenize and align labels\n",
        "trunc_count = 0\n",
        "\n",
        "whitespace = re.compile(r\"\\s\")\n",
        "\n",
        "def is_word_start(text: str, char_idx: int) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic: a character is the start of a word if it is at position 0\n",
        "    or the previous character is any whitespace.\n",
        "    Works well for normal prose tokenised with a BPE WordPiece tokenizer.\n",
        "    \"\"\"\n",
        "    return char_idx == 0 or bool(whitespace.match(text[char_idx - 1]))\n",
        "\n",
        "def tokenize_and_align_labels(example: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    • Pads / truncates to `max_token_length`.\n",
        "    • Sets label = -100 on:\n",
        "        – special tokens ([CLS], [SEP], etc.)\n",
        "        – all sub-tokens *except* the first piece of each word\n",
        "        – all padding tokens\n",
        "    • Emits exactly one label per word, using your B-/I- scheme.\n",
        "    \"\"\"\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        example[\"doc\"],\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_token_length,\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 1. Init every position with ignore_index (-100)\n",
        "    # ------------------------------------------------------------------\n",
        "    labels = [-100] * len(encoding[\"input_ids\"])\n",
        "    doc_text = example[\"doc\"]\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 2. Mark the first sub-token of every *word* as O\n",
        "    # ------------------------------------------------------------------\n",
        "    for idx, (off_start, off_end) in enumerate(encoding[\"offset_mapping\"]):\n",
        "        if off_start == off_end:          # special tokens → keep -100\n",
        "            continue\n",
        "        if is_word_start(doc_text, off_start):\n",
        "            labels[idx] = label2id[\"O\"]   # will be overwritten if it is an entity\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 3. Overwrite labels for entity mentions\n",
        "    # ------------------------------------------------------------------\n",
        "    for ent in example[\"entities\"]:\n",
        "        ent_type = ent[\"type\"]            # e.g. \"PERSON\"\n",
        "        for m_text in ent[\"mentions\"]:\n",
        "            for match in re.finditer(re.escape(m_text), doc_text):\n",
        "                start_char, end_char = match.start(), match.end()\n",
        "\n",
        "                for idx, (off_start, off_end) in enumerate(encoding[\"offset_mapping\"]):\n",
        "                    if off_start >= start_char and off_end <= end_char:\n",
        "                        if off_start == start_char:\n",
        "                            labels[idx] = label2id[f\"B-{ent_type}\"]\n",
        "                        elif is_word_start(doc_text, off_start):\n",
        "                            labels[idx] = label2id[f\"I-{ent_type}\"]\n",
        "                        # every other sub-token stays -100\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 4. Remove the offsets (Trainer doesn’t need them) and attach labels\n",
        "    # ------------------------------------------------------------------\n",
        "    encoding.pop(\"offset_mapping\")\n",
        "    encoding[\"labels\"] = labels\n",
        "    return encoding\n",
        "\n",
        "# Split original dataset into train and validation (preserve raw columns)\n",
        "all_indices = list(range(len(dataset)))\n",
        "train_idx, val_idx = train_test_split(all_indices, test_size=0.1, random_state=42)\n",
        "train_orig = dataset.select(train_idx)\n",
        "val_orig = dataset.select(val_idx)\n",
        "print(f\"Original train size: {len(train_orig)}, validation size: {len(val_orig)}\")\n",
        "\n",
        "# Tokenize & align labels separately, removing raw columns only from tokenized sets\n",
        "train_tok = train_orig.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=False,\n",
        "    remove_columns=['domain','title','doc','triples','entities','label_set','entity_label_set']\n",
        ")\n",
        "val_tok = val_orig.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=False,\n",
        "    remove_columns=['domain','title','doc','triples','entities','label_set','entity_label_set']\n",
        ")\n",
        "print(f\"Documents truncated in training: {trunc_count} / {len(train_tok)}\")\n",
        "\n",
        "# Use tokenized datasets for training and evaluation\n",
        "train_ds = train_tok\n",
        "val_ds = val_tok\n",
        "print(f\"Train set size: {len(train_ds)}, Validation set size: {len(val_ds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7d0c48",
      "metadata": {
        "id": "1f7d0c48"
      },
      "source": [
        "# Baseline NER with Untrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025d714b",
      "metadata": {
        "id": "025d714b"
      },
      "outputs": [],
      "source": [
        "# Create NER pipeline using the fine-tuned model\n",
        "ner_pipe_untrained = pipeline(\n",
        "    'ner',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0,\n",
        "    aggregation_strategy='simple'\n",
        ")\n",
        "\n",
        "# Run NER on validation documents and collect aggregated results\n",
        "val_results = []\n",
        "for idx, example in enumerate(val_orig):\n",
        "    preds = ner_pipe_untrained(example['doc'])\n",
        "    val_results.append({\n",
        "        'index': idx,\n",
        "        'doc_title': example.get('title', f'doc_{idx}'),\n",
        "        'predictions': preds\n",
        "    })\n",
        "\n",
        "# Save to JSON in Google Drive folder\n",
        "output_path = f'{base_path}processed/ner_untrained_predictions.json'\n",
        "save_model_output(val_results, output_path)\n",
        "\n",
        "scores_path = f'{base_path}processed/ner_untrained_scores.json'\n",
        "compute_f1(output_path, scores_path, val_orig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569023bb",
      "metadata": {
        "id": "569023bb"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y4WEIL8GRv9e",
      "metadata": {
        "id": "y4WEIL8GRv9e"
      },
      "outputs": [],
      "source": [
        "def make_safe_weights(\n",
        "    train_dataset: Dataset,\n",
        "    label_column: str = \"labels\",\n",
        "    o_label_id: int = 0,\n",
        "    clip_range: tuple = (0.05, 5.0),\n",
        "    o_label_weight: float = 0.10,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Build a class-weight tensor for token-level NER that\n",
        "\n",
        "    1. is inverse-frequency based (rare labels ↑ weight);\n",
        "    2. has mean weight = 1 (keeps the overall loss scale stable);\n",
        "    3. is clipped to `clip_range` to avoid huge gradients;\n",
        "    4. overwrites the 'O' label weight with `o_label_weight`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_dataset : datasets.Dataset\n",
        "        Your training split after any `-100` masking.\n",
        "    label_column : str\n",
        "        Column that holds the integer tag sequences.\n",
        "    o_label_id : int\n",
        "        ID of the majority 'O' label (usually 0).\n",
        "    clip_range : (float, float)\n",
        "        Min / max weight allowed after normalisation.\n",
        "    o_label_weight : float\n",
        "        Final weight assigned to the 'O' label.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor  shape = (num_labels,)\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Count how many *labelled* tokens of each class you have\n",
        "    # ------------------------------------------------------------------\n",
        "    counts: Counter[int] = Counter()\n",
        "    for seq in train_dataset[label_column]:\n",
        "        for lbl in seq:\n",
        "            if lbl != -100:          # ignore the sub-token masks\n",
        "                counts[lbl] += 1\n",
        "\n",
        "    num_labels = max(counts) + 1     # assumes label ids are 0 … N-1\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Inverse-frequency weighting\n",
        "    # ------------------------------------------------------------------\n",
        "    total = sum(counts.values())\n",
        "    inv_freq = {lbl: total / cnt for lbl, cnt in counts.items()}\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Normalise so that mean(weight)=1, then clip\n",
        "    # ------------------------------------------------------------------\n",
        "    mean_w = sum(inv_freq.values()) / len(inv_freq)\n",
        "    weights = {}\n",
        "    low, high = clip_range\n",
        "    for lbl in range(num_labels):\n",
        "        w = inv_freq.get(lbl, 1.0) / mean_w     # unseen lbls → weight 1\n",
        "        w = max(low, min(w, high))              # clip to safe range\n",
        "        weights[lbl] = w\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Down-weight the 'O' label explicitly\n",
        "    # ------------------------------------------------------------------\n",
        "    weights[o_label_id] = o_label_weight\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Build tensor\n",
        "    # ------------------------------------------------------------------\n",
        "    weight_vector = torch.tensor(\n",
        "        [weights[i] for i in range(num_labels)],\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    return weight_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0w6n4GDR3ypC",
      "metadata": {
        "id": "0w6n4GDR3ypC"
      },
      "outputs": [],
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, *args, loss_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_weights = loss_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = CrossEntropyLoss(\n",
        "            weight=self.loss_weights.to(model.device),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "        # reshape to (batch_size*seq_len, num_labels)\n",
        "        loss = loss_fct(\n",
        "            logits.view(-1, model.config.num_labels),\n",
        "            labels.view(-1)\n",
        "        )\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b30c2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9b30c2f",
        "outputId": "42d1b32a-bc94-47a7-ab35-57ff4276a7ba"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "train_batch_size = 2\n",
        "gradient_accumulation_steps = 8\n",
        "num_epochs = 5\n",
        "learning_rate = 1e-5\n",
        "total_steps = math.ceil(len(train_ds) / train_batch_size / gradient_accumulation_steps) * num_epochs\n",
        "warmup_steps = int(total_steps * 0.1)\n",
        "weight_vector = make_safe_weights(train_ds)\n",
        "print(f\"Using learning_rate={learning_rate}, batch_size={train_batch_size}, epochs={num_epochs}, warmup_steps={warmup_steps}, weight_decay=0.01\")\n",
        "\n",
        "\"\"\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./models/longformer-ner',\n",
        "    num_train_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=train_batch_size,\n",
        "    warmup_steps=warmup_steps,\n",
        "    weight_decay=0.01,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./models/longformer-ner-hp',\n",
        "    num_train_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    lr_scheduler_type='linear',\n",
        "    warmup_ratio=0.2,\n",
        "    warmup_steps=warmup_steps,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "\n",
        ")\n",
        "\n",
        "# Metric computation\n",
        "evaluator = evaluate.load('seqeval')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    preds = predictions.argmax(-1)\n",
        "    true_labels = [[id2label[l] for l in label_seq if l != -100] for label_seq in labels]\n",
        "    true_preds = [[id2label[p_] for (p_, l) in zip(pred_seq, label_seq) if l != -100]\n",
        "                  for pred_seq, label_seq in zip(preds, labels)]\n",
        "    results = evaluator.compute(predictions=true_preds, references=true_labels)\n",
        "    return {\n",
        "        'precision': results['overall_precision'],\n",
        "        'recall': results['overall_recall'],\n",
        "        'f1': results['overall_f1']\n",
        "    }\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    loss_weights=weight_vector,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JWCYFUC9Qg3r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWCYFUC9Qg3r",
        "outputId": "14d19d23-ad6a-4515-d0c1-162ef7b63339"
      },
      "outputs": [],
      "source": [
        "print(weight_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YOGKCutIGFU4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "YOGKCutIGFU4",
        "outputId": "2f1f1a26-edf7-4515-ad38-b195b7ba52ea"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "\n",
        "print(f\"Total truncated documents: {trunc_count} / {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca63db25",
      "metadata": {
        "id": "ca63db25"
      },
      "source": [
        "# NER on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e97528",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6e97528",
        "outputId": "0b4152ee-0d76-4fd0-82e7-1bd660191f7a"
      },
      "outputs": [],
      "source": [
        "# Create NER pipeline using the fine-tuned model\n",
        "ner_pipe_finetuned = pipeline(\n",
        "    'ner',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0,\n",
        "    aggregation_strategy='simple'\n",
        ")\n",
        "\n",
        "# Run NER on validation documents and collect aggregated results\n",
        "val_results = []\n",
        "for idx, example in enumerate(val_orig):\n",
        "    preds = ner_pipe_finetuned(example['doc'])\n",
        "    val_results.append({\n",
        "        'index': idx,\n",
        "        #'doc_title': example.get('title', f'doc_{idx}'),\n",
        "        'predictions': preds\n",
        "    })\n",
        "\n",
        "# Save to JSON in Google Drive folder\n",
        "output_path = f'{base_path}processed/ner_trained_predictions.json'\n",
        "save_model_output(val_results, output_path)\n",
        "\n",
        "scores_path = f'{base_path}processed/ner_trained_scores.json'\n",
        "compute_f1(output_path, scores_path, val_orig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n-qcvU47VvfK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1d4ce6e59399448eb30f99608c8577af",
            "f0d34297d0b7435599991db36e50558b",
            "c157931b748247abb27562ada9a43d6e",
            "373db45ed27b4cda987c1e0c5466f09a",
            "cb770c56115944cda024f0e258efa95a",
            "23b95b0fed7d442d963dbcda0c2988e1",
            "1046a2b3bde8442ea8593d087e04cfcb",
            "890f534fe6e840a2a1b4de75a36052c6",
            "c5e211a1011b4c519c0f9e086892200d",
            "2ec435f183ff421fa86cac0217c3f689",
            "978469a5fe28478cb5bf45a06ca28d2e"
          ]
        },
        "id": "n-qcvU47VvfK",
        "outputId": "b2d320d2-763c-4ac7-b45a-909c94b7678c"
      },
      "outputs": [],
      "source": [
        "train_ds.save_to_disk(f'{base_path}processed/train_ds')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1046a2b3bde8442ea8593d087e04cfcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d4ce6e59399448eb30f99608c8577af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0d34297d0b7435599991db36e50558b",
              "IPY_MODEL_c157931b748247abb27562ada9a43d6e",
              "IPY_MODEL_373db45ed27b4cda987c1e0c5466f09a"
            ],
            "layout": "IPY_MODEL_cb770c56115944cda024f0e258efa95a"
          }
        },
        "23b95b0fed7d442d963dbcda0c2988e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ec435f183ff421fa86cac0217c3f689": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "373db45ed27b4cda987c1e0c5466f09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ec435f183ff421fa86cac0217c3f689",
            "placeholder": "​",
            "style": "IPY_MODEL_978469a5fe28478cb5bf45a06ca28d2e",
            "value": " 945/945 [00:00&lt;00:00, 26204.69 examples/s]"
          }
        },
        "890f534fe6e840a2a1b4de75a36052c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "978469a5fe28478cb5bf45a06ca28d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c157931b748247abb27562ada9a43d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890f534fe6e840a2a1b4de75a36052c6",
            "max": 945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5e211a1011b4c519c0f9e086892200d",
            "value": 945
          }
        },
        "c5e211a1011b4c519c0f9e086892200d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb770c56115944cda024f0e258efa95a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0d34297d0b7435599991db36e50558b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23b95b0fed7d442d963dbcda0c2988e1",
            "placeholder": "​",
            "style": "IPY_MODEL_1046a2b3bde8442ea8593d087e04cfcb",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
