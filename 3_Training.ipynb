{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300eb73b",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9273274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (2.2.2)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: filelock in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: sympy in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/daniellocher/micromamba/envs/nlp_project/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-macosx_10_13_x86_64.whl (12.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53def38a",
   "metadata": {},
   "source": [
    "# Establish Google Drive Connection (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ce452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46f4dd",
   "metadata": {},
   "source": [
    "# 1. Imports and Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bfbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted labels:\n",
      "['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MISC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Load SpanBERT configuration and set up token classification head\\nmodel_name = \"SpanBERT/spanbert-large-cased\"\\nconfig = AutoConfig.from_pretrained(model_name)\\nconfig.num_labels = len(entity_label_set)\\nconfig.id2label = id2label\\nconfig.label2id = label2id\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForTokenClassification.from_pretrained(model_name, config=config)\\nmodel.to(device)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load JSON files and store them in memory\n",
    "val_data_path = \"data/raw/dev\" # change to local path\n",
    "val_data = []\n",
    "\n",
    "# loop through all files in the given folder\n",
    "for root, dirs, files in os.walk(val_data_path):\n",
    "    for file_name in files:\n",
    "        with open(f\"data/raw/dev/{file_name}\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        val_data.append(data)\n",
    "\n",
    "# build a list with all unique entity labels\n",
    "unique_label_set = set()\n",
    "\n",
    "for dataset in val_data:\n",
    "    for record in dataset:\n",
    "        for label in record[\"entity_label_set\"]:\n",
    "            unique_label_set.add(label)\n",
    "\n",
    "entity_label_set = sorted(list(unique_label_set))\n",
    "\n",
    "print(\"Extracted labels:\")\n",
    "print(entity_label_set)\n",
    "\n",
    "# Build label mappings\n",
    "label2id = {label: i for i, label in enumerate(entity_label_set)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "\n",
    "# Load SpanBERT configuration and set up token classification head\n",
    "model_name = \"SpanBERT/spanbert-large-cased\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(entity_label_set)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, config=config)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d0c48",
   "metadata": {},
   "source": [
    "# 2. Baseline NER with Untrained Model on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a token-classification pipeline for baseline inference\n",
    "ner_pipeline = pipeline(\n",
    "    task=\"ner\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Prepare raw texts\n",
    "train_texts = []\n",
    "\n",
    "for dataset in val_data:\n",
    "    train_texts.append([ex['doc'] for ex in dataset])\n",
    "\n",
    "# Run baseline NER\n",
    "baseline_results = [ner_pipeline(text) for text in train_texts]\n",
    "\n",
    "# Display first example\n",
    "print(baseline_results[0])\n",
    "\n",
    "with open(f\"/content/drive/MyDrive/dataset/ner_baseline_output.json\", \"w\") as f:\n",
    "                json.dump(baseline_results, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
